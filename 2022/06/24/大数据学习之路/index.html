<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"xiaowodi.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="[TOC] Java基础复习（JDK1.8）容器篇1.ArrayList关键源码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576  &#x2F;**   * Defaul">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据学习之路">
<meta property="og:url" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="[TOC] Java基础复习（JDK1.8）容器篇1.ArrayList关键源码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576  &#x2F;**   * Defaul">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://github.com/wangzhiwubigdata/God-Of-BigData/raw/master/JVM/JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84.resources/6F3902DB-275A-4FC6-8E3A-754DE6F987BA.jpg">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200824152328561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200824152623936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200824152644220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200824152724174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/1c13b8e41120caccd15369497355b588.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/111fbad04e83f6fc486c78406621ae05.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210128165345527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3c3MzM1MTIz,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/abdedea73525f71775a338dbceeedd2a.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/e1b908c08120b3323a3d5ca408bc569b.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/ae5a11b458e117b8f30fdc064821647c.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/d278aed530716ea4981a02fb8590c946.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/b03dfc84f0de89fe474936cbd70aef32.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/8d9b5b42d191a4ad452074f204507378.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200810150148636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E2NDY3MDU4MTY=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMjAwNDkzNC8yMDIwMDcvMjAwNDkzNC0yMDIwMDcyOTEyNDgyNjk0MC0xMDQyODAzODI0LnBuZw?x-oss-process=image/format,png#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20201221191858529.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200823081540511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200824153251847.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center#">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2020081722362322.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N0ZXZlbjJ4dXB0,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200817001540364.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N0ZXZlbjJ4dXB0,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/4491294-e3bcefb2bacea224.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/CLH%E9%98%9F%E5%88%97-9358587-9358590.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210819193357918-9372839.png">
<meta property="og:image" content="http://c.biancheng.net/uploads/allimg/181114/3-1Q114101Fa22.gif">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MDI1NTkx,size_16,color_FFFFFF,t_70.png">
<meta property="og:image" content="https://img2018.cnblogs.com/blog/1174710/201901/1174710-20190128201003630-2050662608.png">
<meta property="og:image" content="https://img2018.cnblogs.com/blog/1174710/201901/1174710-20190128201011603-1317894910.png">
<meta property="og:image" content="https://img2018.cnblogs.com/blog/1174710/201901/1174710-20190128201021606-1089980279.png">
<meta property="og:image" content="https://img2018.cnblogs.com/blog/1174710/201901/1174710-20190128201034603-681355962.png">
<meta property="og:image" content="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Faliyunzixunbucket.oss-cn-beijing.aliyuncs.com%2Fjpg%2F6dd68b173df7809bc8dc27a30937a22d.jpg%3Fx-oss-process%3Dimage%2Fresize%2Cp_100%2Fauto-orient%2C1%2Fquality%2Cq_90%2Fformat%2Cjpg%2Fwatermark%2Cimage_eXVuY2VzaGk%3D%2Ct_100&refer=http%3A%2F%2Faliyunzixunbucket.oss-cn-beijing.aliyuncs.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1630634532&t=cc5ce4c5d99be9f2529373c3cd481029">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-cf37bafd8a121454b5488c53ff2e0b2e_1440w.jpg">
<meta property="og:image" content="https://github.com/xiaowodi/Resources/blob/main/images/gitImages/Mysql%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84.png?raw=true">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210705153740561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA4NDg4NDU=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/6450093-0c5b3f7a2eceaaef.jpg">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/6450093-dbe2595ee1e293b1.png">
<meta property="og:image" content="https://imgconvert.csdnimg.cn/aHR0cHM6Ly95cWZpbGUuYWxpY2RuLmNvbS9lOGI3ODEzNzIyMGM4OTUyOGVjZDA0NDY0NjJiZDI3Y2FmNGRmNTRkLnBuZw?x-oss-process=image/format,png">
<meta property="og:image" content="https://imgconvert.csdnimg.cn/aHR0cHM6Ly95cWZpbGUuYWxpY2RuLmNvbS9iZTU0N2YxNGY3YmZhZGZlNGQyMjJkOTFhNDIyMTk4NmJkNzU3ZDk2LnBuZw?x-oss-process=image/format,png">
<meta property="og:image" content="https://www.pianshen.com/images/782/2d5555fa9bb1b9bda4614b97abcc7d0e.png">
<meta property="og:image" content="https://www.pianshen.com/images/955/28906ddfe55a52f88180366de7c6b3bb.png">
<meta property="og:image" content="https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726181803473-2052825806.png">
<meta property="og:image" content="https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726181824985-212928464.png">
<meta property="og:image" content="https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726181459479-554913934.png">
<meta property="og:image" content="https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726183159871-354972654.png">
<meta property="og:image" content="https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726183647821-1922505507.png">
<meta property="og:image" content="https://img2.baidu.com/it/u=1912645117,1546810615&fm=15&fmt=auto&gp=0.jpg">
<meta property="og:image" content="https://img1.baidu.com/it/u=3492215315,2225793381&fm=26&fmt=auto&gp=0.jpg">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210808112547253.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16284015946612">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200518170149838.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E1Njg1MjYz,size_16,color_FFFFFF,t_70#pic_center">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210808163959670-16284120015104.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210808171159136-16284139206945.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210809095832482-8474331.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210809102313917-8475795.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/DAGScheduler.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/TaskScheduler.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/TaskScheduler%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/format,png.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%AA%E4%BC%98%E5%8C%96%E7%9A%84HashShuffle.jpeg">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/优化的HashShuffle.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021年8月份秋招复习笔记/普通SortShuffle.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021年8月份秋招复习笔记/byPassSortShuffle.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/src=http%253A%252F%252Fwx1.sinaimg.cn%252Fmw690%252F63918611gy1fe7btgzmz8j20le0fidhc.jpg&refer=http%253A%252F%252Fwx1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/src=http%253A%252F%252Fupload-images.jianshu.io%252Fupload_images%252F9175374-a2a527f62646d62b.png&refer=http%253A%252F%252Fupload-images.jianshu.io&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210809202402983-8511844.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86.jpeg">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/1228818-20180426212853794-858627420.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210809202631142-8511994.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810095712565-8560634-8561548.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810095721948-8560643-8561543.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810102102079-8562063.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810102807976-8562489.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810111030982-8565032.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810111500233-8565301.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810111914642-8565556.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810153402154-8580843.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210917191421213-16318772632891.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-16318778783212-16318778797984.webp">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-16318798352397-16318798366029.webp">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-163188039067910-163188039254812.webp">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-16319326499621-16319326513313.webp">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-16319328669854-16319328684116.webp">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210922152340903-16322954231991.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210922155639198-16322974007352.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210922155810394-16322974914213.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210922160232506-16322977552994.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210922160803410-16322980847205.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/spark1%E6%BA%90%E7%A0%81-16328806267071.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%BB%84%E4%BB%B6%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-16328829051882-16328829071034.webp">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20211002093226904-16331383487631.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AA%84%E4%BE%9D%E8%B5%961.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AA%84%E4%BE%9D%E8%B5%962.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/834652-20170505114231242-1674540562.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E4%BB%B6-16334225169802.svg">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021年8月份秋招复习笔记/src=http%3A%2F%2Fwww.pianshen.com%2Fimages%2F312%2F2c1818a3d8f7bee6d472827c0fc3c908.JPEG&refer=http%3A%2F%2Fwww.pianshen.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210814152542812-16289259443932.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-20210810162423999">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/src=http%253A%252F%252Fimage.bubuko.com%252Finfo%252F202002%252F20200209175734687167.png&refer=http%253A%252F%252Fimage.bubuko.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/src=http%253A%252F%252Fimg2018.cnblogs.com%252Fi-beta%252F1201165%252F202002%252F1201165-20200226224404735-1637370414.png&refer=http%253A%252F%252Fimg2018.cnblogs.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70-20210811102106660.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70-20210811102926988.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16286927154611">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021年8月份秋招复习笔记/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16286931799323">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16286934145025">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16286953647487">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70-20210812102618331.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70-20210812104838555.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70-20210812105106318.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210812145139764-8751101.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/640.jpeg">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/640-20210812152030393.jpeg">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210812153815055-8753896.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/src=http://oscimg.oschina.net/oscnet/c15e5cf0-5f5a-48ac-ba14-b3d370a196cb.png&refer=http://oscimg.oschina.net&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg">
<meta property="og:image" content="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg2020.cnblogs.com%2Fblog%2F1824311%2F202009%2F1824311-20200908213604870-202910531.png&refer=http%3A%2F%2Fimg2020.cnblogs.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1630159603&t=9156f9309a498e83d6fe0a15fa6dc8e9">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/url=http://dingyue.ws.126.net/2021/0727/1e1f12b7j00qwwn0g002ac000hs00umg.jpg&thumbnail=650x2147483647&quality=80&type=jpg">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/i=20210111152147546.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/i=2021011115251435.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/i=20210111153027619.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/i=20210111153426447.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/i=20210111153926877.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/i=2021011115420988-16300710261841.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/i=20210111154304922.png">
<meta property="og:image" content="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fattach.dataguru.cn%2Fattachments%2Fforum%2F201509%2F20%2F084322huqeon7nkwtakmwq.png&refer=http%3A%2F%2Fattach.dataguru.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1633742797&t=84d69a96c3939ab09772d54e6a7613b6">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210909102257596-16311541787475.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210909102436921.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/大数据学习之路/v2-5d7aee3655ceb6be8625ddb58e1724bd_r.jpg">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/1439096-20201115232936579-1750711590.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/70.jpeg">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/70.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/20160406120904661.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021年8月份秋招复习笔记/image-20210826145958572-16299612004811-16299612155912.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210826150352497-16299614343404.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021年8月份秋招复习笔记/image-20210826150649365-16299616136935.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210826152556164-16299627579376.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830141834926-16303043159223.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830141932789.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830143707374-16303054283854.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830143741682-16303054635275.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021年8月份秋招复习笔记/image-20210830144740667.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830144246148.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830145003625.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830141932789.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830150452674.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021年8月份秋招复习笔记/image-20210830150719139.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830153927092.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021年8月份秋招复习笔记/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16289931893091">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021年8月份秋招复习笔记/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16289933054913">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210817101206793-9166328.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210817101527618-9166529.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210817102317518-9167001.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210817155911310-9187152.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210817160123682-9187286.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210817160516330-9187518.png">
<meta property="og:image" content="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210817162816837-9188898.png">
<meta property="article:published_time" content="2022-06-24T08:12:10.818Z">
<meta property="article:modified_time" content="2022-06-24T08:12:10.822Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/wangzhiwubigdata/God-Of-BigData/raw/master/JVM/JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84.resources/6F3902DB-275A-4FC6-8E3A-754DE6F987BA.jpg">

<link rel="canonical" href="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>大数据学习之路 | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://xiaowodi.github.io/2022/06/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          大数据学习之路
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-06-24 08:12:10" itemprop="dateCreated datePublished" datetime="2022-06-24T08:12:10+00:00">2022-06-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>[TOC]</p>
<h1 id="Java基础复习（JDK1-8）"><a href="#Java基础复习（JDK1-8）" class="headerlink" title="Java基础复习（JDK1.8）"></a>Java基础复习（JDK1.8）</h1><h2 id="容器篇"><a href="#容器篇" class="headerlink" title="容器篇"></a>容器篇</h2><h3 id="1-ArrayList"><a href="#1-ArrayList" class="headerlink" title="1.ArrayList"></a>1.ArrayList</h3><p>关键源码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Default initial capacity.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// 默认初识容量为10</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">DEFAULT_CAPACITY</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Shared empty array instance used for empty instances.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Object[] EMPTY_ELEMENTDATA = &#123;&#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Shared empty array instance used for default sized empty instances. We</span></span><br><span class="line"><span class="comment">   * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when</span></span><br><span class="line"><span class="comment">   * first element is added.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">transient</span> Object[] elementData; <span class="comment">// non-private to simplify nested class access</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用空参数构造方法之后，列表还是一个空的列表</span></span><br><span class="line">  <span class="keyword">public</span> <span class="title function_">ArrayList</span><span class="params">()</span> &#123;</span><br><span class="line">      <span class="built_in">this</span>.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The maximum size of array to allocate.</span></span><br><span class="line"><span class="comment">   * Some VMs reserve some header words in an array.</span></span><br><span class="line"><span class="comment">   * Attempts to allocate larger arrays may result in</span></span><br><span class="line"><span class="comment">   * OutOfMemoryError: Requested array size exceeds VM limit</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="comment">// 最大可库容的容量阈值， 当容量超过这个值的时候，ArrayList的最大容量为Integer.MAX_VALUE</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">MAX_ARRAY_SIZE</span> <span class="operator">=</span> Integer.MAX_VALUE - <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 添加元素</span></span><br><span class="line">  <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">add</span><span class="params">(E e)</span> &#123;</span><br><span class="line">      ensureCapacityInternal(size + <span class="number">1</span>);  <span class="comment">// Increments modCount!!</span></span><br><span class="line">      elementData[size++] = e;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">ensureCapacityInternal</span><span class="params">(<span class="type">int</span> minCapacity)</span> &#123;</span><br><span class="line">      ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="type">int</span> <span class="title function_">calculateCapacity</span><span class="params">(Object[] elementData, <span class="type">int</span> minCapacity)</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123;</span><br><span class="line">          <span class="keyword">return</span> Math.max(DEFAULT_CAPACITY, minCapacity);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> minCapacity;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">ensureExplicitCapacity</span><span class="params">(<span class="type">int</span> minCapacity)</span> &#123;</span><br><span class="line">      modCount++;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// overflow-conscious code</span></span><br><span class="line">      <span class="keyword">if</span> (minCapacity - elementData.length &gt; <span class="number">0</span>)</span><br><span class="line">          grow(minCapacity);</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">// 扩容关键性代码</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">grow</span><span class="params">(<span class="type">int</span> minCapacity)</span> &#123;</span><br><span class="line">      <span class="comment">// overflow-conscious code</span></span><br><span class="line">      <span class="type">int</span> <span class="variable">oldCapacity</span> <span class="operator">=</span> elementData.length;</span><br><span class="line">      <span class="type">int</span> <span class="variable">newCapacity</span> <span class="operator">=</span> oldCapacity + (oldCapacity &gt;&gt; <span class="number">1</span>);  <span class="comment">// 每次扩容为原来的1.5倍</span></span><br><span class="line">      <span class="keyword">if</span> (newCapacity - minCapacity &lt; <span class="number">0</span>)</span><br><span class="line">          newCapacity = minCapacity; <span class="comment">//扩容后如果不够，那么就直接扩容为当前容量+1</span></span><br><span class="line">      <span class="keyword">if</span> (newCapacity - MAX_ARRAY_SIZE &gt; <span class="number">0</span>)</span><br><span class="line">          newCapacity = hugeCapacity(minCapacity);</span><br><span class="line">      <span class="comment">// minCapacity is usually close to size, so this is a win:</span></span><br><span class="line">      elementData = Arrays.copyOf(elementData, newCapacity);  <span class="comment">// 通过拷贝数据中的元素到新的数组，进行扩容</span></span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">// 判断是否超过最大的扩容阈值</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="type">int</span> <span class="title function_">hugeCapacity</span><span class="params">(<span class="type">int</span> minCapacity)</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (minCapacity &lt; <span class="number">0</span>) <span class="comment">// overflow</span></span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">OutOfMemoryError</span>();</span><br><span class="line">      <span class="keyword">return</span> (minCapacity &gt; MAX_ARRAY_SIZE) ?</span><br><span class="line">          Integer.MAX_VALUE :</span><br><span class="line">          MAX_ARRAY_SIZE;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><strong>总结：</strong></p>
<ul>
<li>ArrayList底层是通过数组实现的</li>
<li>ArrayList的默认容量为10；</li>
<li>ArrayList为懒加载，只有在添加了第一个元素之后才会真正分配空间</li>
<li>扩容时，每次扩容为原来容量的1.5倍：原来的容量值+容量值&gt;&gt;1</li>
<li>如果扩容后容量超过Integer.MAX_VALUE-8，ArrayList的容量就为Integer的最大值</li>
<li>每次扩容时，是通过将旧的数组中的元素拷贝到扩容后的新的数组中</li>
<li>查询、更新元素效率高</li>
</ul>
<h3 id="2-LinkedList"><a href="#2-LinkedList" class="headerlink" title="2.LinkedList"></a>2.LinkedList</h3><ul>
<li>LinkedList底层是通过双向链表实现的</li>
<li>可以当成Stack与Queue来实现</li>
<li>插入，删除元素效率高</li>
</ul>
<h3 id="3-HashMap"><a href="#3-HashMap" class="headerlink" title="3.HashMap"></a>3.HashMap</h3><p><a target="_blank" rel="noopener" href="https://editor.csdn.net/md/?articleId=113561579">HashMap原理</a></p>
<p>小总结：</p>
<ul>
<li>初识主数组长度：16（1&lt;&lt;4）</li>
<li>主数组最大长度：2^30</li>
<li>默认的负载因子0.75</li>
<li>链表树化阈值1：8</li>
<li>链表树化阈值2：主数组table长度超过64</li>
<li>红黑树退化成链表阈值：树节点少于6</li>
</ul>
<blockquote>
<p>HashMap的主数组长度需要满足2的幂次方</p>
<p>比如输入为1，table长度为2</p>
<p>输入长度为15，table长度为16</p>
</blockquote>
<h4 id="HashMap插入元素底层原理"><a href="#HashMap插入元素底层原理" class="headerlink" title="HashMap插入元素底层原理"></a>HashMap插入元素底层原理</h4><ol>
<li><p>插入元素前对key的HashCode进行扰动函数hash()计算</p>
<ol>
<li><blockquote>
<p>现获取key的hashCode的值h，然后将h与h的高16位进行异或运算</p>
<p>其目的是在进行路由寻址的时候，能够保证在元素个数较少的情况下，路由地址会同时保持高16位和低16位的共同特征</p>
</blockquote>
</li>
</ol>
</li>
<li><p>插入元素</p>
<ol>
<li><blockquote>
<p><strong>路由公式： i=hash &amp; (table.length - 1)</strong></p>
<p>为什么是table.length - 1，而不是table.length呢？</p>
<p>因为table.length为2的幂次方计算出来（1000000000），0多1少</p>
<p>直接与hash进行&amp;运算的时候，都会变为0，更容易发生hash冲突</p>
<p>-1的目的就是将众多的0变为1，<em>与运算之后的值更不容易相同，缓解hash冲突。</em></p>
</blockquote>
</li>
<li><blockquote>
<p>路由地址计算出来后，就要插入元素</p>
<p><strong>插入情景1</strong>：主数组i位置为null（没有冲突），直接插入元素</p>
<p><strong>插入情景2</strong>：主数组i位置存在元素，且key值相同，就新的value值覆盖旧的value值</p>
<p><strong>插入情景3</strong>：主数组i位置存在元素，且没有树化，尾插法插入链表</p>
<p>​                      如果链表长度超过8，同时主数组长度打到64， 才开始树化</p>
<p><strong>插入情景4</strong>：主数组i位置存在元素，且已经树化成红黑树，向红黑树中插入元素</p>
</blockquote>
</li>
</ol>
</li>
<li><p>元素插入达到阈值，进行扩容</p>
<ol>
<li><blockquote>
<p>阈值计算：当前主数组长度 * 负载因子</p>
<p>当map中的元素个数size大于这个阈值的时候，触发扩容</p>
</blockquote>
</li>
<li><p>主数组每次扩容原来的一倍：通过向左移位来实现（避免经过乘法器，耗性能）</p>
</li>
<li><p>扩容情景：</p>
<ol>
<li><p>主数组对应的slot内没有元素（null），不做处理</p>
</li>
<li><p>主数组对应的slot内有元素，但是没有链化，直接用改元素的扰动值hash直接与（新的数组长度-1）进行&amp;运算，计算出新的位置</p>
</li>
<li><p>主数组对应的slot内有元素，但是已经链化了</p>
<ol>
<li><blockquote>
<p>这时就需要进行高低链分链</p>
<p>用key的hash值（扰动后的）与扩容前的旧容量进行&amp;运算，如果为0，即为低链；不为0,即为高链</p>
<p>低链的元素扩容后还在<strong>原索引</strong>位置；高链的元素扩容后在<strong>原索引+旧容量</strong>处</p>
</blockquote>
</li>
<li><p>如果是红黑树进行分链的时候，可能元素会少于6个，这个时候就需要退化成链表</p>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<p><font color=red>HashMap在高并发的情况下，链表会出现环形链表</font></p>
<p><font color=green>这里提及一道经典面试题：如何判断一个链表是否有环？？？</font></p>
<p>使用快慢指针（double pointer）</p>
<p>slow和fast：slow每次走一步，fast每次走两步；如果两个指针能相遇，一定存在环儿；</p>
<p>（<font color=blue>生活中的例子：两个人跑圈，快的人在第二圈的时候一定会遇到慢的那个人</font>）</p>
<h3 id="4-线程安全的容器"><a href="#4-线程安全的容器" class="headerlink" title="4.线程安全的容器"></a>4.线程安全的容器</h3><p>上述提及的容器类都是线程不安全的容器类，在并发环境下应该避免使用</p>
<h4 id="线程安全的List"><a href="#线程安全的List" class="headerlink" title="线程安全的List"></a>线程安全的List</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">List</span> <span class="variable">list</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;(); <span class="comment">// 效率高，不支持并发</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="type">List</span> <span class="variable">list</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Vector</span>&lt;&gt;(); <span class="comment">// 线程安全，但是效率低</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">vector 在添加元素的一些操作的方法，添加了synchronized关键，进行上锁，效率比较低</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="type">List</span> <span class="variable">list</span> <span class="operator">=</span> Collections.synchronizedList(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;()); <span class="comment">// 线程安全，小数量完全可以</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">synchronizedList内部是通过在具体的操作上包裹synchronized关键字，而不是粗暴的同步整个方法</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="type">List</span> <span class="variable">list</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CopyOnWriteArrayList</span>&lt;&gt;(); <span class="comment">// 线程安全，JUC包下的类（写时复制）,适用于多线程环境</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">每次添加元素的时候，先将集合中的元素复制到一个长度+1的新的数组中，然后将新增的元素添加到新的数组中，然后再将数组引用指向新的数组中。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">这就保证了：读和写是在不同的对象上进行的，所以不存在资源竞争关系，不需要加锁</span></span><br><span class="line"><span class="comment">					读写分离思想</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>

<h4 id="线程安全的HashMap"><a href="#线程安全的HashMap" class="headerlink" title="线程安全的HashMap"></a>线程安全的HashMap</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Collections.synchronizedMap</span></span><br><span class="line">Map&lt;Object, Object&gt; synchronizedMap = Collections.synchronizedMap(<span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;());</span><br><span class="line"></span><br><span class="line"><span class="comment">// ConcurrentHashMap</span></span><br><span class="line">ConcurrentHashMap&lt;Object, Object&gt; map = <span class="keyword">new</span> <span class="title class_">ConcurrentHashMap</span>&lt;&gt;();</span><br></pre></td></tr></table></figure>

<h5 id="ConCurrentHashMap"><a href="#ConCurrentHashMap" class="headerlink" title="ConCurrentHashMap"></a>ConCurrentHashMap</h5><p>1.7 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31614308">ConCurrentHashMap小灰漫画</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/ZOKEKAI/article/details/90051567%EF%BC%9AConcurrentHashMap%E8%AF%A6%E8%A7%A3">https://blog.csdn.net/ZOKEKAI/article/details/90051567：ConcurrentHashMap详解</a></p>
<p><code>sizeCtl</code>:</p>
<blockquote>
<p>默认值为0；</p>
<p> -1:表示table正在初始化；通过CAS赋值sizeCtl为-1，然后控制只有一个线程来完成初始化操作，其他线程都需要让出CPU<code>Thread.yield</code></p>
<p>-N:表示有N-1个线程正在扩容；</p>
<p>其它情况：</p>
<ul>
<li>如果table未初始化，表示table需要初始化的大小</li>
<li>如果table初始化完成，表示table的容量，默认是table大小的0.75倍；</li>
</ul>
</blockquote>
<p>扩容时，是从后向前进行元素的<strong>迁移</strong></p>
<p>多线程扩容，每个线程负责主数组16个slot，小于16的使用单线程即可</p>
<p><strong>什么是lastRun节点？</strong></p>
<p>lastRun是在迁移一个链表的数据时，保证后面的节点与自己的取余值相同，避免后面没有必要的循环。</p>
<p>其本质就是作为<strong>低位链</strong>或者<strong>高位链</strong>的头结点，这个节点后边的元素可以不进行循环计算，而前面的节点需要挨个进行hash&amp;运算，如果是<strong>低位链</strong>或者<strong>高位链</strong>的节点，就采用<font color=green>头插法</font>将其节点插入到链表中。剩余的元素就是<strong>高位链</strong>或者<strong>低位链</strong>的元素。</p>
<h2 id="JVM篇"><a href="#JVM篇" class="headerlink" title="JVM篇"></a>JVM篇</h2><p>JVM：java虚拟机，能够识别.class文件，能够将class文件中的字节码指令进行识别并调用操作系统向上的API完成动作。</p>
<p>JRE：Java运行时环境。主要包括两个部分：JVM的标准实现和java的一些基本类库。相对于jvm来说，jre多出来一部分java类库</p>
<p>JDK：Java开发工具包。是整个Java开发的核心，继承了jre和一些好用的小工具。</p>
<h3 id="1-JVM的内存构成"><a href="#1-JVM的内存构成" class="headerlink" title="1.JVM的内存构成"></a>1.JVM的内存构成</h3><p>JAVA内存构成包括：<strong>堆、java栈</strong>、本地方法栈、程序计数器</p>
<p>jdk1.8之后，方法区（元空间并不在jvm中了，而是使用本地内存）</p>
<h4 id="1-1-程序计数器（PC寄存器）"><a href="#1-1-程序计数器（PC寄存器）" class="headerlink" title="1.1 程序计数器（PC寄存器）"></a>1.1 程序计数器（PC寄存器）</h4><h5 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h5><ul>
<li>字节码解释器通过改变程序计数器来一次读取指令，从而实现代码的流程控制</li>
<li>在多线程情况下，程序计数器记录的是当前线程执行的位置，从而当线程切换回来时，就知道上次执行到哪了。</li>
</ul>
<h5 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h5><ul>
<li>是一块较小的内存空间</li>
<li>线程私有，每条线程都有自己的程序计数器</li>
<li>生命周期：随着线程的创建而创建，随着线程的结束而销毁</li>
<li>是唯一一个不会出现OutOfMemroyError的内存区域</li>
</ul>
<h4 id="1-2-Java虚拟机栈（Java栈）"><a href="#1-2-Java虚拟机栈（Java栈）" class="headerlink" title="1.2 Java虚拟机栈（Java栈）"></a>1.2 Java虚拟机栈（Java栈）</h4><h5 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h5><p>Java虚拟机栈是描述Java方法运行过程的内存模型</p>
<p>Java虚拟机栈会为每一个即将运行的Java方法创建一块叫做“栈帧”的区域，用于存放该方法运行过程中的一些信息，如：</p>
<ul>
<li>局部变量表</li>
<li>操作数栈</li>
<li>动态链接</li>
<li>方法出口信息</li>
<li>…….</li>
</ul>
<p><img src="https://github.com/wangzhiwubigdata/God-Of-BigData/raw/master/JVM/JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84.resources/6F3902DB-275A-4FC6-8E3A-754DE6F987BA.jpg" alt="875223b19a3ea457678d5a09acb950e0"></p>
<h5 id="压栈出栈过程"><a href="#压栈出栈过程" class="headerlink" title="压栈出栈过程"></a>压栈出栈过程</h5><p>当方法运行过程中需要创建局部变量时，就将局部变量的值存入栈帧中的局部变量表中。</p>
<p>Java虚拟机栈的栈顶的栈帧是当前正在执行的活动栈，也就是当前正在执行的方法，PC寄存器也会指向这个地址。只有这个活动的栈帧的本地变量可以被操作数栈使用，当在这个栈帧中调用另一个方法，与之对应的栈帧又会被创建，新创建的栈帧压入栈顶，变为当前的活动栈帧。</p>
<p>方法结束之后，当前栈帧被移除，栈帧的返回值变成新的活动栈帧中操作数栈的一个操作数。如果没有返回值，那么新的活动栈帧中操作数栈的操作数没有变化。</p>
<blockquote>
<p>由于Java虚拟机栈是线程对应的，数据不是线程共享的，因此不同关系数据一致性问题，也不会存在同步锁的问题。</p>
</blockquote>
<h5 id="Java栈的特点"><a href="#Java栈的特点" class="headerlink" title="Java栈的特点"></a>Java栈的特点</h5><ul>
<li>局部变量表随着栈帧的创建而创建，它的大小在编译时确定，创建时只需分配事先规定的大小即可。在方法运行过程中，局部变量表的大小不会发生改变。</li>
<li>Java栈会出现两种异常：<code>StackOverFlowError</code> 和 <code>OutOfMemoryError</code><ul>
<li>StackOverFlowError若Java虚拟机栈的大小不允许动态扩展，那么当线程请求栈的深度超过当前Java虚拟机栈的最大深度时，抛出StackOverFlowError异常</li>
<li>OutOfMemoryError若允许动态扩展，那么当线程请求栈时内存用完了，无法再动态扩展时，抛出OutOfMemoryError异常。</li>
</ul>
</li>
<li>Java栈也是线程私有的，随着线程的创建而创建，随着线程的结束而销毁。</li>
</ul>
<blockquote>
<p>出现StackOverFlowError时，内存空间可能还有很多。</p>
</blockquote>
<h4 id="1-3-本地方法栈（C栈）"><a href="#1-3-本地方法栈（C栈）" class="headerlink" title="1.3 本地方法栈（C栈）"></a>1.3 本地方法栈（C栈）</h4><h5 id="本地方法栈的定义："><a href="#本地方法栈的定义：" class="headerlink" title="本地方法栈的定义："></a>本地方法栈的定义：</h5><p>本地方法栈是为JVM运行Native方法准备的空间，由于很多Native方法都是用C语言实现的，所以它通常又叫做C栈。它与Java虚拟机栈实现的功能类似，只不过本地方法栈是描述本地方法运行过程的内存模型。</p>
<h5 id="栈帧变化过程"><a href="#栈帧变化过程" class="headerlink" title="栈帧变化过程"></a>栈帧变化过程</h5><p>本地方法被执行时，在本地方法栈也会创建一块栈帧，用于存放该方法的局部变量表、操作数栈、动态链接、方法出口信息等。</p>
<p>方法执行结束后，相应的栈帧也会出栈，并释放本地内存空间。也会抛出<code>StackOverFlowError</code>和<code>OutOfMemoryError</code>异常。</p>
<blockquote>
<p>如果Java虚拟机本身不支持Native方法，或者本身不依赖与传统栈，那么可以不提供本地方法栈。如果本地支持方法栈，那么这个栈一般会在线程创建的时候按线程分配。</p>
</blockquote>
<h4 id="1-4-Java堆Heap"><a href="#1-4-Java堆Heap" class="headerlink" title="1.4 Java堆Heap"></a>1.4 Java堆Heap</h4><h5 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h5><p>堆是用来存放对象的内存空间，几乎所有的对象都存储在堆中</p>
<h5 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h5><ul>
<li>线程共享，整个Java虚拟机只有一个堆，所有的线程都访问同一个堆。而程序计数器、Java栈、本地方法栈都是一个线程对应一个。</li>
<li>在虚拟机启动时创建。</li>
<li>是垃圾回收的主要场所</li>
<li>进一步可分为：新生代（Eden区， From Survivor、To Survivor）、老年代</li>
</ul>
<p>不同的区域存放不同生命周期的对象，这样可以根据不同的区域使用不同的垃圾回收算法，更具有针对性。</p>
<p>堆的大小既可以固定也可以扩展，但对于主流的虚拟机，堆的大小是可扩展的，因为当线程请求分配内存，但堆已满，且内存已无法再扩展时，就抛出<code>OutOfMemoryError</code></p>
<blockquote>
<p>Java堆所使用的内存不需要保证是连续的。而由于堆是被所有线程共享的，所以对它的访问需要注意同步问题，方法和对应的属性都需要保证一致性。</p>
</blockquote>
<h5 id="堆的划分"><a href="#堆的划分" class="headerlink" title="堆的划分"></a>堆的划分</h5><p><img src="https://img-blog.csdnimg.cn/20200824152328561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p><strong>逻辑上：</strong></p>
<ul>
<li>新生代（Young）<ul>
<li>Eden</li>
<li>幸存区：From 和To</li>
</ul>
</li>
<li>老年代（Old）</li>
<li>元空间</li>
</ul>
<p><strong>物理上：</strong></p>
<p>物理上分为 <strong>新生代+老年代</strong>，而元空间使用的是直接内存</p>
<h5 id="内存分配策略"><a href="#内存分配策略" class="headerlink" title="内存分配策略"></a>内存分配策略</h5><h6 id="1-对象优先分配在Eden区"><a href="#1-对象优先分配在Eden区" class="headerlink" title="1. 对象优先分配在Eden区"></a>1. 对象优先分配在Eden区</h6><p>大多数情况下，对象在新生代Eden区中分配。当Eden去没有足够空间进行分配时，虚拟机将发起一次Minor GC。</p>
<p>👇<strong>Minior GC vs Major GC</strong></p>
<ul>
<li>Minior GC：回收新生代（包括Eden和Survivor区域），因为Java对象大多都具备朝生夕灭的特性，所以Minior GC非常频繁，一般回收速度也比较快。</li>
<li>Major GC：回收老年代，Major GC的速度一般会比Minor GC慢10倍以上。</li>
</ul>
<h6 id="2-大对象直接进入老年代"><a href="#2-大对象直接进入老年代" class="headerlink" title="2.大对象直接进入老年代"></a>2.大对象直接进入老年代</h6><p>大对象是指需要大量连续内存空间的Java对象，如很长的字符串或数据。</p>
<p>虚拟机提供了一个<code>-XX:PretenureSizeThreshold</code>参数，令大于这个设置值的对象直接在老年代分配，这样做的目的是避免在Eden以及两个Survivor区之间发生大量的内存复制。</p>
<blockquote>
<p>只要分配的对象的内存大小大于这个参数的时候就会直接分配到老年代</p>
</blockquote>
<h6 id="3-长期存活的对象将进入老年代（默认15）"><a href="#3-长期存活的对象将进入老年代（默认15）" class="headerlink" title="3. 长期存活的对象将进入老年代（默认15）"></a>3. 长期存活的对象将进入老年代（默认15）</h6><p>JVM给每个对象定义了一个对象年龄计数器。当新生代发生一次Minor GC后，存活下来的对象年龄+1，当年龄超过<code>-XX:MaxTenuringThreshold, 默认为15</code>设置的值时，就将超过该值的所有对象转移到老年代中。</p>
<h6 id="4-动态年龄判定"><a href="#4-动态年龄判定" class="headerlink" title="4. 动态年龄判定"></a>4. 动态年龄判定</h6><p>Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半</p>
<p>（<code>-XX:TargetSurvivorRatio 默认值为50， 意味Survivor区对象使用率阈值为50%</code> ），年龄大于或等于该年龄的对象直接进入老年代。</p>
<h5 id="老年代空间分配担保"><a href="#老年代空间分配担保" class="headerlink" title="老年代空间分配担保"></a>老年代空间分配担保</h5><p>什么是空间分配担保？</p>
<blockquote>
<p>在发生Minor GC之前，虚拟机会检查老年代最大可用的连续空间是否大于新生代所有对象的总空间：</p>
<p>​    如果大于，则此次Minor GC是安全的</p>
<p>​    如果小于, 则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。</p>
<p>​            如果<code>HandlePromotionFailure=true</code>，那么会继续检查老年代最大可用连续空间是否大于<code>历次晋升到老年代的对象的平均大小</code>，如果大于，则尝试进行一次Minor GC，但是这次Minor GC 依然是有风险的（<font color=green>因为Minor Gc之后，再次进入到老年代的对象的总大小有可能超过老年代最大可用连续空间</font>）</p>
<p>​            如果小于或者<code>HandlePromotionFailure=false</code>则改为进行一次Full GC</p>
</blockquote>
<p>为什么要进行空间担保？</p>
<blockquote>
<p>是因为新生代采用<strong>复制收集算法</strong>，假如大量对象在Minor GC后仍然存活（最极端情况为内存回收后新生代中所有对象均存活），而Survivor空间是比较小的，这时就需要老年代进行分配担保，把Survivor无法容纳的对象放到老年代。<strong>老年代要进行空间分配担保，前提是老年代得有足够空间来容纳这些对象</strong>，但一共有多少对象在内存回收后存活下来是不可预知的，<strong>因此只好取之前每次垃圾回收后晋升到老年代的对象大小的平均值作为参考</strong>。使用这个平均值与老年代剩余空间进行比较，来决定是否进行Full GC来让老年代腾出更多空间。</p>
</blockquote>
<p>总结起来：新生代存在大量存活的对象，Survivor无法容纳这些对象，老年代要进行空间分配担保；担保前要判断自生有没有能力，如果没有能力就需要触发Full GC。</p>
<h4 id="1-5-方法区"><a href="#1-5-方法区" class="headerlink" title="1.5 方法区"></a>1.5 方法区</h4><h5 id="方法区定义"><a href="#方法区定义" class="headerlink" title="方法区定义"></a>方法区定义</h5><p>Java虚拟机规范中定义方法区是堆的一个逻辑部分。方法区存放一下信息：</p>
<ul>
<li>已经被虚拟机加载的类信息</li>
<li>常量</li>
<li>静态变量</li>
<li>即时编译器编译后的代码</li>
</ul>
<h5 id="方法区的特点"><a href="#方法区的特点" class="headerlink" title="方法区的特点"></a>方法区的特点</h5><ul>
<li>线程共享。方法区是堆的一个逻辑部分，因此和堆一样，都是线程共享的。整个虚拟机中只有一个方法区。</li>
<li>内存回收效率低。方法区中的信息一般需要长期存在，回收一遍之后可能只有少量信息无效。主要回收目标是：对常量池的回收；对类型的卸载</li>
</ul>
<h5 id="运行时常量池"><a href="#运行时常量池" class="headerlink" title="运行时常量池"></a>运行时常量池</h5><p>方法区中存放：<strong>类信息、常量、静态变量、即时编译器编译后的代码</strong>。常量就存放在运行时常量池中</p>
<h3 id="2-垃圾收集策略-amp-算法"><a href="#2-垃圾收集策略-amp-算法" class="headerlink" title="2. 垃圾收集策略&amp;算法"></a>2. 垃圾收集策略&amp;算法</h3><h4 id="2-1-判断对象是否存活"><a href="#2-1-判断对象是否存活" class="headerlink" title="2.1 判断对象是否存活"></a>2.1 判断对象是否存活</h4><ol>
<li><strong>引用计数法</strong></li>
</ol>
<blockquote>
<p>在对象头维护着一个counter计数器，对象被引用一次则计数器+1；若引用失效则计数器-1.当计数器为0时，就认为该对象无效了。</p>
<p>引用计数算法的实现简单，判定效率也很高，在大部分情况下它都是一个不错的算法。但是主流的 Java 虚拟机里没有选用引用计数算法来管理内存，主要是因为它很难解决对象之间循环引用的问题。</p>
<blockquote>
<p>举个栗子👉对象 objA 和 objB 都有字段 instance，令 objA.instance = objB 并且 objB.instance = objA，由于它们互相引用着对方，导致它们的引用计数都不为 0，于是引用计数算法无法通知 GC 收集器回收它们。</p>
</blockquote>
</blockquote>
<ol start="2">
<li><strong>可达性分析（GC Roots）</strong></li>
</ol>
<p>基本思路就是通过一些列名为”GC Roots“的对象作为起始点，开始向下搜索，如果一个对象到GCRoots没有任何引用链，就说明这个对象已经没有引用了，就可以作为垃圾。</p>
<p>也即给定一个集合的引用作为根出发，通过引用关系遍历对象图，能被遍历到的（可达到的）对象就被判定为存活，没有被遍历到的就自然判定为死亡对象。</p>
<h6 id="哪些对象可以作为GC-Roots"><a href="#哪些对象可以作为GC-Roots" class="headerlink" title="哪些对象可以作为GC Roots"></a>哪些对象可以作为GC Roots</h6><ul>
<li><strong>虚拟机栈中的引用的对象</strong></li>
<li><strong>方法区中的类静态属性引用的对象</strong></li>
<li><strong>方法区中的常量引用的对象</strong></li>
<li><strong>synchronized同步的对象</strong></li>
</ul>
<p><strong>GC Roots并不包括堆中对象引用的对象，这样就不会有循环引用的问题。</strong></p>
<h4 id="2-1-垃圾收集算法"><a href="#2-1-垃圾收集算法" class="headerlink" title="2.1 垃圾收集算法"></a>2.1 垃圾收集算法</h4><h6 id="标记-清除算法"><a href="#标记-清除算法" class="headerlink" title="标记-清除算法"></a>标记-清除算法</h6><p><img src="https://img-blog.csdnimg.cn/20200824152623936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>缺点：内存会产生碎片化</p>
<h6 id="标记-复制"><a href="#标记-复制" class="headerlink" title="标记-复制"></a>标记-复制</h6><p><img src="https://img-blog.csdnimg.cn/20200824152644220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>缺点：预留一半的内存区域；整个内存空间只有一半可以使用</p>
<h6 id="标记-整理"><a href="#标记-整理" class="headerlink" title="标记-整理"></a>标记-整理</h6><p><img src="https://img-blog.csdnimg.cn/20200824152724174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>缺点：标记整理虽然可以解决内存碎片化问题，也不存在内存空间浪费，但是需要移动存活的对象，但是，当内存中存活对象多，并且都是一些微小对象，而且垃圾对象较少时，要移动大量的存活对象才能换取少量的内存空间。</p>
<h6 id="分代收集算法"><a href="#分代收集算法" class="headerlink" title="分代收集算法"></a>分代收集算法</h6><p>一块独立的内存区域只能使用一种垃圾回收算法，根据对象生命周期特征，将其划分到不同的区域，再对特定区域使用特定的垃圾回收算法，只有这样才能将垃圾回收算法的优点发挥到极致，这种组合的垃圾回收算法称之为：分代收集算法（分代回收算法）</p>
<p>根据对象存活周期的不同，将内存划分为几块。一般是把 Java 堆分为新生代和老年代，针对各个年代的特点采用最适当的收集算法。</p>
<ul>
<li>新生代：复制算法</li>
<li>老年代：标记-清除算法、标记-整理算法</li>
</ul>
<h3 id="3-HotSpot垃圾收集器（7种）"><a href="#3-HotSpot垃圾收集器（7种）" class="headerlink" title="3. HotSpot垃圾收集器（7种）"></a>3. HotSpot垃圾收集器（7种）</h3><h4 id="3-1-新生代垃圾收集器"><a href="#3-1-新生代垃圾收集器" class="headerlink" title="3.1 新生代垃圾收集器"></a>3.1 新生代垃圾收集器</h4><h5 id="1-Serial-GC收集器"><a href="#1-Serial-GC收集器" class="headerlink" title="1. Serial GC收集器"></a>1. Serial GC收集器</h5><p><img src="https://img-blog.csdnimg.cn/img_convert/1c13b8e41120caccd15369497355b588.png" alt="1c13b8e41120caccd15369497355b588.png"></p>
<blockquote>
<p>单线程，只会使用一个cpu或一条线程去完成垃圾收集工作，这也意味着在进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束为止（<font color=red>臭名昭著的Stop The World</font>）</p>
<p><strong>收集算法</strong>：复制算法</p>
<p>与用户线程串行执行，单线程地好处就是减少上下文切换，减少系统资源的开销。但这种方式的缺点也很明显，在GC的过程中，会暂停程序的执行。若GC不是频繁发生，这或许是一个不错的选择，否则将会影响程序的执行性能。 对于新生代来说，区域比较小，停顿时间短，所以比较使用。</p>
<p><strong>参数</strong>：<code>-XX:+UseSerialGC</code> </p>
<p>在JDK Client模式，不指定JVM参数，默认是串行垃圾收集器</p>
</blockquote>
<h5 id="2-ParNew收集器"><a href="#2-ParNew收集器" class="headerlink" title="2. ParNew收集器"></a>2. ParNew收集器</h5><p>ParNew收集器是Serial GC的多线程版本，除了使用多线程进行垃圾收集外，其余行为包括Serial收集器可用的所有控制参数、收集算法(复制算法)、Stop The World、对象分配规则、回收策略等与Serial收集器完全相同，两者共用了相当多的代码。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/111fbad04e83f6fc486c78406621ae05.png" alt="111fbad04e83f6fc486c78406621ae05.png"></p>
<p>ParNew收集器除了使用了多线程收集外，其他与Serial收集器相比并无太多创新之外，但是它是许多运行在Server模式下的虚拟机首选的新生代收集器，其中有一个与性能无关的重要原因是，除了Serial收集器外，目前只有它能和CMS收集器配合工作。</p>
<blockquote>
<p><strong>算法</strong>：复制算法</p>
<p>用于新生代</p>
<p>GC时需要暂停所有用户线程，直到GC结束</p>
<p><strong>参数</strong>：</p>
<p>​    <code>-XX:+UseConcMarkSweepGC</code>：指定使用CMS后，会默认使用ParNew作为新生代收集器</p>
<p>​    <code>-XX:+UseParNewGC</code>：强制指定使用ParNew</p>
<p>​    <code>-XX:ParallelGCThreads</code>：指定垃圾收集的线程数量，ParNew默认开启的收集线程与CPU的数量相同</p>
</blockquote>
<h5 id="3-Parallel-Scavenge收集器（吞吐量优先）"><a href="#3-Parallel-Scavenge收集器（吞吐量优先）" class="headerlink" title="3. Parallel Scavenge收集器（吞吐量优先）"></a>3. Parallel Scavenge收集器（吞吐量优先）</h5><p>Parallel收集器同样也采用了复制算法，并行回收和STW机制；和ParNew不同之处在于，Parallel收集器的目标则是达到一个可控制的吞吐量，也被称为吞吐量优先的垃圾收集器。</p>
<p><img src="https://img-blog.csdnimg.cn/20210128165345527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3c3MzM1MTIz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<blockquote>
<p>参数：</p>
<p><code>-XX:MaxGCPauseMillis</code> 设置最大停顿时间STW，这参数设置的越小，停顿时间可能会缩短，但也会导致吞吐量下降，当值垃圾收集发生的更频繁。</p>
<p><code>-XX:GCtimeRatio</code> 垃圾收集时间占时间总比   用于衡量吞吐量</p>
<p>​    垃圾收集执行时间占应用程序执行时间的比例计算方法：<code>1/(n+1)</code></p>
<p>​    例如<code>-XX:GCTimeRatio=19</code>,那么设置了垃圾收集时间占总时间的5% = 1/(19+1);</p>
<p>​    默认值是99，即1%；</p>
<p><code>-XX:UseAdaptiveSizePolicy</code> 设置Parallel收集器具有自适应调节功能；开启这个参数后，就不用手工指定一些细节参数了，如<code>新生代大小 -Xmn</code>、<code>Eden与Survivor区的比例 -XX:SurvivorRation</code>、<code>晋升老年代的对象年龄 -XX:MaxTenuringThreshold</code></p>
</blockquote>
<p>JVM会根据当前系统运行情况收集性能监控信息，动态调整这些参数，以提供最合适的停顿时间或最大的吞吐量，这种调节方式称为GC自适应的调节策略(GC Ergonomiscs)；</p>
<p>另外值得注意的一点是，Parallel Scavenge收集器无法与CMS收集器配合使用，所以在JDK 1.6推出Parallel Old之前，如果新生代选择Parallel Scavenge收集器，老年代只有Serial Old收集器能与之配合使用。</p>
<h4 id="3-2-老年代垃圾收集器"><a href="#3-2-老年代垃圾收集器" class="headerlink" title="3.2 老年代垃圾收集器"></a>3.2 老年代垃圾收集器</h4><h5 id="1-Serial-Old收集器"><a href="#1-Serial-Old收集器" class="headerlink" title="1. Serial Old收集器"></a>1. Serial Old收集器</h5><p><img src="https://img-blog.csdnimg.cn/img_convert/abdedea73525f71775a338dbceeedd2a.png" alt="abdedea73525f71775a338dbceeedd2a.png"></p>
<blockquote>
<p><strong>算法</strong>：标记-整理</p>
<p>可作为CMS收集器的后备预案，并在CMS发生”Concurrent Mode Failure“ 时使用。</p>
</blockquote>
<h5 id="2-Parallel-Old"><a href="#2-Parallel-Old" class="headerlink" title="2. Parallel Old"></a>2. Parallel Old</h5><p><img src="https://img-blog.csdnimg.cn/img_convert/e1b908c08120b3323a3d5ca408bc569b.png" alt="e1b908c08120b3323a3d5ca408bc569b.png"></p>
<p>Parallel Scavenge收集器的老年代版本，并行收集器，吞吐量优先</p>
<blockquote>
<p><strong>算法</strong>：标记-整理</p>
<p><strong>参数</strong>：<code>-XX:UseparallelOldGC</code> 指定使用Parallel Old收集器</p>
</blockquote>
<h5 id="3-CMS并发清除（Concurrent-Mark-Sweep）"><a href="#3-CMS并发清除（Concurrent-Mark-Sweep）" class="headerlink" title="3. CMS并发清除（Concurrent Mark Sweep）"></a>3. CMS并发清除（Concurrent Mark Sweep）</h5><p>这个收集器有与工作线程执行<strong>并发</strong>的能力。</p>
<blockquote>
<p><strong>算法</strong>：标记-清除</p>
<p><strong>特点</strong>：收集过程中不需要暂停用户线程，以获取最短回收停顿时间为目标</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/img_convert/ae5a11b458e117b8f30fdc064821647c.png" alt="ae5a11b458e117b8f30fdc064821647c.png"></p>
<p>CMS GC过程分四步：</p>
<ol>
<li><p><strong>初始标记</strong>（initial mark）</p>
<blockquote>
<p>单线程执行， 需要STW，但仅仅把GC Roots的直接关联可达的对象给标记一下，由于直接关联对象比较小，所以这里的速度非常快。</p>
</blockquote>
</li>
<li><p><strong>并发标记</strong>（Concurrent mark）</p>
<blockquote>
<p>对于初识标记过程所标记的初识标记对象，进行并发跟踪标记</p>
<p>此时其他线程仍可以继续工作。此处时间较长，但不停顿，并不能保证可以标记出所有的存活对象；</p>
<p><strong>与用户线程并发执行</strong></p>
</blockquote>
</li>
<li><p><strong>重新标记</strong>（remark）</p>
<blockquote>
<p>在并发标记的过程中，由于可能还会产生新的垃圾，所以此时需要重新标记新产生的垃圾。</p>
<p>此处执行<strong>并行标记</strong>，与用户线程不并发，所以依然是STW</p>
<p>且停顿时间比初识标记稍长，但远比并发标记短。</p>
</blockquote>
</li>
<li><p><strong>并发清除</strong>（Concurrent sweep）</p>
<blockquote>
<p> 并发清除之前所有标记的垃圾；</p>
<p> 其他用户线程仍可以工作，不需要停顿。</p>
<p> <strong>与用户线程并发执行</strong></p>
</blockquote>
</li>
</ol>
<p>Tips：初始标记和重新标记仍然需要STW</p>
<blockquote>
<p>并发的意思是指与用户线程是否是并发执行的</p>
</blockquote>
<p><strong>初始标记</strong>仅仅标记一下GC Roots能直接关联到的对象，速度很快；</p>
<p><strong>并发标记</strong>就是进行GC Roots Tracing的过程；</p>
<p>而<strong>重新标记</strong>阶段则是为了修正并发标记期间因用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段长，但远比并发标记的时间短。</p>
<p>由于整个过程中耗时最长的并发标记和并发清除过程，收集器线程都可以与用户线程一起工作，所以整体上说，CMS收集器的内存回收过程是与用户线程一共并发执行的。</p>
<blockquote>
<p><strong>参数</strong>：</p>
<p><code>-XX:+UseConcMarkSweepGC</code>：使用CMS收集器</p>
<p><code>-XX:+UseCMSCompactAtFullCollection</code>：Full GC后，进行一次碎片整理；整理过程是独占的，会引起停顿时间变长。</p>
<p><code>-XX:+CMSFullGCsBeforeCompaction</code>：设置进行几次Full GC后，进行一次碎片整理</p>
<p><code>-XX:ParallelCMSThreads</code>：设置CMS的线程数量（一般情况约等于可用CPU数量）</p>
</blockquote>
<p><strong>优点</strong>：</p>
<p>总体来说，与Parallel Old垃圾收集器相比，CMS减少了执行老年代垃圾收集时应用暂停的时间；但却增加了新生代垃圾收集时应用暂停的时间，降低了吞吐量而且需要占用更大的堆空间；</p>
<p>由于耗时的<strong>并发标记</strong>和<strong>并发清除</strong>阶段都不需要暂停工作，所以整体的回收是低停顿的。</p>
<p>由于CMS以上特性，缺点也是比较明显的。</p>
<p><strong>缺点</strong>：</p>
<ol>
<li><p>对CPU资源非常敏感</p>
</li>
<li><p>浮动垃圾</p>
<p>由于CMS并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS无法在当次收集中处理掉它们，只好留待下一次GC时再清理掉。这一部分垃圾就称为“浮动垃圾”。</p>
<p>由于在垃圾收集阶段用户线程还需要运行，那就还需要预留有足够的内存空间给用户线程使用，因此CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，也可以认为CMS所需要的空间比其他垃圾收集器大；</p>
<p><code>-XX:CMSInitiatingOccupancyFraction</code>: 设置CMS预留内存空间</p>
</li>
<li><p>”Concurrent Mode Failure“失败</p>
<p>如果如果CMS运行期间预留的内存无法满足程序需要，就会出现一次“Concurrent Mode Failure”失败，这时虚拟机将启动后备预案：临时启用Serial Old收集器来重新进行老年代的垃圾收集，这样会导致另一次Full GC的产生。这样停顿时间就更长了，代价会更大，所以 <code>-XX:CMSInitiatingOccupancyFraction</code>不能设置得太大。</p>
</li>
<li><p>产生大量内存碎片</p>
<p>这个问题并不是CMS的问题，而是算法的问题。由于CMS基于”标记-清除”算法，清除后不进行压缩操作，所以会产生碎片</p>
<p>“标记-清除”算法介绍时曾说过：</p>
<p>产生大量不连续的内存碎片会导致分配大内存对象时，无法找到足够的连续内存，从而需要提前触发另一次Full GC动作。</p>
<p><strong>碎片解决方法：</strong></p>
<ul>
<li><p> <code>-XX:+UseCMSCompactAtFullCollection</code></p>
</li>
<li><p>使得CMS出现上面这种情况时不进行Full GC，而开启内存碎片的合并整理过程；但合并整理过程无法并发，停顿时间会变长；</p>
</li>
<li><p><code>-XX:+CMSFullGCsBeforeCompation</code></p>
<ul>
<li><p>设置执行多少次不压缩的Full GC后，来一次压缩整理</p>
</li>
<li><p>为减少合并整理过程的停顿时间；</p>
<p>默认为0，也就是说每次都执行Full GC，不会进行压缩整理；</p>
<p>由于空间不再连续，CMS需要使用可用”空闲列表”内存分配方式，这比简单使用”碰撞指针”分配内存消耗大；</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="3-3-G1收集器"><a href="#3-3-G1收集器" class="headerlink" title="3.3 G1收集器"></a>3.3 G1收集器</h4><p>G1(Garbage - First)名称的由来是G1跟踪各个Region里面的垃圾堆的价值大小(回收所获得的空间大小以及回收所需时间的经验值)，在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region。</p>
<p>注意：G1与前面的垃圾收集器有很大不同，它把新生代、老年代的划分取消了！</p>
<p>这样我们再也不用单独的空间对每个代进行设置了，不用担心每个代内存是否足够。</p>
<p>取而代之的是，G1算法将堆划分为若干个区域(Region)，它仍然属于分代收集器。不过，这些区域的一部分包含新生代，新生代的垃圾收集依然采用暂停所有应用线程的方式，将存活对象拷贝到老年代或者Survivor空间。老年代也分成很多区域，G1收集器通过将对象从一个区域复制到另外一个区域，完成了清理工作。这就意味着，在正常的处理过程中，G1完成了堆的压缩(至少是部分堆的压缩)，这样也就不会有CMS内存碎片问题的存在了。<br><img src="https://img-blog.csdnimg.cn/img_convert/d278aed530716ea4981a02fb8590c946.png" alt="d278aed530716ea4981a02fb8590c946.png"></p>
<p>G1收集器运作过程</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/b03dfc84f0de89fe474936cbd70aef32.png" alt="b03dfc84f0de89fe474936cbd70aef32.png"></p>
<ol>
<li><p>初识标记</p>
<ul>
<li>初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快</li>
</ul>
</li>
<li><p>并发标记</p>
<ul>
<li><p>进行GC Roots Tracing的过程，从刚才产生的集合中标记出存活对象；(也就是从GC Roots 开始对堆进行可达性分析，找出存活对象。)</p>
<p>耗时较长，但应用程序也在运行；</p>
<p>并不能保证可以标记出所有的存活对象；</p>
</li>
</ul>
</li>
<li><p>最终标记</p>
<ul>
<li><p>最终标记和CMS的重新标记阶段一样，也是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，</p>
<p>这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短，</p>
<p>也需要“Stop The World”。(修正Remebered Set)</p>
</li>
</ul>
</li>
<li><p>筛选回收</p>
<ul>
<li>首先排序各个Region的回收价值和成本</li>
<li>然后根据用户期望的GC停顿时间来制定回收计划；</li>
<li>最后按计划回收一些价值高的Region中垃圾对象</li>
</ul>
<p>回收时采用”复制算法“，从一个或多个Region复制存活对象到堆上的另一个空间Region，并且在此过程中压缩和释放内存；</p>
<p>可以并发进行，降低停顿时间，并增加吞吐量</p>
<blockquote>
<p><strong>参数</strong>：</p>
<p><code>-XX:+UseG1GC</code>：指定使用G1收集器</p>
<p><code>-XX:InitiatingHeapOccupancyPercent</code>: 当整个Java堆的占用率达到参数值时，开始并发标记阶段；默认为45；</p>
<p><code>-XX:MaxGCPauseMillis</code>： 为G1设置暂停时间目标，默认值为200毫秒；</p>
<p><code>-XX:G1HeapRegionSize</code>：设置每个Region大小，范围1MB到32MB；目标是在最小Java堆时可以拥有约2048个</p>
</blockquote>
</li>
</ol>
<h4 id="小结："><a href="#小结：" class="headerlink" title="小结："></a>小结：</h4><p><img src="https://img-blog.csdnimg.cn/img_convert/8d9b5b42d191a4ad452074f204507378.png" alt="8d9b5b42d191a4ad452074f204507378.png"></p>
<h3 id="2-类加载过程"><a href="#2-类加载过程" class="headerlink" title="2.类加载过程"></a>2.类加载过程</h3><p>类从被加载到虚拟机内存中开始，到卸载出内存位置，他的整个生命周期如下如：</p>
<p><img src="https://img-blog.csdnimg.cn/20200810150148636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E2NDY3MDU4MTY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<ol>
<li><strong>加载</strong><ul>
<li>通过一个类的全限定名来获取定义此类的二进制字节流</li>
<li>将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构</li>
<li>在Java堆中生成一个代表这个类的java.lang.Class对象，作为方法区数据的访问入口</li>
</ul>
</li>
<li><strong>验证</strong><ul>
<li>验证阶段作用是保证Class文件的字节流包含的信息是否符合JVM规范，不会给JVM造成危害。如果验证失败，就会抛出一个java.lang.VerifyError异常或子类异常。验证过程分为四个阶段：<ul>
<li>文件格式验证：验证字节流文件是否符合Class文件格式的规范，并且能被当前虚拟机正确的处理</li>
<li>元数据验证：是对字节码描述的信息进行语义分析，以保证其描述信息符合Java语言的规范</li>
<li>字节码验证：主要是进行数据流和控制流的分析，保证被校验类的方法在运行时不会危害JVM</li>
<li>符号引用验证：符号引用验证发生在虚拟机将符号引用转化为直接引用的时候，这个转化动作将在解析阶段中发生。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><font color=green>符号引用（Symbolic References）说明</font></p>
<blockquote>
<p>符号引用以一组符号来描述所引用的目标，符号引用与虚拟机的内存布局无关，引用的目标不一定加载到内存中。</p>
<p><font color=blue>在java中，一个java类将会编译成一个class文件。在编译时，java类并不知道所引用的类的实际地址，因此只能使用符号引用来代替</font></p>
<p>比如org.simple.People类引用了org.simple.Language类，在编译时People类并不知道Language类的实际内存地址，因此只能使用符号org.simple.Language（假设是这个，当然实际中是由类似于CONSTANT_Class_info的常量来表示的）来表示Language类的地址。各种虚拟机实现的内存布局可能有所不同，但是它们能接受的符号引用都是一致的，因为符号引用的字面量形式明确定义在Java虚拟机规范的Class文件格式中。</p>
</blockquote>
<ol start="3">
<li><p><strong>准备</strong></p>
<ul>
<li><p>准备阶段为变量分配内存并设置类变量的初始化。在这个阶段分配的仅为类的变量（static修饰的变量），而不包括类的实例变量（实例变量在new 的时候初始化）。对已非final的变量，JVM会将其设置成”零值“，而不是其赋值语句的值：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">int</span> <span class="variable">size</span> <span class="operator">=</span> <span class="number">12</span>;</span><br></pre></td></tr></table></figure>

<p>那么这个阶段，size的值为0，而不是12。final修饰的类变量将会赋值成真实的值。</p>
</li>
</ul>
</li>
<li><p><strong>解析</strong></p>
<ul>
<li>解析过程是将常量池内的<strong>符号引用</strong>替换成<strong>直接引用</strong>。主要包括四种类型引用的解析。<strong>类或接口的解析</strong>、<strong>字段解析</strong>、<strong>方法解析</strong>、<strong>接口方法解析</strong>。</li>
</ul>
</li>
<li><p><strong>初始化</strong></p>
<ul>
<li>在准备阶段，类变量已经经过一次初始化了，在这个阶段，则是根据程序员通过程序制定的计划去初始化类的变量和其他资源。这些资源有static{}块，构造函数，父类的初始化等。</li>
</ul>
</li>
<li><p><strong>使用</strong></p>
</li>
<li><p><strong>卸载</strong></p>
</li>
</ol>
<h3 id="3-双亲委派机制"><a href="#3-双亲委派机制" class="headerlink" title="3. 双亲委派机制"></a>3. 双亲委派机制</h3><p><strong>java中的四种类加载器</strong></p>
<ol>
<li><p>启动（Bootstrap）类加载器</p>
<blockquote>
<p>启动类加载器是本地代码实现的类加载器，它负责将<JavaRuntimeHome>/lib下面的类库加载到内存中。由于启动类加载器涉及到虚拟机本地实现细节，开发者无法直接取到启动类加载器的引用。</p>
</blockquote>
</li>
<li><p>标准扩展（Extension）类加载器</p>
<blockquote>
<p>扩展类加载器负责将<JavaRuntimeHome>/lib/ext或者系统变量java.ext.dir指定位置中的类库加载到内存中。开发者可以直接使用标准扩展类加载器</p>
</blockquote>
</li>
<li><p>应用程序（Application）类加载器</p>
<blockquote>
<p>应用程序类加载器负责加载用户路径（classpath）上的类库</p>
</blockquote>
</li>
</ol>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWcyMDIwLmNuYmxvZ3MuY29tL2Jsb2cvMjAwNDkzNC8yMDIwMDcvMjAwNDkzNC0yMDIwMDcyOTEyNDgyNjk0MC0xMDQyODAzODI0LnBuZw?x-oss-process=image/format,png#pic_center" alt="在这里插入图片描述"></p>
<p><strong>双亲委派机制</strong></p>
<blockquote>
<p>当一个类收到类加载请求时，它首先不会尝试自己去加载这个类，而是把这个请求<code>委派给父类加载器</code>去完成，每一层的类加载器都是如此，因此所有的加载请求都应该传送到启动类加载器中，只有当父类加载器反馈自己无法完成这个请求的时候（在它的加载路径下没有找到所需加载的Class）（从最顶层的BootStrap -》Extension-》Application-》自己定义的类加载器），子类加载器才会尝试自己加载。</p>
</blockquote>
<p><strong>双亲委派机制的作用</strong></p>
<blockquote>
<p>为了保证自己写的代码不污染java出厂自带的源代码。如果有人想替换系统级别的类：如String.java.篡改它的实现，但是在这种机制下这些系统的类已经被Bootstrap ClassLoader加载过了，所以并不会再去加载，从一定程度上<strong>防止了危险代码的植入</strong>。</p>
<ol>
<li><strong>防止重复加载用一个.class。通过委托去上层，加载过了，就不用再加载一遍</strong>。保证数据安全</li>
<li>保证了使用不同的类加载器最终得到的都是同一个Object对象。</li>
</ol>
</blockquote>
<h3 id="3-创建（new）对象的过程"><a href="#3-创建（new）对象的过程" class="headerlink" title="3.创建（new）对象的过程"></a>3.创建（new）对象的过程</h3><ol>
<li><strong>检查类是否已经被加载</strong><ul>
<li>当JVM遇到一条字节码new指令时，首先检查该引用指向的类是否能够在常量池中被找到（也就是检查方法区中有没有该类的信息），如果没有，先加载这个类；有的话就执行下一步，为对象分配内存</li>
</ul>
</li>
<li><strong>为对象分配内存空间</strong><ul>
<li>类加载检查通过后，接下来虚拟机会为对象分配内存。对象需要多大的内存在类加载完成后便可完全确定，为对象分配内存就是把一块确定大小的内存块从堆上划分出来。</li>
</ul>
</li>
<li><strong>为对象字段设置零值</strong><ul>
<li>分配完内存后，需要对对象的字段进行零值初始化，（也就是对象的实例数据部分，对象的内存布局被分为三个部分：<strong>对象头</strong>、<strong>实例数据</strong>、<strong>对齐填充</strong>），对象头除外，零值初始化意思就是对对象的字段赋0值，或者null值。</li>
</ul>
</li>
<li><strong>设置对象头</strong><ul>
<li>虚拟机需要对这个将要创建出来的对象，进行信息标记，包括是否为新生代/老年代，对象的hash码，元数据信息，这些标记存放在对象头信息中。</li>
</ul>
</li>
<li><strong>执行构造方法</strong><ul>
<li>执行对象的构造方法，初始化对象，这样一个对象才算被成功创建。</li>
</ul>
</li>
</ol>
<h3 id="4-对象的内存布局"><a href="#4-对象的内存布局" class="headerlink" title="4.对象的内存布局"></a>4.对象的内存布局</h3><blockquote>
<p>提问：<code>Object o = new Object();</code> 请问一个object对象占多少内存空间？</p>
</blockquote>
<p>Java对象的内存布局：<strong>对象头（Header）</strong>、<strong>实例数据（Instance Data）</strong>和<strong>对齐填充（Padding）</strong>，<strong>8字节对齐</strong>。</p>
<p><img src="https://img-blog.csdnimg.cn/20201221191858529.png" alt="在这里插入图片描述"></p>
<ul>
<li><strong>Mark Word</strong><ul>
<li>存储对象的hashCode、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳</li>
</ul>
</li>
<li><strong>Class Pointer</strong><ul>
<li>指向对象对应的Class对象（类对象）的内存地址</li>
</ul>
</li>
<li><strong>Instance Date</strong><ul>
<li>具体的数据大小，如对象含有一个int成员变量，即为4字节</li>
</ul>
</li>
<li><strong>Padding</strong><ul>
<li>8字节对齐</li>
</ul>
</li>
</ul>
<p>在64bit的JVM中，MarkWord为64bit-8字节，这样一个<code>Object</code>对象占有<strong>16个字节</strong></p>
<h3 id="5-四大引用类型"><a href="#5-四大引用类型" class="headerlink" title="5. 四大引用类型"></a>5. 四大引用类型</h3><h4 id="1-强引用"><a href="#1-强引用" class="headerlink" title="1. 强引用"></a>1. 强引用</h4><p><code>StrongReference</code>是java的默认引用形式，使用时不需要显示定义。任何通过强引用所使用的对象，不管jvm内存是否充足，Java GC都不会主动回收具有强引用的对象。</p>
<blockquote>
<p>如果一个对象具有强引用，那么垃圾收集器不会回收它</p>
<p>当JVM内存空间不足时，Java虚拟机宁愿抛出OurOfMemoryError错误，使程序异常终止，也不会回收强引用对象。</p>
</blockquote>
<h4 id="2-软引用"><a href="#2-软引用" class="headerlink" title="2. 软引用"></a>2. 软引用</h4><p><code>SoftReference&lt;String[]&gt; softArr = new SoftReference&lt;String[]&gt;(new String[] &#123;&quot;a&quot;, &quot;b&quot;, &quot;c&quot;&#125;);</code></p>
<p>软引用在内存充足时，GC不会回收；如果内存不足时，GC会回收这个对象。</p>
<p><strong>应用场景</strong></p>
<blockquote>
<p>实现内存敏感的高速缓存，比如网页缓存，图片缓存等。使用软引用能防止内存泄漏</p>
</blockquote>
<h4 id="3-弱引用"><a href="#3-弱引用" class="headerlink" title="3. 弱引用"></a>3. 弱引用</h4><p><code>WeakReference&lt;String[]&gt; weakArr=new WeakReference&lt;String[]&gt;(new String[]&#123;&quot;a&quot;,&quot;b&quot;,&quot;c&quot;&#125;);</code></p>
<p>如果一个对象只具有弱引用，无论内存充足与否，Java GC后对象都会被回收。</p>
<p><strong>应用场景</strong></p>
<blockquote>
<p>ThreadLocal</p>
</blockquote>
<h5 id="ThreadLocal-弱引用造成的数据泄漏问题"><a href="#ThreadLocal-弱引用造成的数据泄漏问题" class="headerlink" title="ThreadLocal 弱引用造成的数据泄漏问题"></a>ThreadLocal 弱引用造成的数据泄漏问题</h5><p><code>ThreadLocalMap</code>内部Entry类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Entry</span> <span class="keyword">extends</span> <span class="title class_">WeakReference</span>&lt;ThreadLocal&lt;?&gt;&gt; &#123;</span><br><span class="line">  <span class="comment">/** The value associated with this ThreadLocal. */</span></span><br><span class="line">  Object value;</span><br><span class="line"></span><br><span class="line">  Entry(ThreadLocal&lt;?&gt; k, Object v) &#123;</span><br><span class="line">    <span class="built_in">super</span>(k);</span><br><span class="line">    value = v;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看出，ThreadLocal内部每个线程维护的本地变量map中的Entry的<font color=red>key是弱引用类型<code>WeakReference</code>,不管JVM内存空间是否充足，在GC的时候，都会回收里面的key</font>。</p>
<p><font color=red>但是value依然是强引用类型</font>，这就会造成这种情况：GC回收的时候，把key进行了回收，变为了<code>null</code>,但是其对应的value还有值存在，但是无法被引用到了，这就造成了**<code>内存泄漏</code>**,因此，在实际使用ThreadLocal的过程中，使用完毕后需要及时调用<code>remove()</code>方法，避免造成数据泄漏。</p>
<h4 id="4-虚引用"><a href="#4-虚引用" class="headerlink" title="4. 虚引用"></a>4. 虚引用</h4><p>虚引用需要配合引用队列<code>ReferenceQueue</code>联合使用。当执行Java GC时如果一个对象只有虚引用，就会把这个对象加入到与之关联的<code>ReferenceQueue</code>中。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//虚引用PhantomReference</span></span><br><span class="line">    <span class="comment">//必须和引用队列联合使用</span></span><br><span class="line">    ReferenceQueue&lt;String[]&gt; rqueue = <span class="keyword">new</span> <span class="title class_">ReferenceQueue</span>&lt;&gt;();</span><br><span class="line">    PhantomReference&lt;String[]&gt; phanArr = <span class="keyword">new</span> <span class="title class_">PhantomReference</span>&lt;String[]&gt;(<span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>&#125;,rqueue);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 应用场景：</span></span><br><span class="line"><span class="comment">     *大多被用于引用销毁前的处理工作</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     */</span></span><br></pre></td></tr></table></figure>

<p>当垃圾回收期准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。</p>
<p>程序可以通过判断引用队列中是否已经加入了虚引用，来了解引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列中，那么就可以在所引用的对象的内存被回收之前采取必要的行动。</p>
<h3 id="6-JVM常用调优参数"><a href="#6-JVM常用调优参数" class="headerlink" title="6. JVM常用调优参数"></a>6. JVM常用调优参数</h3><p><a target="_blank" rel="noopener" href="https://www.oracle.com/java/technologies/javase/vmoptions-jsp.html">Oracle关于JVM参数配置参考表</a></p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>功能</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td><code>-Xms</code></td>
<td>初始堆大小。如<code>-Xms256m</code></td>
<td></td>
</tr>
<tr>
<td><code>-Xmx</code></td>
<td>最大堆大小。如<code>-Xmx1024m</code></td>
<td></td>
</tr>
<tr>
<td><code>-Xmn</code></td>
<td>新生代大小。通常为<code>Xmx</code>的1/3或1/4.<br />新生代=Eden+2个Survivor空间。<br />实际可用空间为=Eden+1个Survivor，即90%</td>
<td></td>
</tr>
<tr>
<td><code>-Xss</code></td>
<td>每个线程堆栈大小，默认为1M</td>
<td></td>
</tr>
<tr>
<td><code>-XX:NewRatio</code></td>
<td>老年代/新生代的比例，默认<code>-XX:NewRatio=2</code>,代表老年代：新生代=2：1</td>
<td></td>
</tr>
<tr>
<td><code>-XX:SurvivorRatio</code></td>
<td>新生代中Eden与Survivor的比值。默认值为8  <code>-XX:SurvivorRatio=8</code></td>
<td></td>
</tr>
<tr>
<td><code>java -XX:+PrintFlagsFinal -version</code></td>
<td>查看jvm所有参数选项的值</td>
<td></td>
</tr>
<tr>
<td><code>-XX:MaxTenuringThreshold</code></td>
<td>新生代晋升老年的的年龄<br />默认值<code>-XX:MaxTenuringThreshold=15</code></td>
<td></td>
</tr>
<tr>
<td><code>-XX:MetaspaceSize</code></td>
<td>元空间大小</td>
<td></td>
</tr>
<tr>
<td><code>-XX:MaxMetaspaceSize</code></td>
<td>元空间最大空间大小</td>
<td></td>
</tr>
<tr>
<td><code>-XX:PretenureSizeThreshold</code></td>
<td>大对象所占空间超过这个阈值，直接分配到老年代</td>
<td></td>
</tr>
<tr>
<td><code>-XX:+PrintGCDetails</code></td>
<td>打印GC信息</td>
<td></td>
</tr>
<tr>
<td><code>关于设置垃圾收集器</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>-XX:+UseSerialGC</code></td>
<td>新生代使用Serial GC， 老年代使用Serial old</td>
<td></td>
</tr>
<tr>
<td><code>-XX:+UseParNewGC</code></td>
<td>新生代使用ParNew收集器，老年代使用Serial Old</td>
<td></td>
</tr>
<tr>
<td><code>-XX:+UseConcMarkSweepGC</code></td>
<td>新生代使用ParNew收集器，老年代使用CMS</td>
<td></td>
</tr>
<tr>
<td><code>-XX:ParallelGCThreads=8</code></td>
<td>这个参数指定并行GC线程的数量，<br />一般最好和cpu核心数相当。</td>
<td></td>
</tr>
<tr>
<td><code>-XX:+UseParallelOldGC</code></td>
<td>新生代使用ParallelGC收集器，<br />老年代使用ParallelOldGC收集器</td>
<td></td>
</tr>
<tr>
<td><code>-XX:ConcGCThreads</code></td>
<td>设置CMS并发线程数</td>
<td></td>
</tr>
<tr>
<td><code>-XX:+UseG1GC</code></td>
<td>开启G1收集器</td>
<td></td>
</tr>
<tr>
<td><code>关于锁</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>-XX:+UseSpinning</code></td>
<td>启用自旋锁优化，jdk1.6之后默认开启</td>
<td></td>
</tr>
<tr>
<td><code>-XX:PreBlockSpin</code></td>
<td>设置自旋多少次后升级为重量级锁；默认<code>-XX:PreBlockSpin=10</code></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2 id="并发与多线程篇"><a href="#并发与多线程篇" class="headerlink" title="并发与多线程篇"></a>并发与多线程篇</h2><h3 id="1-进程与线程"><a href="#1-进程与线程" class="headerlink" title="1. 进程与线程"></a>1. 进程与线程</h3><p>进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。同时，每个进程还占有某些系统资源如CPU时间，内存空间，文件，输入输出设备的使用权等等。</p>
<p><strong>进程与进程之间的通信方式</strong></p>
<ol>
<li><strong>管道pipe</strong><ul>
<li>通常指无名管道，<strong>unix</strong>系统IPC最古老的形式</li>
<li>只能用于具有亲缘关系的进程之间的通信（父子进程，兄弟进程）</li>
</ul>
</li>
<li><strong>命名管道FIFO</strong><ul>
<li>在磁盘上有对应的节点，但是没有数据块。一旦建立，任何进程都可以通过文件名将其打开和进行读写，而不局限于父子进程，当然前提是进程对FIFO有适当的访问权。当不再被进程使用时，FIFO在内存中释放，但磁盘节点仍然存在。</li>
</ul>
</li>
<li><strong>消息队列MessageQueue</strong><ul>
<li>消息队列，就是一个消息的链表，是一系列保存在内核中消息的列表。用户进程可以向消息队列添加消息，也可以从消息队列读取消息。</li>
<li>消息队列与管道通信相比，其优势是对每个消息指定特定的消息类型，接收的时候不需要按照队列次序，而是可以根据自定义条件接收特定类型的消息。</li>
<li>进程间通过消息队列通信，主要是：创建或打开消息队列，添加消息，读取消息和控制消息队列</li>
</ul>
</li>
<li><strong>共享存储SharedMemory</strong><ul>
<li>共享内存允许两个或多个进程共享一个给定的存储区，这一段存储区可以被两个或两个以上的进程映射至自身的地址空间中，一个进程写入共享内存的信息，可以被其他使用这个共享内存的进程，通过一个简单的内存读取策略读出，从而实现了进程间的通信。</li>
<li>采用共享内存进行通信的一个主要好处是<strong>效率高</strong>，因为进程可以直接读写内存，而不需要任何数据的拷贝，对于像管道和消息队列等通信方式，则需要在内核和用户空间进行四次的数据拷贝，而共享内存则只拷贝两次：<code> 一次从输入文件到共享内存</code>和 <code>一次从共享内存输出文件</code></li>
</ul>
</li>
<li><strong>信号量Semaphore</strong><ul>
<li>信号量用于实现进程间的互斥与同步，而不是用于存储进程间通信数据。</li>
</ul>
</li>
<li><strong>套接字Socket</strong><ul>
<li>适合同一主机的不同进程间和不同主机的进程间进行全双工网络通信</li>
</ul>
</li>
<li>信号（sinal）</li>
</ol>
<h4 id="线程的几种状态"><a href="#线程的几种状态" class="headerlink" title="线程的几种状态"></a>线程的几种状态</h4><p><code>Thread.State</code>枚举类查看线程的各种状态</p>
<ol>
<li><strong>NEW（新建）</strong></li>
<li><strong>RUNNABLE（就绪）</strong></li>
<li><strong>BLOCKED（阻塞）</strong></li>
<li><strong>WAITING（等待）</strong></li>
<li><strong>TIMED_WAITING（超时等待）</strong></li>
<li><strong>TERMINATED（终止）</strong></li>
</ol>
<p><strong>wait/sleep都会导致线程的阻塞，有什么区别？</strong></p>
<ul>
<li><strong>wait</strong>放开手去睡，放开手里的锁</li>
<li><strong>sleep</strong>握紧手去睡，醒了手里还有锁</li>
</ul>
<h4 id="Java中实现多线的方式"><a href="#Java中实现多线的方式" class="headerlink" title="Java中实现多线的方式"></a>Java中实现多线的方式</h4><ol>
<li>继承Thread类，实现run方法</li>
<li>实现Runnable接口，实现run方法</li>
<li>实现Callable接口，实现call方法。注意：新建Thread的时候，Thread的构造方法中没有接收Callable的。（中间商赚差价！！！）所有我们需要找到一个既可以联系Runnable接口又联系Callable接口的类（FutureTask））<ul>
<li><code>FutureTask</code>中的<code>get()</code>方法会阻塞线程，一直等待线程计算完成后才执行下面的后续代码。一般放在最后。</li>
<li>同一个futureTask对象只能被线程调用一次，当有新的线程调用了已经被调用过的futuretask对象时，这次只会复用上一次的结果，不会再执行一次。</li>
</ul>
</li>
<li>线程池<code>ExecutorService</code>(ThreadPoolExecutor类)</li>
</ol>
<h3 id="2-JUC"><a href="#2-JUC" class="headerlink" title="2. JUC"></a>2. JUC</h3><h4 id="JUC强大的辅助类："><a href="#JUC强大的辅助类：" class="headerlink" title="JUC强大的辅助类："></a>JUC强大的辅助类：</h4><h5 id="1-CountDownLatch类"><a href="#1-CountDownLatch类" class="headerlink" title="1. CountDownLatch类"></a>1. CountDownLatch类</h5><p>计数器不为0，<code>countDownLatch.await(); </code>方法后面的代码都被一直阻塞</p>
<p>每调用一个线程，就需要执行<code>countDownLatch.countDown()</code>,将其计数器减一</p>
<h5 id="2-CyclicBarrier"><a href="#2-CyclicBarrier" class="headerlink" title="2. CyclicBarrier"></a>2. CyclicBarrier</h5><p>一句话：集齐七颗龙，召唤神龙。</p>
<p>没调用一个线程，就需要执行<code>cyclicBarrier.await()</code>,将计数器加1</p>
<p><strong>CyclicBarrier类与CountDownLatch类的区别</strong></p>
<table>
<thead>
<tr>
<th>CountDownLatch</th>
<th>CyclicBarrier</th>
</tr>
</thead>
<tbody><tr>
<td>减计数方式</td>
<td>加计数方式</td>
</tr>
<tr>
<td>计算为0时释放所有等待的线程</td>
<td>计数达到指定值时释放所有等待线程</td>
</tr>
<tr>
<td>计数为0时，无法重置</td>
<td>计数达到指定值时，计数置为0重新开始</td>
</tr>
<tr>
<td>调用countDown()方法计数减一，调用await()方法只进行阻塞，对计数没任何影响</td>
<td>调用await()方法计数加1，若加1后的值不等于构造方法的值，则线程阻塞</td>
</tr>
<tr>
<td>不可重复利用</td>
<td>可重复利用</td>
</tr>
</tbody></table>
<h5 id="信号量：Semaphore类（类似于PV操作）"><a href="#信号量：Semaphore类（类似于PV操作）" class="headerlink" title="信号量：Semaphore类（类似于PV操作）"></a>信号量：Semaphore类（类似于PV操作）</h5><p>两个关键性操作：</p>
<ol>
<li><code>semaphore.acquire()</code> 请求资源</li>
<li><code>semaphore.release() </code> 释放资源</li>
</ol>
<blockquote>
<p>acquire：当一个线程调用acquire操作时，它要么成功，获取信号量（信号量-1）；要么一直等待下去，直到有线程释放了信号量，或者超时</p>
<p>release：实际上会将信号量的值加1，然后唤醒等待的线程。</p>
</blockquote>
<p>信号量的主要作用：</p>
<blockquote>
<p>用于<strong>对多个共享资源的互斥使用</strong></p>
<p>用于<strong>并发线程数的控制</strong></p>
</blockquote>
<h3 id="阻塞队列（BlockingQueue）"><a href="#阻塞队列（BlockingQueue）" class="headerlink" title="阻塞队列（BlockingQueue）"></a>阻塞队列（BlockingQueue）</h3><p><img src="https://img-blog.csdnimg.cn/20200823081540511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>线程1往阻塞队列中生产元素，线程2 从阻塞队列中消费元素。</p>
<p>阻塞队列满了，生产线程阻塞；</p>
<p>阻塞队列空了，消费线程阻塞。</p>
<p><em><strong>阻塞队列的用处</strong></em></p>
<blockquote>
<p>在多线程领域：所谓阻塞，在某些情况下会 挂起 线程（即阻塞），一旦满足条件，被挂起的线程又会被自动唤醒。<br>为什么需要BlockingQueue，好处是我们不需要关心什么时候 需要阻塞线程，什么时候需要唤醒线程因为这一切BlockingQueue都给你一手包办了</p>
</blockquote>
<table>
<thead>
<tr>
<th>实现类</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><font color=red>ArrayBlockingQueue</font></td>
<td>由数组结构组成的有界阻塞队列</td>
</tr>
<tr>
<td><font color=red>LinkedBlockingQueue</font></td>
<td>由链表结构组成的有界（但大小默认值为integer.MAX_VALUE）阻塞队列</td>
</tr>
<tr>
<td><font color=red>SynchrousQueue</font></td>
<td>不存储元素的阻塞队列，也即单个元素的阻塞队列</td>
</tr>
<tr>
<td>PriorityBlockingQueue</td>
<td>支持优先级排序的无界阻塞队列</td>
</tr>
<tr>
<td>DelayQueue</td>
<td>使用优先级队列实现的延迟无界阻塞队列</td>
</tr>
<tr>
<td>LinkedTransferQueue</td>
<td>由链表组成的无界阻塞队列</td>
</tr>
<tr>
<td>LinkedBlockingDeque</td>
<td>由链表组成的双向阻塞队列</td>
</tr>
</tbody></table>
<h3 id="3-线程池"><a href="#3-线程池" class="headerlink" title="3. 线程池"></a>3. 线程池</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">ThreadPoolExecutor</span><span class="params">(</span></span><br><span class="line"><span class="params">  												// 常驻核心线程数</span></span><br><span class="line"><span class="params">  												<span class="type">int</span> corePoolSize,</span></span><br><span class="line"><span class="params">  												// 最大线程数</span></span><br><span class="line"><span class="params">                          <span class="type">int</span> maximumPoolSize,</span></span><br><span class="line"><span class="params">  												// 空闲线程的存活时间，</span></span><br><span class="line"><span class="params">                          <span class="type">long</span> keepAliveTime,</span></span><br><span class="line"><span class="params">  												// 时间单位</span></span><br><span class="line"><span class="params">                          TimeUnit unit,</span></span><br><span class="line"><span class="params">  												// 阻塞队列：用于存放被提交但尚未被执行的任务，类似于银行的候客区：窗口已经满了，需要排队等待</span></span><br><span class="line"><span class="params">                          BlockingQueue&lt;Runnable&gt; workQueue,</span></span><br><span class="line"><span class="params">  												// 线程池中工作线程的线程工厂，用于创建线程。</span></span><br><span class="line"><span class="params">                          ThreadFactory threadFactory,</span></span><br><span class="line"><span class="params">  												// 拒绝策略（阻塞队列满了，无法再容纳更多的线程任务）</span></span><br><span class="line"><span class="params">                          RejectedExecutionHandler handler)</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (corePoolSize &lt; <span class="number">0</span> ||</span><br><span class="line">      maximumPoolSize &lt;= <span class="number">0</span> ||</span><br><span class="line">      maximumPoolSize &lt; corePoolSize ||</span><br><span class="line">      keepAliveTime &lt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalArgumentException</span>();</span><br><span class="line">  <span class="keyword">if</span> (workQueue == <span class="literal">null</span> || threadFactory == <span class="literal">null</span> || handler == <span class="literal">null</span>)</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">NullPointerException</span>();</span><br><span class="line">  <span class="built_in">this</span>.acc = System.getSecurityManager() == <span class="literal">null</span> ?</span><br><span class="line">    <span class="literal">null</span> :</span><br><span class="line">  AccessController.getContext();</span><br><span class="line">  <span class="built_in">this</span>.corePoolSize = corePoolSize;</span><br><span class="line">  <span class="built_in">this</span>.maximumPoolSize = maximumPoolSize;</span><br><span class="line">  <span class="built_in">this</span>.workQueue = workQueue;</span><br><span class="line">  <span class="built_in">this</span>.keepAliveTime = unit.toNanos(keepAliveTime);</span><br><span class="line">  <span class="built_in">this</span>.threadFactory = threadFactory;</span><br><span class="line">  <span class="built_in">this</span>.handler = handler;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="线程池拒绝策略"><a href="#线程池拒绝策略" class="headerlink" title="线程池拒绝策略"></a>线程池拒绝策略</h4><table>
<thead>
<tr>
<th>拒绝策略</th>
<th>说明</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td><strong><code>ThreadPoolExecutor.AbortPolicy(默认)</code></strong></td>
<td>直接抛出RejectedExecutionException异常阻止系统正常运行</td>
<td></td>
</tr>
<tr>
<td><strong><code>ThreadPoolExecutor.CallerRunsPolicy</code></strong></td>
<td><strong>调用者运行</strong>机制：该策略既不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者，从而降低新任务的流量（谁让你找我的，你回去找谁去）</td>
<td></td>
</tr>
<tr>
<td><strong><code>ThreadPoolExecutor.DiscardOldestPolicy</code></strong></td>
<td>抛弃队列中等待最久的任务，然后把当前任务加入队列中尝试再次提交当前任务。</td>
<td></td>
</tr>
<tr>
<td><strong><code>ThreadPoolExecutor.DiscardPolicy</code></strong></td>
<td>该策略默默地丢弃无法处理的任务，不予任何处理也不抛出异常。如果任务允许丢失，这是最好的一种策略。</td>
<td></td>
</tr>
</tbody></table>
<h3 id="4-volatile-amp-JMM内存模型"><a href="#4-volatile-amp-JMM内存模型" class="headerlink" title="4. volatile&amp;JMM内存模型"></a>4. volatile&amp;JMM内存模型</h3><h4 id="JMM内存模型"><a href="#JMM内存模型" class="headerlink" title="JMM内存模型"></a>JMM内存模型</h4><img src="https://img-blog.csdnimg.cn/20200824153251847.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center#" alt="在这里插入图片描述" style="zoom:50%;" />

<blockquote>
<p>JMM是java的内存模型，JMM定义了程序中各个共享变量的访问规则，即在虚拟机中奖变量存储到内存和从内存读取变量这样的底层细节。</p>
<p>设计JMM主要的目的是：屏蔽各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。</p>
<p>由于JVM运行程序的实体是线程，而每个线程创建时JVM都会为其创建一个工作内存（有些地方成为栈空间），工作内存是每个线程私有的数据区域，而Java内存模型中规定所有变量都存储在主存中，主存是共享内存区域，所有线程都可以访问。</p>
<p>但是线程对变量的操作（读取赋值等）必须在工作内存中进行，首先要将变量先从主存拷贝到线程自己的工作内存空间，然后对变量进行操作，操作完成后再将变量写回主存。</p>
<p>不能直接操作主存中的变量，各个线程中的工作内存中存储着主存中的变量副本，因此不同线程间无法访问对方的工作内存，线程间的通信（传值）必须通过主存来完成。</p>
</blockquote>
<h4 id="volatile"><a href="#volatile" class="headerlink" title="volatile"></a>volatile</h4><h5 id="volatile特性："><a href="#volatile特性：" class="headerlink" title="volatile特性："></a>volatile特性：</h5><ol>
<li><p><strong>可见性</strong></p>
<ul>
<li><p>保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的</p>
</li>
<li><blockquote>
<p><strong>volatile底层实现可见性的原理</strong></p>
<p>以两核CPU为例（双核）</p>
<p>由于cpu的速度要比内存快的多，为了弥补这个性能差异，cpu内核都会有自己的高速缓冲区，当内核运行线程执行一段代码时，首先将这段代码的指令集进行缓存行填充到高速缓存，如果非volatile变量，当CPU执行修改了此变量之后，会将修改后的值回写到高速缓存，然后再刷新到内存中。如果刷新回内存之前，由于是共享变量，那么core2中的线程执行的代码也用到了这个变量，这时变量的值依然是旧的。</p>
<p>volatile关键字就会解决这个问题</p>
<blockquote>
<p>首先被volatile关键字修饰的共享变量在转换成汇编语言时，会加上一个lock为前缀的指令，当cpu发现这个指令时，立即做两件事：</p>
<ol>
<li><font color=green>将当前内核高速缓存行的数据立刻回写到内存；</font></li>
<li><font color=green>使其他内核里缓存了该内存地址的高速缓存中的数据无效。重写从主存中读取该数据</font></li>
</ol>
</blockquote>
</blockquote>
</li>
</ul>
</li>
<li><p><strong>禁止指令重排</strong></p>
<ul>
<li>volatile内存区的读写，通过加屏障来禁止指令重排列</li>
<li>LoadLoad屏障：对于这样的语句<code>Load1; LoadLoad; Load2</code>, 在<code>Load2</code>以及后续读取操作要读取的数据被访问前，要保证<code>Load1</code>要读取的数据被读取完毕。</li>
<li>StoreStore屏障：对于这样的语句<code>Store1;StoreStore;Store2</code>, 在<code>Store2</code>以及后续写入操作执行前，保证<code>Store1</code>的写入操作对其它处理器可见。</li>
<li>LoadStore屏障：对于这样的语句<code>Load1;LoadStore;Store2</code>,在<code>Store2</code>以及后续写入操作被刷出前，保证<code>Load1</code>要读取的数据被读取完毕</li>
<li>StoreLoad屏障：对于这样的语句<code>Store1;StoreLoad;Load2</code>,在<code>Load2</code>以及后续所有读取操作执行前，保证<code>Store1</code>的写入对所有处理器可见。</li>
</ul>
</li>
<li><p><strong>不保证原子性</strong></p>
</li>
</ol>
<h3 id="5-synchronized原理"><a href="#5-synchronized原理" class="headerlink" title="5. synchronized原理"></a>5. synchronized原理</h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/aspirant/p/11470858.html">深入分析Synchronized原理</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SynchronizedDemo</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title function_">method</span><span class="params">()</span>&#123;</span><br><span class="line">        <span class="keyword">synchronized</span> (<span class="built_in">this</span>)&#123;</span><br><span class="line">            System.out.println(</span><br><span class="line">                    <span class="string">&quot;Synchronized Demo&quot;</span></span><br><span class="line">            );</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述代码通过 <code>javap -c -l -p .class</code>反编译成字节码结果如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">Compiled from <span class="string">&quot;SynchronizedDemo.java&quot;</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SynchronizedDemo</span> &#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="title function_">SynchronizedDemo</span><span class="params">()</span>;</span><br><span class="line">    descriptor: ()V</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: aload_0</span><br><span class="line">       <span class="number">1</span>: invokespecial #<span class="number">1</span>                  <span class="comment">// Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V</span></span><br><span class="line">       <span class="number">4</span>: <span class="keyword">return</span></span><br><span class="line">    LineNumberTable:</span><br><span class="line">      line <span class="number">1</span>: <span class="number">0</span></span><br><span class="line">    LocalVariableTable:</span><br><span class="line">      Start  Length  Slot  Name   Signature</span><br><span class="line">          <span class="number">0</span>       <span class="number">5</span>     <span class="number">0</span>  <span class="built_in">this</span>   LSynchronizedDemo;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title function_">method</span><span class="params">()</span>;</span><br><span class="line">    descriptor: ()V</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: aload_0</span><br><span class="line">       <span class="number">1</span>: dup</span><br><span class="line">       <span class="number">2</span>: astore_1</span><br><span class="line">       <span class="number">3</span>: monitorenter			<span class="comment">// 一次 monitorenter</span></span><br><span class="line">       <span class="number">4</span>: getstatic     #<span class="number">2</span>                  <span class="comment">// Field java/lang/System.out:Ljava/io/PrintStream;</span></span><br><span class="line">       <span class="number">7</span>: ldc           #<span class="number">3</span>                  <span class="comment">// String Synchronized Demo</span></span><br><span class="line">       <span class="number">9</span>: invokevirtual #<span class="number">4</span>                  <span class="comment">// Method java/io/PrintStream.println:(Ljava/lang/String;)V</span></span><br><span class="line">      <span class="number">12</span>: aload_1</span><br><span class="line">      <span class="number">13</span>: monitorexit			<span class="comment">// 两次monitorexit</span></span><br><span class="line">      <span class="number">14</span>: goto          <span class="number">22</span></span><br><span class="line">      <span class="number">17</span>: astore_2</span><br><span class="line">      <span class="number">18</span>: aload_1	</span><br><span class="line">      <span class="number">19</span>: monitorexit			<span class="comment">// 两次monitorexit</span></span><br><span class="line">      <span class="number">20</span>: aload_2</span><br><span class="line">      <span class="number">21</span>: athrow</span><br><span class="line">      <span class="number">22</span>: <span class="keyword">return</span></span><br><span class="line">    Exception table:</span><br><span class="line">       from    to  target type</span><br><span class="line">           <span class="number">4</span>    <span class="number">14</span>    <span class="number">17</span>   any</span><br><span class="line">          <span class="number">17</span>    <span class="number">20</span>    <span class="number">17</span>   any</span><br><span class="line">    LineNumberTable:</span><br><span class="line">      line <span class="number">4</span>: <span class="number">0</span></span><br><span class="line">      line <span class="number">5</span>: <span class="number">4</span></span><br><span class="line">      line <span class="number">8</span>: <span class="number">12</span></span><br><span class="line">      line <span class="number">9</span>: <span class="number">22</span></span><br><span class="line">    LocalVariableTable:</span><br><span class="line">      Start  Length  Slot  Name   Signature</span><br><span class="line">          <span class="number">0</span>      <span class="number">23</span>     <span class="number">0</span>  <span class="built_in">this</span>   LSynchronizedDemo;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(java.lang.String[])</span>;</span><br><span class="line">    descriptor: ([Ljava/lang/String;)V</span><br><span class="line">    Code:</span><br><span class="line">       <span class="number">0</span>: <span class="keyword">return</span></span><br><span class="line">    LineNumberTable:</span><br><span class="line">      line <span class="number">12</span>: <span class="number">0</span></span><br><span class="line">    LocalVariableTable:</span><br><span class="line">      Start  Length  Slot  Name   Signature</span><br><span class="line">          <span class="number">0</span>       <span class="number">1</span>     <span class="number">0</span>  args   [Ljava/lang/String;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol>
<li><code>monitorenter</code>：每个对象都是一个监视器锁（<strong>monitor</strong>）。当<strong>monitor</strong>被占用时就会处于锁定状态，线程执行<code>monitorenter</code>指令时，尝试获取<strong>monitor</strong>的所有权，过程如下：</li>
</ol>
<blockquote>
<ol>
<li>如果<strong>monitor</strong>的进入数为0，则该线程进入<strong>monitor</strong>，然后将进入数设置为1，该线程即为<strong>monitor</strong>的所有者；</li>
<li>如果线程已经占有该<strong>monitor</strong>，只是重新进入，则进入<strong>monitor</strong>的进入数加1；</li>
<li>如果其他线程已经占用了<strong>monitor</strong>，则该线程进入阻塞状态，直到<strong>monitor</strong>的进入数为0，再重新尝试获取<strong>monitor</strong>的所有权</li>
</ol>
</blockquote>
<ol start="2">
<li><code>monitorexit</code>:执行<code>monitorexit</code>的线程必须是<code>objectref</code>所对应的<code>monitor</code>的所有者。指令执行时，<code>monitor</code>的进入数减1，如果减1后进入数为0，那线程退出<code>monitor</code>，不再是这个<code>monitor</code>的所有者。其他被这个<code>monitor</code>阻塞的线程可以尝试去获取这个<code>monitor</code>的所有权</li>
</ol>
<blockquote>
<p><font color=red>monitorexit指令出现了两次，第1次为同步正常退出释放锁；第2次为发生异常退出释放锁；</font></p>
</blockquote>
<p><strong>Synchronized</strong>的语义底层是通过一个monitor的对象来完成的。</p>
<h3 id="6-锁-amp-锁升级"><a href="#6-锁-amp-锁升级" class="headerlink" title="6.锁&amp;锁升级"></a>6.锁&amp;锁升级</h3><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/36eedeb3f912">浅谈偏向锁、轻量级锁、重量级锁</a></p>
<p>Synchronized加锁时，进程会从<strong>用户态</strong>转换成<strong>内核态</strong>，让操作系统帮忙调度。用户态与内核态的转换是非常耗时的，所以说Synchronized是重要级的锁。</p>
<p><strong>关于Synchronized的升级</strong></p>
<p>参考文献<a target="_blank" rel="noopener" href="https://blog.csdn.net/steven2xupt/article/details/108047270">https://blog.csdn.net/steven2xupt/article/details/108047270</a></p>
<p>由于synchronized性能问题在JDK1.6前饱受诟病，同时和@author Doug Lea大神写的目前在JUC下的AQS实现的锁差距太大，synchronized开发人员感觉脸上挂不住，所以在1.6版本进行了大幅改造升级，于是就出现了现在常通说的锁升级或锁膨胀的概念,整体思路就是能不打扰操作系统大哥就不打扰大哥，能在用户态解决的就不经过内核。</p>
<p><img src="https://img-blog.csdnimg.cn/2020081722362322.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N0ZXZlbjJ4dXB0,size_16,color_FFFFFF,t_70#pic_center" alt="img"></p>
<p><strong>升级过程</strong></p>
<ol>
<li>无锁态</li>
<li>偏向锁</li>
<li>轻量级锁（自旋锁：CAS）</li>
<li>重量级锁</li>
</ol>
<p><strong>MarkWord</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200817001540364.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N0ZXZlbjJ4dXB0,size_16,color_FFFFFF,t_70#pic_center" alt="img"></p>
<p>简单来说：</p>
<table>
<thead>
<tr>
<th>状态</th>
<th>标志位</th>
<th>存储内容</th>
</tr>
</thead>
<tbody><tr>
<td>未锁定</td>
<td>01</td>
<td>对象哈希码、对象分代年龄</td>
</tr>
<tr>
<td>轻量级锁定</td>
<td>00</td>
<td>指向锁记录的指针</td>
</tr>
<tr>
<td>重量级锁定</td>
<td>10</td>
<td>执行重量级锁定的指针</td>
</tr>
<tr>
<td>GC标记</td>
<td>11</td>
<td>空(不需要记录信息)</td>
</tr>
<tr>
<td>偏向锁</td>
<td>01</td>
<td>偏向线程ID、偏向时间戳、对象分代年龄</td>
</tr>
</tbody></table>
<p><img src="https://upload-images.jianshu.io/upload_images/4491294-e3bcefb2bacea224.png" alt="img"></p>
<p>对象内存布局中的markWord会记录当前只有锁的线程id，如果一个线程获取到锁，那么这个线程线程的id会被记录到markword中。</p>
<p><strong>锁升级的大致过程：</strong></p>
<blockquote>
<p>从<strong>无锁态</strong>，如果有对象尝试获取锁，则进入<strong>偏向锁状态</strong>（这时只有一个线程在使用资源）；</p>
<p>如果下一次还是这个线程使用资源，（尝试获取锁），先会比较markword中的线程id是否为这个线程，如果是，直接获得<strong>偏向锁</strong>。</p>
<p>如果有竞争（就是不同线程争抢锁），就会升级到<strong>轻量级锁</strong>（自旋锁CAS）；</p>
<p>如果竞争激烈，自旋了好久（<strong>默认10次</strong>）都没有竞争到锁，那么就会升级为<strong>重量级锁</strong>，然后<strong>挂起此线程</strong>，等待资源的释放后<strong>重新竞争锁</strong></p>
</blockquote>
<blockquote>
<p> JDK1.6引入了自适应自旋锁，所谓自适应自旋锁，就意味着自旋的次数不再是固定的，具体规则如下：</p>
<p>自旋次数通常由前一次在同一个锁上的自旋时间及锁的拥有者的状态决定。如果线程【T1】自旋成功，自旋次数为17次，那么等到下一个线程【T2】自旋时，也会默认认为【T2】自旋17次成功，</p>
<p>如果【T2】自旋了5次就成功了，那么此时这个自旋次数就会缩减到5次。</p>
<p>自适应自旋锁随着程序运行和性能监控信息，从而使得虚拟机可以预判出每个线程大约需要的自旋次数</p>
</blockquote>
<h3 id="7-AQS（AbstractQueuedSynchronizer）：抽象队列同步器"><a href="#7-AQS（AbstractQueuedSynchronizer）：抽象队列同步器" class="headerlink" title="7.AQS（AbstractQueuedSynchronizer）：抽象队列同步器"></a>7.AQS（AbstractQueuedSynchronizer）：抽象队列同步器</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21040559/article/details/112388069">AQS参考贴子</a></p>
<p>AQS是用来构建锁或者其他同步器组件的<strong>重量级基础框架及整个JUC体系的基石</strong>，通过内置的FIFO队列来完成资源获取线程的排队工作，并通过一个<strong>int类型的变量</strong>表示<strong>持有锁的状态</strong>。</p>
<p><strong>锁</strong>：面向锁的使用者</p>
<p><strong>同步器</strong>：面向锁的实现者</p>
<h4 id="CLH"><a href="#CLH" class="headerlink" title="CLH"></a>CLH</h4><p>AQS是JUC的核心，而CLH锁又是AQS的基础，说核心也不为过，因为AQS就是用了变种的CLH锁。如果要学好Java并发编程，那么必定要学好JUC；学好JUC，必定要先学好AQS；学好AQS，那么必定先学好CLH。因此，这就是我们为什么要学习CLH的原因。</p>
<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/CLH%E9%98%9F%E5%88%97-9358587-9358590.png" alt="CLH队列" style="zoom:67%;" />

<p>CLH：AQS中的队列是CLH变体的虚拟双向队列FIFO。</p>
<p>相关说明：</p>
<blockquote>
<p>抢到资源的线程直接使用处理业务逻辑，抢不到资源的必然涉及一种排队等候机制。抢占资源失败的线程继续去等待（类似于银行业务办理窗口都满了，暂时没有受理窗口的顾客只能去候客区排队等待），但等待线程仍然保留获取锁的可能且获取锁流程仍在继续（候客区的顾客也在等着叫号，轮到了再去受理窗口办理业务）。</p>
<p>既然说到了<font color=red>排队等候机制</font>,那么就一定会有某种队列来管理，这个队列就是我们所说的<strong>CLH</strong></p>
<p>如果共享资源被占用，就需要一定的阻塞等待唤醒机制来保证锁的分配。这个机制主要用的是CLH队列的变体实现的。将暂时获取不到锁🔐的线程加入到队列中，这个队列就是AQS的抽象表现。它将请求共享资源的线程封装成队列的节点（Node），通过CAS，自旋以及<code>LockSupport.park()</code>方式，维护state变量的状态，使并发达到同步的控制效果。</p>
<p><font color=green>Node</font>有两种模式：<strong>SHARED</strong>：共享模式；<strong>EXCLUSIVE</strong>：排他模式</p>
<p>队列中的每个节点<font color=green>Node</font>中有一个字段<code>waitStatus</code>代表当前节点在CLH中的状态：</p>
<ul>
<li><strong>0</strong>：当前一个Node被初始化的时候的默认值</li>
<li><strong>CANCELLED</strong>（1）：表示线程获取锁的请求已经取消了，不想等了，直接取消了</li>
<li><strong>CONDITION</strong>（-2）：表示节点在等待队列中，节点线程等待唤醒（等待Condition唤醒）</li>
<li><strong>PROPAGATE</strong>（-3）：当前线程处在SHARED情况下，该字段才会使用</li>
<li><strong>SIGNAL</strong>（-1）：表示线程已经准备好了，等待资源释放</li>
</ul>
</blockquote>
<h4 id="AQS初步"><a href="#AQS初步" class="headerlink" title="AQS初步"></a>AQS初步</h4><h5 id="AQS初识"><a href="#AQS初识" class="headerlink" title="AQS初识"></a>AQS初识</h5><p>AQS使用一个<code>volatile</code>的<code>int</code>类型的成员变量来表示同步状态，通过内置的<code>FIFO</code>队列来完成资源获取的排队工作将每条要去抢占资源的线程封装成一个Node节点来实现锁的分配，通过CAS来完成对<code>State</code>值的修改。</p>
<h5 id="AQS内部体系架构"><a href="#AQS内部体系架构" class="headerlink" title="AQS内部体系架构"></a>AQS内部体系架构</h5><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210819193357918-9372839.png" alt="image-20210819193357918" style="zoom:50%;" />



<p><strong>相关说明</strong></p>
<p>当AQS中的状态为State为0的时候，说明此时没有资源竞争，那么当前线程占用当前资源，同时State状态位+1;如果State大于等于1时（大于1时表示重入锁🔐），就说明当前资源又线程被占用，其它线程需要进行等待，其它线程会被封装成Node节点，然后放入到CLH中。</p>
<p>在CLH中，双向链表中的第一个节点时一个<font color=red>傀儡节点</font>, 起到站位的作用，其实际并不存储任何信息。真正存储数据的节点是从第二个开始的。因此**<code>傀儡节点（哨兵节点）中的Thread=null</code>**.</p>
<p>没到CLH中的线程节点争抢到资源的时候，那么之前的<font color=red>傀儡节点</font>将会被断开引用，然后挣抢到资源的线程所在节点的<code>Thread字段置为空</code>,然后这个节点成为新的<font color=red>傀儡节点</font></p>
<p><strong>AQS = CLH + State</strong></p>
<p><strong>串插一个面试题</strong></p>
<p>智力题-找机器<br>十个机器，其中九个机器生产的货物是5g，只有一个机器生产的货物是4g，给你一个称，如何一次找出那个生产4g货物的机器。</p>
<p>解题：<br>将10个机器依次进行编号，分别为1-10。1号机器生产1个货物，2号机器生产2个货物，以此类推，10号机器生产10个货物，共计55个货物。<br>假设每个机器生产的货物都是5g，则这55个货物的总重量为275g。因此，275g - 实际的重量 = 机器的编号。<br>原因：假设是n号机器生产的货物为4g，则n号机器总共生产n个货物，即重量减少了ng。</p>
<h2 id="常用设计模式"><a href="#常用设计模式" class="headerlink" title="常用设计模式"></a>常用设计模式</h2><h3 id="1-单例模式"><a href="#1-单例模式" class="headerlink" title="1.单例模式"></a>1.单例模式</h3><h3 id="2-工厂模式"><a href="#2-工厂模式" class="headerlink" title="2.工厂模式"></a>2.工厂模式</h3><h3 id="3-模板模式"><a href="#3-模板模式" class="headerlink" title="3.模板模式"></a>3.模板模式</h3><h3 id="4-代理模式"><a href="#4-代理模式" class="headerlink" title="4.代理模式"></a>4.代理模式</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProxyTest</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Proxy</span> <span class="variable">proxy</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Proxy</span>();</span><br><span class="line">        proxy.Request();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//抽象主题</span></span><br><span class="line"><span class="keyword">interface</span> <span class="title class_">Subject</span> &#123;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">Request</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//真实主题</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RealSubject</span> <span class="keyword">implements</span> <span class="title class_">Subject</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">Request</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;访问真实主题方法...&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//代理</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Proxy</span> <span class="keyword">implements</span> <span class="title class_">Subject</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> RealSubject realSubject;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">Request</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (realSubject == <span class="literal">null</span>) &#123;</span><br><span class="line">            realSubject = <span class="keyword">new</span> <span class="title class_">RealSubject</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        preRequest();</span><br><span class="line">        realSubject.Request();</span><br><span class="line">        postRequest();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">preRequest</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;访问真实主题之前的预处理。&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">postRequest</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;访问真实主题之后的后续处理。&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3 id="5-建造者模式"><a href="#5-建造者模式" class="headerlink" title="5.建造者模式"></a>5.建造者模式</h3><h3 id="6-观察者模式"><a href="#6-观察者模式" class="headerlink" title="6.观察者模式"></a>6.观察者模式</h3><h3 id="7-原型设计模式"><a href="#7-原型设计模式" class="headerlink" title="7. 原型设计模式"></a>7. 原型设计模式</h3><p>原型（Prototype）模式的定义如下：用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型相同或相似的新对象。在这里，原型实例指定了要创建的对象的种类。用这种方式创建对象非常高效，根本无须知道对象创建的细节。例如，Windows 操作系统的安装通常较耗时，如果复制就快了很多。在生活中复制的例子非常多，这里不一一列举了。</p>
<h4 id="原型模式的优点："><a href="#原型模式的优点：" class="headerlink" title="原型模式的优点："></a>原型模式的优点：</h4><ul>
<li><a target="_blank" rel="noopener" href="http://c.biancheng.net/java/">Java</a> 自带的原型模式基于内存二进制流的复制，在性能上比直接 new 一个对象更加优良。</li>
<li>可以使用深克隆方式保存对象的状态，使用原型模式将对象复制一份，并将其状态保存起来，简化了创建对象的过程，以便在需要的时候使用（例如恢复到历史某一状态），可辅助实现撤销操作。</li>
</ul>
<h4 id="原型模式的缺点："><a href="#原型模式的缺点：" class="headerlink" title="原型模式的缺点："></a>原型模式的缺点：</h4><ul>
<li>需要为每一个类都配置一个 clone 方法</li>
<li>clone 方法位于类的内部，当对已有类进行改造的时候，需要修改代码，违背了开闭原则。</li>
<li>当实现深克隆时，需要编写较为复杂的代码，而且当对象之间存在多重嵌套引用时，为了实现深克隆，每一层对象对应的类都必须支持深克隆，实现起来会比较麻烦。因此，深克隆、浅克隆需要运用得当。</li>
</ul>
<h4 id="原型模式的实现"><a href="#原型模式的实现" class="headerlink" title="原型模式的实现"></a>原型模式的实现</h4><p><img src="http://c.biancheng.net/uploads/allimg/181114/3-1Q114101Fa22.gif" alt="原型模式的结构图"></p>
<p>由于 Java 提供了对象的 clone() 方法，所以用 Java 实现原型模式很简单。</p>
<p>原型模式包含以下主要角色。</p>
<ol>
<li>抽象原型类：规定了具体原型对象必须实现的接口。</li>
<li>具体原型类：实现抽象原型类的 clone() 方法，它是可被复制的对象。</li>
<li>访问类：使用具体原型类中的 clone() 方法来复制新的对象。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//具体原型类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Realizetype</span> <span class="keyword">implements</span> <span class="title class_">Cloneable</span> &#123;</span><br><span class="line">    Realizetype() &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;具体原型创建成功！&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> Object <span class="title function_">clone</span><span class="params">()</span> <span class="keyword">throws</span> CloneNotSupportedException &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;具体原型复制成功！&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> (Realizetype) <span class="built_in">super</span>.clone(); <span class="comment">// 浅拷贝</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//原型模式的测试类</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">PrototypeTest</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> CloneNotSupportedException &#123;</span><br><span class="line">        <span class="type">Realizetype</span> <span class="variable">obj1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Realizetype</span>();</span><br><span class="line">        <span class="type">Realizetype</span> <span class="variable">obj2</span> <span class="operator">=</span> (Realizetype) obj1.clone();</span><br><span class="line">        System.out.println(<span class="string">&quot;obj1==obj2?&quot;</span> + (obj1 == obj2));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/xzwblog/p/7230788.html">Java中的深浅拷贝（clone）</a></p>
<h2 id="列举Java几个异常"><a href="#列举Java几个异常" class="headerlink" title="列举Java几个异常"></a>列举Java几个异常</h2><ol>
<li><code> java.lang.OutOfMemoryError</code><ul>
<li>当可用内存不足以让Java虚拟机分配给一个对象时抛出该错误。</li>
</ul>
</li>
<li><code> java.lang.StackOverflowError</code><ul>
<li>当一个应用递归调用的层次太深而导致堆栈溢出或者陷入死循环时抛出该错误</li>
</ul>
</li>
<li><code>java.lang.CloneNotSupportedException</code><ul>
<li>clone方法所在类没有继承Cloneable接口</li>
</ul>
</li>
<li><code>java.util.ConcurrentModificationException</code> </li>
</ol>
<h1 id="Linux-复习"><a href="#Linux-复习" class="headerlink" title="Linux 复习"></a>Linux 复习</h1><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看磁盘情况</span></span><br><span class="line"><span class="built_in">df</span> -h </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看目录，或文件使用磁盘情况</span></span><br><span class="line"><span class="built_in">du</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看内存使用情况</span></span><br><span class="line">free -h</span><br><span class="line"></span><br><span class="line"><span class="comment"># http 工具</span></span><br><span class="line"></span><br><span class="line">curl URL</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进程</span></span><br><span class="line">ps -ef</span><br><span class="line"></span><br><span class="line">top</span><br><span class="line"></span><br><span class="line">htop 交互式top命令</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 远程同步</span></span><br><span class="line">scp</span><br><span class="line"></span><br><span class="line">rsync   <span class="comment"># 同步，同步之间会比较之前的文件，只会同步更改的内容</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较两个文件的差异</span></span><br><span class="line">diff 文件1 文件2 -y -W</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看历史命令</span></span><br><span class="line"><span class="built_in">history</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 服务管理命令</span></span><br><span class="line"></span><br><span class="line">service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 管理systemd的资源Unit</span></span><br><span class="line">systemctl</span><br><span class="line"></span><br><span class="line">xargs</span><br><span class="line"><span class="comment"># 例如：</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;11@22@33&#x27;</span> | xargs -d <span class="string">&#x27;@&#x27;</span> <span class="built_in">echo</span></span><br><span class="line">11 22 33</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计文件行数</span></span><br><span class="line"><span class="built_in">wc</span> -l test1.txt</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">脚本同步命令</span></span><br><span class="line"><span class="meta">#</span><span class="language-bash">!/bin/sh</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=$#</span><br><span class="line">if((pcount==0)); then</span><br><span class="line">        echo no args...;</span><br><span class="line">        exit;</span><br><span class="line">fi</span><br><span class="line"><span class="meta"># </span><span class="language-bash">获取文件名称</span></span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"><span class="meta"># </span><span class="language-bash">获取上级目录到绝对路径</span></span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"><span class="meta"># </span><span class="language-bash">获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"><span class="meta"># </span><span class="language-bash">循环</span></span><br><span class="line">for((host=2; host&lt;=10; host++)); do</span><br><span class="line">        echo $pdir/$fname $user@slave$host:$pdir</span><br><span class="line">        echo ==================slave$host==================</span><br><span class="line">        rsync -rvl $pdir/$fname $user@slave$host:$pdir</span><br><span class="line">done</span><br><span class="line"><span class="meta">#</span><span class="language-bash">Note:这里的slave对应自己主机名，需要做相应修改。另外，<span class="keyword">for</span>循环中的host的边界值</span></span><br></pre></td></tr></table></figure>







<h3 id="top详解"><a href="#top详解" class="headerlink" title="top详解"></a>top详解</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">当前时间、系统已运行时间、当前登录用户的数量、最近5、10、15分钟内的平均负载</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了</span></span><br><span class="line">top - 15:56:34 up 46 days, 20:13,  0 users,  load average: 8.20, 9.77, 10.36</span><br><span class="line"><span class="meta"># </span><span class="language-bash">tasks 统计，系统现在共有34个任务，1个正在运行，33</span></span><br><span class="line">Tasks:  34 total,   1 running,  33 sleeping,   0 stopped,   0 zombie</span><br><span class="line"><span class="meta"># </span><span class="language-bash">CPU 使用情况</span> </span><br><span class="line"><span class="meta"># </span><span class="language-bash">us：用户空间占用情况</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">sy：内核空间占用情况</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">ni：改变过优先级的进程占用CPU的百分比</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">id</span>：空闲CPU百分比</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">wa：IO等待占用CPU的百分比</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">hi：硬中断占用cpu百分比</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">si：软中断占用cpu百分比</span></span><br><span class="line"><span class="meta">%</span><span class="language-bash">Cpu(s):  5.2 us,  5.1 sy,  0.6 ni, 89.0 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span></span><br><span class="line">KiB Mem : 26390454+total, 18626068 free, 18532243+used, 59956032 buff/cache</span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used. 47536756 avail Mem </span><br></pre></td></tr></table></figure>



<h1 id="MySql复习"><a href="#MySql复习" class="headerlink" title="MySql复习"></a>MySql复习</h1><h2 id="1-数据库三范式"><a href="#1-数据库三范式" class="headerlink" title="1. 数据库三范式"></a>1. 数据库三范式</h2><ol>
<li><strong>第一范式：每个列不可再分</strong></li>
<li><strong>第二范式：不存在部分函数依赖</strong></li>
<li><strong>第三范式：不存在传递函数依赖</strong></li>
</ol>
<h2 id="2-MySql存储引擎"><a href="#2-MySql存储引擎" class="headerlink" title="2.MySql存储引擎"></a>2.MySql存储引擎</h2><p>MySql中的数据，索引以及其他对象是如何存储的，是一套文件系统实现的（Storage Engine）</p>
<p>MY_SQL存储引擎有以下几种：</p>
<ul>
<li>MRG_MYISAM</li>
<li>MyISM</li>
<li>BLACKHOLE</li>
<li>CSV</li>
<li>MEMORY</li>
<li>ARCHIVE</li>
<li>InnoDB</li>
<li>PERFORMANCE_SCHEMA</li>
</ul>
<p><strong>InnoDB引擎</strong>：InnoDB引擎提供了对数据库ACID事务的支持。并且还提供了行级锁和外键的约束。它的设计的目标就是处理大数据容量的数据库系统。</p>
<p><strong>MyISAM引擎</strong>：不提供事务的支持，也不支持行级锁和外键</p>
<p><strong>Memory引擎</strong>：所有的数据都在内存中，数据的处理速度快，但是安全性不搞。</p>
<p><strong>常见的<em>MyISAM</em>与<em>InnoDB</em>的比较</strong></p>
<table>
<thead>
<tr>
<th></th>
<th>MyISAM</th>
<th>InnoDB</th>
</tr>
</thead>
<tbody><tr>
<td>存储结构</td>
<td>每张表被存放在三个文件：<br />frm-表格定义<br />MYD（MYData）-数据文件<br />MYI（MYIndex）-索引文件</td>
<td>所有的表都保存在同一个数据文件中<br />（也可能是多个文件，或者是独立的表空间文件）<br />InnoDB表的大小只受限于操作系统文件的大小，<br />一般为2GB</td>
</tr>
<tr>
<td>存储空间</td>
<td>MyISAM可被压缩，存储空间较小</td>
<td>InnoDB的表需要更多的内存和存储，<br />它会在主内存中建立专用的缓冲池<br />用于高速缓冲数据和索引</td>
</tr>
<tr>
<td>可移植性、备份及恢复</td>
<td>由于MyISAM的数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。<br />在备份和恢复时可单独针对某个表进行操作。</td>
<td>数据和索引都是集中存储的</td>
</tr>
<tr>
<td>记录存储顺序</td>
<td>按记录插入顺序保存</td>
<td>按主键大小有序插入</td>
</tr>
<tr>
<td>外键</td>
<td><strong>不支持</strong></td>
<td><strong>支持</strong></td>
</tr>
<tr>
<td>事务</td>
<td><strong>不支持</strong></td>
<td><strong>支持</strong></td>
</tr>
<tr>
<td>锁支持（锁是避免资源争用的一个机制，MySql锁对用户几乎是透明的）</td>
<td><strong>表级锁定</strong></td>
<td><strong>行级锁定</strong>，<strong>表级锁定</strong>，锁定粒度小并发能力高</td>
</tr>
<tr>
<td>SELECT</td>
<td>MyIsam更有</td>
<td></td>
</tr>
<tr>
<td>INSERT、UPDATE、DELETE</td>
<td></td>
<td>InnoDB更优</td>
</tr>
<tr>
<td>索引的实现方式</td>
<td>B+树索引，myisam是堆表</td>
<td>B+树索引，InnoDB是索引组织表</td>
</tr>
<tr>
<td>哈希索引</td>
<td>不支持</td>
<td>支持</td>
</tr>
<tr>
<td>全文索引</td>
<td>支持</td>
<td>不支持</td>
</tr>
</tbody></table>
<p><strong>MyISAM索引与InnoDB索引的区别</strong></p>
<ul>
<li>InnoDB索引是聚簇索引（索引的存储顺序与实际的数据物理存储顺序保持一致），MyIsam索引是非聚簇索引</li>
<li>InnoDB的主键索引的叶子结点存储着行数据，因此主键索引非常高效</li>
<li>MyISAM索引的叶子结点存储的是行数据地址，需要再寻址一次才能得到数据</li>
<li>InnoDB非主键索引的叶子结点存储的是主键和其他索引的列数据，因此查询时做到覆盖索引会非常高效。</li>
</ul>
<h2 id="3-索引"><a href="#3-索引" class="headerlink" title="3. 索引"></a>3. 索引</h2><h3 id="3-1-什么是索引？"><a href="#3-1-什么是索引？" class="headerlink" title="3.1 什么是索引？"></a>3.1 什么是索引？</h3><p>索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。</p>
<p><strong>B树与B+树的却别</strong></p>
<blockquote>
<p>B树中每个关键字都保存着数据；而B+树中只有叶子节点存储实际的数据，其余节点只存储关键字信息。这使得B+查找数据必须查询到叶子节点，而B树只要匹配即可（不管元素的具体位置），因此B+树查找更稳定</p>
<p>对于范围查找来说，B+树只需遍历叶子结点链表即可，b树却需要重复地中序遍历。</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MDI1NTkx,size_16,color_FFFFFF,t_70.png" alt="在这里插入图片描述"></p>
</blockquote>
<h3 id="3-2-索引的优点"><a href="#3-2-索引的优点" class="headerlink" title="3.2 索引的优点"></a>3.2 索引的优点</h3><ul>
<li>可以大大加快数据的检索速度，这就是创建索引的最主要的原因</li>
<li>通过使用索引，可以在查询的过程中，使用优化器，提高系统的性能</li>
</ul>
<h3 id="3-3-索引的缺点"><a href="#3-3-索引的缺点" class="headerlink" title="3.3 索引的缺点"></a>3.3 索引的缺点</h3><ul>
<li>时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加，删除和修改的时候，索引也要动态的维护，会降低增删改的执行效率。</li>
<li>空间方面：索引需要占物理空间</li>
</ul>
<h3 id="3-4-索引使用注意事项"><a href="#3-4-索引使用注意事项" class="headerlink" title="3.4 索引使用注意事项"></a>3.4 索引使用注意事项</h3><p>索引虽好，但是索引使用不恰当就会造成索引失效</p>
<p><strong>索引使用口诀：</strong></p>
<ol>
<li><strong>全值匹配我最爱</strong><ul>
<li>就是查询的列都是索引（覆盖索引，这中情况不需要查询实际的物理数据，只需要查询索引树即可）</li>
</ul>
</li>
<li><strong>最佳左前缀法则</strong><ul>
<li>如果索引了多列，在使用时就要遵守最左前缀法则（mysql会经过优化器去优化字段的顺序，使得查询顺序与所以创建的顺序保持一致）<ul>
<li>带头大哥不能死</li>
<li>中间兄弟不能断</li>
</ul>
</li>
</ul>
</li>
<li><strong>索引列上不计算</strong><ul>
<li>使用sum，avg等计算或者 自动(手动)类型转换，都会导致索引失效</li>
</ul>
</li>
<li><strong>范围之后全失效</strong></li>
<li><strong>like语句%加在右边</strong></li>
<li><strong>字符串里有引号</strong><ul>
<li>字符串不加单引号，会导致索引失效</li>
</ul>
</li>
<li>其他<ul>
<li>使用**!=，&lt;&gt;, is null； is not null ；or**都会导致索引失效</li>
</ul>
</li>
</ol>
<p><strong>非聚簇索引一定会产生回表吗？</strong></p>
<p>不一定。如果查询语句中的的列全部命中索引(<strong>覆盖索引</strong>)，那就不必再进行回表查询了。</p>
<h2 id="4-MySql日志"><a href="#4-MySql日志" class="headerlink" title="4. MySql日志"></a>4. MySql日志</h2><p>MariaDB/MySql中日志包括：</p>
<ol>
<li>错误日志（<strong>Error log</strong>）：记录mysql服务启动时正确和错误的信息，还记录启动、停止、运行过程中的错误信息</li>
<li>查询日志（<strong>General log</strong>）：记录建立的客户端连接和执行的语句。</li>
<li>二进制日志（<strong>binlog</strong>）：记录所有更改数据的语句，可用于数据复制</li>
<li>慢查询日志（<strong>show log</strong>）：记录所有执行时间超过long_query_time的所有查询或不使用索引的查询</li>
<li>中继日志（<strong>relay log</strong>）：主从复制时使用的日志。</li>
<li>InnoDB引擎还有事务日志</li>
</ol>
<h3 id="4-1-二进制日志"><a href="#4-1-二进制日志" class="headerlink" title="4.1 二进制日志"></a>4.1 二进制日志</h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/f-ck-need-u/archive/2018/05/08/9010872.html">事务日志详解</a></p>
<p>MySql支持statement、row、mixed三种形式的<strong>binlog</strong>记录方式。</p>
<ul>
<li>statement<ul>
<li>将所有的相关操作记录为SQL语句的形式</li>
<li>这样的记录方式对某些特殊信息无法同步记录，例如uuid，now()等这样的动态变化的值。</li>
</ul>
</li>
<li>row<ul>
<li>基于行来记录，将相关行的每一列的值都在日志中保存下来</li>
<li>这样的结果会导致日志文件变得非常大，但是保证了动态值的确定性。</li>
</ul>
</li>
<li>mixed<ul>
<li>statement与row混合形式</li>
<li>默认采用statement的方式记录，只有以下几种情况会采用row的形式来记录日志<ul>
<li>表的存储引擎为NDB，这是对表的DML操作都会以row的格式记录</li>
<li>使用了uuid(), user(), current_user(), found_rows(), row_cuount()等不确定函数。但是测试发现对now()函数仍然会以statement格式记录，而sysdate()函数会以row格式记录。</li>
<li>使用了insert delay语句</li>
<li>使用了临时表</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="4-2-事务日志"><a href="#4-2-事务日志" class="headerlink" title="4.2 事务日志"></a>4.2 事务日志</h3><p>InnoDB存储引擎的事务日志包括：undo log 和 redo log</p>
<p>redo log 通常是物理日志，记录的是数据页的物理页修改，而不是某一行或某几行修改成怎么样，它用来恢复提交后的物理数据页（恢复数据页，且只能恢复到最后一次提交的位置）</p>
<p>undo用来回滚行记录到某个版本。undo log一般是逻辑日志，根据每行记录进行记录。</p>
<h4 id="1-redo-log"><a href="#1-redo-log" class="headerlink" title="1. redo log"></a>1. redo log</h4><p>redo log不是二进制日志。虽然二进制日志也记录了innodb表的很多操作，也能实现重做的功能，但是他们之间有很大区别。</p>
<ol>
<li>二进制日志是在<strong>存储引擎的上层</strong>产生的，不管是什么存储引擎，对数据库进行了修改都会产生二进制日志。而redo log是innodb层产生的，只记录该存储引擎中表的修改。<strong>并且二进制日志先于redo log被记录</strong>。具体的见后文group commit小结。</li>
<li>二进制日志记录操作的方法是逻辑性的语句。即便它是基于行格式的记录方式，其本质也还是逻辑的SQL设置，如该行记录的每列的值是多少。而redo log是在物理格式上的日志，它记录的是数据库中每个页的修改。</li>
<li><font color=green>二进制日志只在每次事务提交的时候一次性写入缓存中的日志”文件”(对于非事务表的操作，则是每次执行语句成功后就直接写入)</font>。而redo log在数据准备修改前写入缓存中的redo log中，然后才对缓存中的数据执行修改操作；而且保证在发出事务提交指令时，先向缓存中的redo log写入日志，写入完成后才执行提交动作。</li>
<li>因为二进制日志只在提交的时候一次性写入，所以二进制日志中的记录方式和提交顺序有关，且一次提交对应一次记录。而redo log中是记录的物理页的修改，redo log文件中同一个事务可能多次记录，最后一个提交的事务记录会覆盖所有未提交的事务记录。例如事务T1，可能在redo log中记录了 T1-1,T1-2,T1-3，T1* 共4个操作，其中 T1* 表示最后提交时的日志记录，所以对应的数据页最终状态是 T1* 对应的操作结果。而且redo log是并发写入的，不同事务之间的不同版本的记录会穿插写入到redo log文件中，例如可能redo log的记录方式如下：<code>T1-1,T1-2,T2-1,T2-2,T2*,T1-3,T1*</code>。</li>
<li>事务日志记录的是物理页的情况，它具有幂等性，因此记录日志的方式极其简练。幂等性的意思是多次操作前后状态是一样的，例如新插入一行后又删除该行，前后状态没有变化。而二进制日志记录的是所有影响数据的操作，记录的内容较多。例如插入一行记录一次，删除该行又记录一次。</li>
</ol>
<p><strong>Redo log的基本概念</strong></p>
<p>redo log包括两部分：一是内存中的日志缓冲(redo log buffer)，该部分日志是易失性的；二是磁盘上的重做日志文件(redo log file)，该部分日志是持久的。</p>
<p>在概念上，innodb通过***force log at commit***机制实现事务的持久性，即在事务提交的时候，必须先将该事务的所有事务日志写入到磁盘上的redo log file和undo log file中进行持久化。</p>
<p>为了确保每次日志都能写入到事务日志文件中，在每次将log buffer中的日志写入日志文件的过程中都会调用一次操作系统的fsync操作(即fsync()系统调用)。因为MariaDB/MySQL是工作在用户空间的，MariaDB/MySQL的log buffer处于用户空间的内存中。要写入到磁盘上的log file中(redo:ib_logfileN文件,undo:share tablespace或.ibd文件)，中间还要经过操作系统内核空间的os buffer，调用fsync()的作用就是将OS buffer中的日志刷到磁盘上的log file中。</p>
<p><strong>redo log的格式</strong></p>
<p>因为InnoDB存储引擎数据的单元是页，所以redo log也是基于页的格式来记录的。InnoDB的页大小是16kb，一个页可以存放非常多的log blcok（512字节），而log block中记录的有时数据页的变化。</p>
<p><strong>InnoDB的恢复行为</strong></p>
<p>在启动InnoDB的时候，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。</p>
<p>因为redo log记录的是数据页的物理变化，因此恢复的时候速度比逻辑日志（比如binlog）要快很多。而且，InnoDB自身也做了一定程度的优化，让恢复速度变得更快。</p>
<h4 id="2-undo-log"><a href="#2-undo-log" class="headerlink" title="2. undo log"></a>2. undo log</h4><p>undo log有两个作用：提供回滚和多个行版本控制（MVCC）</p>
<p>在数据修改的时候，不仅记录了<strong>redo</strong>，还记录了相应的<strong>undo</strong>，如果因为某些原因导致事务失败或回滚，可以借助该undo进行回滚。</p>
<p>undo log 和 redo log记录物理日志不一样，它是逻辑日志。<font color=red>可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。</font>但是实际并不是这样记录的。</p>
<p>当执行rollback时，就可以从undo log中的逻辑读取到相应的内容并进行回滚。有时候应用到行版本控制的时候，也是通过undo log来实现的：当读取的某一行被其他事务锁定时，它可以从undo log中分析出该行记录以前的数据是什么，从而提供该行版本信息，让用户实现非锁定一致性读取。</p>
<p><font color=red>undo log 是采用段（segment）的方式来记录的，每个undo操作在记录的时候占用一个undo log segment。</font></p>
<p>另外，undo log也会产生redo log，因为undo log也要实现持久性保护。</p>
<p><strong>undo log 的存储方式</strong></p>
<p>InnoDB存储引擎对undo 的管理采用段的方式。rollback segment称为回滚段，每个混滚段中有1024个undo log segment。</p>
<p>另一篇帖子中的介绍：<a target="_blank" rel="noopener" href="http://www.llbiancheng.com/5623.html">http://www.llbiancheng.com/5623.html</a></p>
<p><strong>Undo</strong>：意为取消，以撤销操作为目的，返回指定某个状态的操作。</p>
<p><strong>Undo Log</strong>：数据库事务提交之前，会将事务修改数据的镜像（即修改前的旧版本）存放到 undo 日志里，当事务回滚时，或者数据库奔溃时，可以利用 undo 日志，即旧版本数据，撤销未提交事务对数据库产生的影响。。</p>
<ul>
<li>对于 insert 操作，undo 日志记录新数据的 PK(ROW_ID)，回滚时直接删除；</li>
<li>对于 delete/update 操作，undo 日志记录旧数据 row，回滚时直接恢复；</li>
<li>他们分别存放在不同的buffer里。</li>
</ul>
<p><strong>Undo Log 是为了实现事务的原子性而出现的产物。</strong></p>
<p><strong>Undo Log 实现事务原子性</strong>：事务处理过程中，如果出现了错误或者用户执行了 ROLLBACK 语句，MySQL 可以利用 Undo Log 中的备份将数据恢复到事务开始之前的状态。</p>
<p>InnoDB 发现可以基于 Undo Log 来实现多版本并发控制。</p>
<p><strong>Undo Log 在 MySQL InnoDB 存储引擎中用来实现多版本并发控制。</strong></p>
<p><strong>Undo Log 实现多版本并发控制</strong>：事务未提交之前，Undo Log 保存了未提交之前的版本数据，Undo Log 中的数据可作为数据旧版本快照供其他并发事务进行快照读。</p>
<p>关于Undo log是怎么实现MVCC的，请参考上篇文章：<a target="_blank" rel="noopener" href="https://blog.csdn.net/u013277209/article/details/114360409">吃透MySQL（九）：MVCC多版本并发控制</a></p>
<p><strong>Redo</strong>：顾名思义就是重做。以恢复操作为目的，重现操作。</p>
<p><strong>Redo Log</strong>：指事务中操作的任何数据，将最新的数据备份到一个地方（Redo Log）。</p>
<p><strong>Redo Log 的持久化</strong>：不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入 Redo Log 中，具体的落盘策略可以进行配置。</p>
<p><strong>Redo Log 是为了实现事务的持久性而出现的产物。</strong></p>
<p><strong>Redo Log 实现事务持久性</strong>：防止在发生故障的时间点，缓冲池（buffer pool）尚有脏页未写入表的 IBD 文件中，在重启 MySQL 服务的时候，根据 Redo Log 进行重做，从而达到事务的未入磁盘数据进行持久化这一特性。</p>
<p>一旦事务成功提交且数据从缓冲池（buffer pool）持久化到表的 IBD 文件中之后，此时 Redo Log 中的对应事务数据记录就失去了意义，所 以 Redo Log 的写入是日志文件循环写入的过程，也就是覆盖写的过程。</p>
<h3 id="4-事务"><a href="#4-事务" class="headerlink" title="4. 事务"></a>4. 事务</h3><h4 id="4-1-什么是事务？"><a href="#4-1-什么是事务？" class="headerlink" title="4.1 什么是事务？"></a>4.1 什么是事务？</h4><p>事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务是逻辑上的一组操作，要么都执行，要么都不执行。</p>
<h4 id="4-2-事务的四大特性（ACID）"><a href="#4-2-事务的四大特性（ACID）" class="headerlink" title="4.2 事务的四大特性（ACID）"></a>4.2 事务的四大特性（ACID）</h4><ul>
<li><strong>原子性</strong>（Atomicity）</li>
<li><strong>一致性</strong>（Consistency）</li>
<li><strong>隔离性</strong>（Isolation）</li>
<li><strong>持久性</strong>（Durability）</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/kismetv/p/10331633.html">参考文章</a></p>
<h5 id="4-2-1-原子性"><a href="#4-2-1-原子性" class="headerlink" title="4.2.1 原子性"></a>4.2.1 原子性</h5><p>原子性是指一个事务是一个不可分割的工作单位，其中的操作要么都做，要么都不做；如果事务中一个sql语句执行失败，则已执行的语句也必须回滚，数据库退回到事务前的状态。</p>
<ul>
<li><strong>实现原理：undo log</strong></li>
</ul>
<h5 id="4-2-2-持久性"><a href="#4-2-2-持久性" class="headerlink" title="4.2.2 持久性"></a>4.2.2 持久性</h5><p>持久性是指事务一旦提交，它对数据库的改变就应该是永久性的。就下来的其他操作或故障不应该对其有任何影响。</p>
<ul>
<li><strong>实现原理：redo log</strong></li>
</ul>
<h5 id="4-2-3-隔离性"><a href="#4-2-3-隔离性" class="headerlink" title="4.2.3 隔离性"></a>4.2.3 隔离性</h5><p>与原子性、持久性侧重于研究事务本身有所不同，隔离性研究的是不同事务之间的相互影响。隔离性是指，事务内部的操作与其他事务是隔离的，并发执行的各个事务之间不能互相干扰。</p>
<p>隔离性追求的是并发情形下事务之间互不干扰。</p>
<p>隔离性探讨的问题主要分为两方面：</p>
<ul>
<li>（一个事务）写操作对（另一个事务）写操作的影响：锁机制保证隔离性</li>
<li>（一个事务）写操作对（另一个事务）读操作的影响：MVCC保证隔离性</li>
</ul>
<p>首先来看两个事务的写操作之间的相互影响。隔离性要求同一时刻只能有一个事务对数据进行写操作，InnoDB通过锁机制来保证这一点。</p>
<p>锁机制的基本原理可以概括为：事务在修改数据之前，需要先获得相应的锁；获得锁之后，事务便可以修改数据；该事务操作期间，这部分数据是锁定的，其他事务如果需要修改数据，需要等待当前事务提交或回滚后释放锁。</p>
<p><strong>行锁与表锁</strong></p>
<p>按照粒度，锁可以分为表锁、行锁以及其他位于二者之间的锁。表锁在操作数据时会锁定整张表，并发性能较差；行锁则只锁定需要操作的数据，并发性能好。但是由于加锁本身需要消耗资源(获得锁、检查锁、释放锁等都需要消耗资源)，因此在锁定数据较多情况下使用表锁可以节省大量资源。MySQL中不同的存储引擎支持的锁是不一样的，例如MyIsam只支持表锁，而InnoDB同时支持表锁和行锁，且出于性能考虑，绝大多数情况下使用的都是行锁。</p>
<p><strong>脏读、不可重复读和幻读</strong></p>
<p>并发情况下，读操作可能存在的三类问题：</p>
<ol>
<li><p>脏读：当前事务A可以读到其他事务B未提交的数据（脏数据），这种现象为脏读</p>
<blockquote>
<p><img src="https://img2018.cnblogs.com/blog/1174710/201901/1174710-20190128201003630-2050662608.png" alt="img"></p>
</blockquote>
</li>
<li><p>不可重复读：在事务A中先后两次读取同一个数据，两次读取的结果不一样，这种现象称为不可重复读。</p>
<ul>
<li>脏读与不可重复读的区别在于：前者读到的是其他事务未提交的数据，后者读到的是其他事务已提交的数据</li>
</ul>
<blockquote>
<p><img src="https://img2018.cnblogs.com/blog/1174710/201901/1174710-20190128201011603-1317894910.png" alt="img"></p>
</blockquote>
</li>
<li><p>幻读：在事务A中按照某个条件先后两次查询数据库，两次查询结果的条数不同，这种现象称为幻读。</p>
<ul>
<li>不可重复读与幻读的区别在于：前者是数据变了，后者是数据的行变了</li>
</ul>
<blockquote>
<p><img src="https://img2018.cnblogs.com/blog/1174710/201901/1174710-20190128201021606-1089980279.png" alt="img"></p>
</blockquote>
</li>
</ol>
<p><strong>事务的隔离级别</strong></p>
<p>SQL标准中定义了四种隔离级别，并规定了每种隔离级别下上述几个问题是否存在。一般来说，隔离级别越低，系统开销越低，可支持的并发越高，但隔离性也越差。隔离级别与读问题的关系如下：</p>
<p><img src="https://img2018.cnblogs.com/blog/1174710/201901/1174710-20190128201034603-681355962.png" alt="img"></p>
<ul>
<li>Read UnCommitted 读取未提交内容<ul>
<li>在这个隔离级别，所有事务都可以“看到”为提交事务的执行结果。（会造成脏读、不可重复读、幻读）</li>
</ul>
</li>
<li>Read Committed 读取提交内容<ul>
<li>一个事务从开始到提交前，所做的任何数据改变都是不可见的，除非已经提交了。（解决了脏读，但是没有解决不可重复读  和 幻读）</li>
</ul>
</li>
<li>Repeatable Read 可重复读<ul>
<li>MySql数据库默认的隔离级别</li>
<li>它保证同一事务的多个实例在并发读取事务时，会“看到同样的”数据行。（解决了 脏读 和 不可重复读，但是没有解决  幻读）</li>
</ul>
</li>
<li>Serializable 可串行化<ul>
<li>该级别是最高级别的隔离级。它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简而言之，SERIALIZABLE是在每个读的数据行上加锁。在这个级别，可能导致大量的超时<code>Timeout</code>和锁竞争<code>Lock Contention</code>现象，实际应用中很少使用到这个级别，但如果用户的应用为了数据的稳定性，需要强制减少并发的话，也可以选择这种隔离级</li>
</ul>
</li>
</ul>
<p><strong>MVCC多版本并发控制</strong></p>
<p>MVCC可以解决幻读</p>
<p><font color=green>InnoDB的MVCC实现机制</font></p>
<ul>
<li>InnoDB的MVCC实现，是通过保存数据在某个时间点的快照实现的。</li>
<li>一个事务，不管其执行多长时间，其内部看到的数据是一致的。事务在执行过程中不会相互影响</li>
</ul>
<p>MySql中，每条实际的行数据除了我们定义的字段外，还有几个隐藏的列，其中有关于MVCC的重要字段有两个：<strong>DATA_TRX_ID</strong>和<strong>DELETE_BIT</strong></p>
<ul>
<li><p><font color=red>DATA_TRX_ID</font> 标记了最新更新这条行记录的transaction id，每处理一个事务，其值自动+1</p>
</li>
<li><p><font color=red>DELETE_BIT </font>用于标识该记录是否被删除，这里的不是真正的删除数据，而是标志出来的删除。真正意义的删除是在commit的时候。</p>
</li>
</ul>
<p>下面分别以select、delete、insert、update语句来说明：</p>
<ul>
<li><strong>INSERT</strong><ul>
<li>InnoDB为每个新增行记录当前系统版本号（事务ID）作为创建ID（DATA_TRX_ID）</li>
</ul>
</li>
<li><strong>DELETE</strong><ul>
<li>InnoDB为每个删除行记录当前系统版本号（事务ID）作为删除ID（DELETE_BIT）</li>
</ul>
</li>
<li><strong>UPDATE</strong><ul>
<li>InnoDB复制了一行。这个新行的版本号使用了系统版本号。它也把系统版本号作为了删除行的版本。</li>
</ul>
</li>
<li><strong>SELECT</strong><ul>
<li>InnoDB检查每行数据，确保他们符合两个标准<ul>
<li>InnoDB只查找早于当前事务版本的数据行（也就是数据行的版本必须小于等于事务的版本），这确保当前事务读取的行都是事务之前已经存才的，或者是由当前事务创建或修改的行。</li>
<li>行的删除操作的版本一定是未定义的或者大于当前事务版本号，确定了当前事务开始之前，行没有被删除</li>
</ul>
</li>
<li>符合了以上两点则返回查询结果</li>
</ul>
</li>
</ul>
<p>InnoDB中的MVCC实现方式：</p>
<ul>
<li>事务以排它锁的形式修改原始数据</li>
<li>把修改前的数据存放于undo log，通过回滚指针与主数据关联</li>
<li>修改成功（commit）啥都不做，失败则恢复undo log中的数据（rollback）</li>
</ul>
<p>二者最本质的区别是，当修改数据时是否要排他锁定，如果锁定了还算不算是MVCC？ </p>
<p>Innodb的实现真算不上MVCC，因为并没有实现核心的多版本共存，undo log中的内容只是串行化的结果，记录了多个事务的过程，不属于多版本共存。但理想的MVCC是难以实现的，当事务仅修改一行记录使用理想的MVCC模式是没有问题的，可以通过比较版本号进行回滚；但当事务影响到多行数据时，理想的MVCC据无能为力了。</p>
<p>比如，如果Transaciton1执行理想的MVCC，修改Row1成功，而修改Row2失败，此时需要回滚Row1，但因为Row1没有被锁定，其数据可能又被Transaction2所修改，如果此时回滚Row1的内容，则会破坏Transaction2的修改结果，导致Transaction2违反ACID。</p>
<p>理想MVCC难以实现的根本原因在于企图通过乐观锁代替二段提交。修改两行数据，但为了保证其一致性，与修改两个分布式系统中的数据并无区别，而二提交是目前这种场景保证一致性的唯一手段。二段提交的本质是锁定，乐观锁的本质是消除锁定，二者矛盾，故理想的MVCC难以真正在实际中被应用，Innodb只是借了MVCC这个名字，提供了读的非阻塞而已。</p>
<h5 id="4-2-4-一致性"><a href="#4-2-4-一致性" class="headerlink" title="4.2.4 一致性"></a>4.2.4 一致性</h5><p>一致性是指事务执行结束后，<strong>数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态</strong>。数据库的完整性约束包括但不限于：实体完整性（比如行的主键存在且唯一）、列完整性（如字段的类型，大小，长度要符合要求）、外键约束、用户自定义完整性。</p>
<p>可以说，一致性是事务追求的最终目标：前面提到的原子性，持久性和隔离性，都是为了保证数据库状态的一致性。此外，除了数据库层面的保障，一致性的实现也需要应用层面进行保障。</p>
<p>实现一致性的措施：</p>
<ul>
<li>保证原子性、持久性和隔离，如果这些特性无法保证，事务的一致性也无法保证。</li>
<li>数据库本身提供保障，例如不允许向整型列插入字符串值，字符串长度不能超过列的限制等</li>
<li>应用层面进行保障，例如如果转账操作只扣除转账者的余额，而没有增加接受者的余额，无论数据库实现的多么完美，也无法保证状态的一致性。</li>
</ul>
<p>​            </p>
<h2 id="5-锁"><a href="#5-锁" class="headerlink" title="5. 锁"></a>5. 锁</h2><p>MySQL里面的锁大致可以分成 <strong>全局锁</strong>、<strong>表级锁</strong>和<strong>行级锁</strong>这三类。</p>
<h3 id="全局锁"><a href="#全局锁" class="headerlink" title="全局锁"></a>全局锁</h3><p>全局锁就是对整个数据库实例加锁。</p>
<p><strong>全局锁的典型使用场景：做全库逻辑备份</strong>，也就是把整库每个表都select出来存成文本。</p>
<p>但是全局锁会导致整个库进入只读状态，在实际的线上任务中，这很危险。</p>
<h3 id="表级锁"><a href="#表级锁" class="headerlink" title="表级锁"></a>表级锁</h3><p>Mysql中的表级别的锁有两种：</p>
<ul>
<li><p>表锁</p>
<ul>
<li>MyISAM引擎<ul>
<li><strong>表共享读锁</strong><ul>
<li>不会阻塞其他线程对同一个表的读操作请求，但会阻塞其他线程的写操作请求；</li>
</ul>
</li>
<li><strong>表独占写锁</strong><ul>
<li>一旦表被加上独占写锁，那么无论其他线程是读操作还是写操作，都会被阻塞。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>默认情况下，写锁比读锁具有更高的优先级；当一个锁释放后，那么它会优先相应写锁等待队列中的请求，然后再是读锁中等待的获取锁的请求。</p>
<ul>
<li>InnoDB引擎<ul>
<li>表锁———<strong>-意向锁</strong><ul>
<li>由于表锁和行锁虽然作用范围不同，但是会相互冲突。当你要加表锁时，势必要先遍历表的所有记录，判断是否有<strong>排它锁</strong>。这种遍历检查的方式显然是一种低效的方式。InnoDB引入了<strong>意向锁</strong>，来检测表锁和行锁的冲突。</li>
<li>意向锁也是表级锁，分为**读意向锁(IS)<strong>，和</strong>写意向锁(IX)**。当事务要在记录上加行锁时，要首先在表上加意向锁。这样判断表中是否有记录正在加锁就很简单了，只要看下表上是否有意向锁就行了。从而就能提升效率。</li>
<li>意向锁之间不会产生冲突，它只会阻塞表级读锁或写锁。意向锁不与行锁发生冲突。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>元数据锁（MDL）</p>
<ul>
<li>元数据锁MDL是系统默认加的</li>
<li>当表的结构发生变化时，这个锁就会生效</li>
</ul>
</li>
</ul>
<blockquote>
<p>表锁不会出现死锁，发生锁的冲突几率高，并发低</p>
<p>MyISAM在执行查询语句（select）前，会自动给涉及的所有表加读锁，在执行insert、delete和update前，会自动给涉及的表加写锁。</p>
<p>读锁会阻塞写，写锁会阻塞读和写</p>
<ul>
<li>MyISAM表的读操作，不会阻塞其他线程对同一表的读请求，但会阻塞对同一表的写请求。只有当读锁释放后，才会执行其他进程的写操作。</li>
<li>对MyISAM表的写操作，会阻塞其他进程对同一表的读和写操作，只有当写锁释放后，才会执行其他进程的读写操作。</li>
</ul>
<p>MyISAM引擎不适合做写为主表的引擎，因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永远阻塞。</p>
</blockquote>
<h3 id="行锁"><a href="#行锁" class="headerlink" title="行锁"></a>行锁</h3><p><strong>InnoDB中的行锁</strong></p>
<p>InnoDB实现了一下两种类型的行锁：</p>
<ul>
<li>共享锁（S）：加了锁的记录，所有事务都能去读取但不能修改，同时阻止其他事务获得相同数据集的排它锁</li>
<li>排它锁（X）：允许已经获得排它锁的事务去更新数据，阻止其他事务获得相同数据集的共享锁和排它锁。</li>
</ul>
<p><strong>锁模式的兼容矩阵</strong></p>
<p>下面表显示了了各种锁之间的兼容情况：</p>
<table>
<thead>
<tr>
<th></th>
<th>X</th>
<th>IX</th>
<th>S</th>
<th>IS</th>
</tr>
</thead>
<tbody><tr>
<td>X</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>IX</td>
<td></td>
<td>兼容</td>
<td></td>
<td>兼容</td>
</tr>
<tr>
<td>S</td>
<td></td>
<td></td>
<td>兼容</td>
<td>兼容</td>
</tr>
<tr>
<td>IS</td>
<td></td>
<td>兼容</td>
<td>兼容</td>
<td>兼容</td>
</tr>
</tbody></table>
<p>（注意上面的X与S是说表级的X锁和S锁，意向锁不和行级锁发生冲突）</p>
<p>如果一个事务请求的锁模式与当前的锁兼容，InnoDB就将请求的锁授予该事务；如果两者不兼容，那么该事务就需要等待锁的释放。</p>
<p><font color=red>注意：</font></p>
<p><font color=green>InnoDB的行锁是作用在索引上的，哪怕建表的时候没有定义一个索引，InnoDB也会创建一个聚簇索引并将其作为锁作用的索引。</font></p>
<p><font color=green>行锁必须有索引才能实现，否则会自动锁全表。</font></p>
<ul>
<li>两个事务不能锁同一个索引</li>
<li>insert，delete，update在事务中都会自动默认加上排它锁</li>
</ul>
<h3 id="间隙锁"><a href="#间隙锁" class="headerlink" title="间隙锁"></a>间隙锁</h3><h2 id="6-性能分析与优化"><a href="#6-性能分析与优化" class="headerlink" title="6. 性能分析与优化"></a>6. 性能分析与优化</h2><h3 id="Exlain查询执行计划"><a href="#Exlain查询执行计划" class="headerlink" title="Exlain查询执行计划"></a>Exlain查询执行计划</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">explain <span class="keyword">select</span> </span><br><span class="line">								id</span><br><span class="line">								,name</span><br><span class="line">								,addr</span><br><span class="line">				<span class="keyword">from</span>		t1</span><br><span class="line">        <span class="keyword">where</span>		t1.id <span class="operator">=</span> <span class="number">20</span></span><br></pre></td></tr></table></figure>

<p>通过在sql前面添加explain关键字即可查询sql的执行计划</p>
<p><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Faliyunzixunbucket.oss-cn-beijing.aliyuncs.com%2Fjpg%2F6dd68b173df7809bc8dc27a30937a22d.jpg%3Fx-oss-process%3Dimage%2Fresize%2Cp_100%2Fauto-orient%2C1%2Fquality%2Cq_90%2Fformat%2Cjpg%2Fwatermark%2Cimage_eXVuY2VzaGk%3D%2Ct_100&refer=http%3A%2F%2Faliyunzixunbucket.oss-cn-beijing.aliyuncs.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1630634532&t=cc5ce4c5d99be9f2529373c3cd481029" alt="img"></p>
<p>如上图，sql的执行计划共有10个字段</p>
<ol>
<li><p><strong>id</strong></p>
<blockquote>
<p>表的查询顺序，需要越大表就越先被查询，相同id，表从上之下依次执行</p>
</blockquote>
</li>
<li><p><strong>select_type</strong></p>
<blockquote>
<p>查询类型：</p>
<p><strong>SIMPLE</strong>：不带有任何复杂查询</p>
<p><strong>PRIMARY</strong>：查询中若包含任何复杂的子部分，最外层查询则被标记为PRIMARY</p>
<p><strong>SUBQUERY</strong>：在select或where列表中包含了子查询</p>
<p><strong>DERIVED</strong>衍生：在from列表中包含的子查询标记为DERIVEN（MySql会递归执行这些子查询，把结果放在临时表里）</p>
<p><strong>UNION</strong>：若第二个select出现在union之后，则被标记为UNION</p>
<p>​                若UNION包含在from子句的子查询中，外层select将被标记为：DERIVED</p>
<p><strong>UNION RESULT</strong>：从UNION表获取结果的select</p>
</blockquote>
</li>
<li><p><strong>table</strong></p>
<ul>
<li>显示这一行数据是关于哪张表的</li>
</ul>
</li>
<li><p><strong>type</strong></p>
<blockquote>
<p><strong>查询的访问类型</strong></p>
<p>常见的访问类型：system–const–eq_ref–ref–range–index–ALL</p>
<ol>
<li><strong>System</strong>:表中只有一行记录，这是const类型的特例</li>
<li><strong>const</strong>：表示通过索引一次就找到了，const用于比较primary key 或者unique索引，因为只匹配一行数据，所以很快</li>
<li><strong>eq_ref</strong>：唯一性索引扫描，对于每个索引键，表中只用一条记录与之匹配。常见于主键或唯一性索引</li>
<li><strong>ref</strong>：非唯一性索引扫描，返回匹配某个单独值的所有行，本质上也是一种索引访问。</li>
<li><strong>range</strong>：指索引给定范围的行，使用一个索引来选择行；between、&lt;、&gt;和in等的查询</li>
<li><strong>index</strong>：出现index是sql使用了索引但是没有通过索引进行过滤，一般是使用了覆盖索引或者利用索引进行排序，分组等。</li>
<li><strong>ALL</strong>：全表扫描</li>
</ol>
<p>index 与 ALL的区别：都是全表扫描，但是index遍历的只有索引树，而ALL是从硬盘中读取全部的表数据，前者的速度要快于后者。</p>
</blockquote>
<p>一般情况下，查询至少达到range级别，最好达到ref级别</p>
<ol start="5">
<li><p><strong>possible_keys</strong></p>
<blockquote>
<p>显示可能应用在这张表中的索引，一个或者多个</p>
<p>查询涉及到的字段若存在索引，该索引将被列出</p>
<p><strong>但是不一定被查询实际使用</strong></p>
</blockquote>
</li>
<li><p><strong>key</strong></p>
<blockquote>
<p>实际使用的索引。如果为NULL，则没有使用索引</p>
<p>查询中若使用了覆盖索引，则该索引仅出现在key列表中</p>
</blockquote>
</li>
<li><p><strong>key_len</strong></p>
<blockquote>
<p>表示索引中使用的字节数，通过该列计算查询中使用的索引长度，在不损失精确性的情况下，长度越短越好。</p>
<p>key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的。</p>
</blockquote>
</li>
<li><p><strong>ref</strong></p>
<blockquote>
<p>显示索引的哪一列被使用了，如果可能的话，是一个常数，哪些列或常量被用于查找索引列上的值</p>
</blockquote>
</li>
<li><p><strong>rows</strong></p>
<blockquote>
<p>根据表统计信息即索引选取情况，大致估算出找到所需的记录所需要读取的行数</p>
</blockquote>
</li>
<li><p><strong>Extra</strong></p>
<ol>
<li><strong>using filesort</strong><ul>
<li>说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取</li>
<li>mysql中无法利用索引完成的排序操作称为：“文件排序”</li>
</ul>
</li>
<li><strong>using temporary</strong><ul>
<li>使用了临时表保存中间结果，mysql在对查询结果排序时，使用临时表。常见于排序order by和分组查询group by</li>
</ul>
</li>
<li><strong>using index</strong><ul>
<li>using index表示相应的select操作中使用了覆盖索引，避免访问了表的数据行，效率不错！</li>
<li>如果同时出现using where，表名索引被用来执行索引键值的查找；如果没有同时出现using where，表名索引只是用来读取数据而非利用索引执行查找</li>
<li>利用索引进行了排序或者查找</li>
</ul>
</li>
<li><strong>using where</strong></li>
<li><strong>using join buffer</strong></li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="7-Mysql主从复制-amp-集群"><a href="#7-Mysql主从复制-amp-集群" class="headerlink" title="7. Mysql主从复制 &amp; 集群"></a>7. Mysql主从复制 &amp; 集群</h2><h3 id="7-1-MySql主从复制"><a href="#7-1-MySql主从复制" class="headerlink" title="7.1 MySql主从复制"></a>7.1 MySql主从复制</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44617722/article/details/111996883">docker 搭建mysql主从复制</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/felix-h/p/11072743.html">mysql授权用户</a></p>
<p>MySQL主从复制是指数据可以从一个MySQL数据库服务器主节点复制到一个或多个从节点。</p>
<p>MySQL主从复制默认采用异步复制方式，这样从节点不用一直访问主服务器来更新自己的数据，数据的更新可以在远程连接上进行，从节点可以复制主数据库中的所有数据库或者特定的数据，或者特定的表。</p>
<p><strong>Mysql主从复制原理</strong></p>
<ol>
<li>master服务器将数据的改变记录到二进制日志binlog日志，当master上的数据发生改变时，则将其改变写入二进制日志中；</li>
<li>slave服务器会在一定时间间隔内对master二进制日志进行探测其是否发生改变，如果发生改变，则开始一个I/O Thread请求master二进制事件。</li>
<li>同时主节点为每个I/O线程启动一个dump线程，用于向其发送二进制事件，并保存至从节点本地的<strong>中继日志中（relay log）</strong>，从节点将启动SQL线程从中继日志中读取二进制日志，在本地重放，似的其数据和主节点的保持一致，最后I/O Thread和SQL Thread将进入睡眠，等待下一下被唤醒。</li>
</ol>
<ul>
<li>从库会生成两个线程：一个I/O线程，一个SQL线程</li>
<li>I/O线程会去请求主库的binlog，并将得到的binlog写到本地的relay-log（中继日志）文件中</li>
<li>主库会生成一个log dump线程，用来给从库I/O线程传binlog；</li>
<li>SQL线程会读取relay log文件中的日志，并解析成sql语句逐一执行；</li>
</ul>
<p><strong>注意</strong></p>
<ol>
<li>master将操作语句记录到binlog日志中，然后授予slave远程连接的权限（master一定要开启binlog二进制日志功能；通常为了数据安全考虑，slave也开启binlog功能）。</li>
<li>slave开启两个线程：IO线程和SQL线程。其中：IO线程负责读取master的binlog内容到中继日志relay log里；SQL线程负责从relay log日志里读出binlog内容，并更新到slave的数据库里，这样就能保证slave数据和master数据保持一致了。 </li>
<li>Mysql复制至少需要两个Mysql的服务，当然Mysql服务可以分布在不同的服务器上，也可以在一台服务器上启动多个服务。</li>
<li>Mysql复制最好确保master和slave服务器上的Mysql版本相同（如果不能满足版本一致，那么要保证master主节点的版本低于slave从节点的版本）</li>
<li>master和slave两节点间时间需同步</li>
</ol>
<p><img src="https://pic3.zhimg.com/80/v2-cf37bafd8a121454b5488c53ff2e0b2e_1440w.jpg" alt="img"></p>
<p>具体细节</p>
<ol>
<li>从库通过手工执行change master to 语句连接主库，提供了连接的用户一切条件（user 、password、port、ip），并且让从库知道，二进制日志的起点位置（file名 position 号）； start slave</li>
<li>从库的IO线程和主库的dump线程建立连接。</li>
<li>从库根据change master to 语句提供的file名和position号，IO线程向主库发起binlog的请求。</li>
<li>主库dump线程根据从库的请求，将本地binlog以events的方式发给从库IO线程。</li>
<li>从库IO线程接收binlog events，并存放到本地relay-log中，传送过来的信息，会记录到<a href="https://link.zhihu.com/?target=http://master.info">master.info</a>中</li>
<li>从库SQL线程应用relay-log，并且把应用过的记录到<a href="https://link.zhihu.com/?target=http://relay-log.info">relay-log.info</a>中，默认情况下，已经应用过的relay 会自动被清理purge</li>
</ol>
<p><strong>主从复制优缺点</strong></p>
<ul>
<li>读写分离：一主多从，主写，从读，分散压力。</li>
<li>缺点<ul>
<li>数据库服务存在单点故障（主库所在机器可能宕机）</li>
<li>数据库服务器资源无法满足增长的读写请求</li>
<li>高峰时数据库连接数经常超过上线</li>
<li>同步机制为<strong>异步</strong></li>
</ul>
</li>
</ul>
<h3 id="7-2-Mysql集群"><a href="#7-2-Mysql集群" class="headerlink" title="7.2 Mysql集群"></a>7.2 Mysql集群</h3><h2 id="8-补充"><a href="#8-补充" class="headerlink" title="8. 补充"></a>8. 补充</h2><h3 id="8-1-SQL的生命周期"><a href="#8-1-SQL的生命周期" class="headerlink" title="8.1 SQL的生命周期"></a>8.1 SQL的生命周期</h3><img src="https://github.com/xiaowodi/Resources/blob/main/images/gitImages/Mysql%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84.png?raw=true" alt="Mysql基本架构.png" style="zoom:30%;" />





<ol>
<li><p>首先客户端向服务器提交要执行的SQL；</p>
<p>客户端需要通过<strong>连接器</strong>连接到<strong>Server</strong>，并验证这个客户端的权限等</p>
</li>
<li><p>连接建立完成后，就可以执行sql，执行逻辑的第二步就是要<strong>查询缓存</strong>，如果缓存中有之前查询的结果，就直接返回给客户端。（缓存中类似于Key-Value的形式）</p>
</li>
<li><p>如果缓存没有命中，接下来就需要<strong>分析器</strong>，经过<em>词法分析</em>，<em>语法分析</em>，来分析这个sql语句是否符合sql语法规范。</p>
</li>
<li><p>对于可以执行的sql要经过<strong>优化器</strong>进行优化</p>
</li>
<li><p>优化后的sql就会到<strong>执行器</strong>中，执行这个sql逻辑</p>
<ul>
<li>开始执行的时候，要先判断一下客户端对这个表有没有执行查询的权限，如果没有，就会返回权限错误。</li>
<li>如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。</li>
</ul>
</li>
</ol>
<h1 id="Hadoop复习"><a href="#Hadoop复习" class="headerlink" title="Hadoop复习"></a>Hadoop复习</h1><h2 id="1-分布式文件存储系统HDFS"><a href="#1-分布式文件存储系统HDFS" class="headerlink" title="1. 分布式文件存储系统HDFS"></a>1. 分布式文件存储系统HDFS</h2><h3 id="1-1-HDFS的机架感知策略"><a href="#1-1-HDFS的机架感知策略" class="headerlink" title="1.1 HDFS的机架感知策略"></a>1.1 HDFS的机架感知策略</h3><ul>
<li>机架：存放服务器的架子，也叫机柜。一般来说一个机房有很多机柜，每个机柜有很多服务器</li>
</ul>
<p><strong>副本存放策略</strong></p>
<p>HDFS分布式文件系统的内部有一个副本存放策略：以默认的副本数=3为例：</p>
<ol>
<li>第一个副本块存本机</li>
<li>第二个副本块存放在跟本机同机架内的其他服务器节点</li>
<li>第三个副本块存放在不同于本机架的一个服务器节点上</li>
</ol>
<p>==好处：==</p>
<ol>
<li>如果本机数据损坏或者丢失，那么客户端可以从同机架的相邻节点获取数据，速度肯定要比跨机架获取数据要快。</li>
<li>如果本机所在的机架出现问题，那么之前在存储的时候没有把所有副本都放在一个机架内，这就能保证数据的安全性，此种情况出现，就能保证客户端也能取到数据。</li>
</ol>
<p>HDFS为了降低整体的网络带宽消耗和读取延时，HDFS集群一定会让客户端尽量去读取近的副本，那么按照以上解释的副本存放策略：</p>
<ol>
<li>如果在本机有数据，那么直接读取；</li>
<li>如果在跟本机同机架的服务器节点中有该数据块，则直接读取</li>
<li>如果该HDFS集群跨多个数据中心，那么客户端也一定会优先读取本数据中心的数据。</li>
</ol>
<p>但是HDFS是如何确定两个节点是否属于同一个机架，如何确定不同服务器跟客户端的远近呢？那就是<strong>机架感知</strong></p>
<h3 id="1-2-NameNode-amp-DataNode-amp-Secondary-NameNode"><a href="#1-2-NameNode-amp-DataNode-amp-Secondary-NameNode" class="headerlink" title="1.2 NameNode &amp; DataNode &amp; Secondary NameNode"></a>1.2 <strong>NameNode &amp; DataNode &amp; Secondary NameNode</strong></h3><p>整个HDFS集群由Namenode和Datanode构成master-worker（主从）模式。Namenode负责构建命名空间，管理文件的元数据等，而Datanode负责实际存储数据，负责读写工作。</p>
<h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a><strong>NameNode</strong></h4><p>NameNode存放文件系统树以及所有文件、目录的元数据。</p>
<p>元数据持久化为2种形式：</p>
<ul>
<li>namespace image</li>
<li>edit log</li>
</ul>
<p>在HDFS中，Namenode可能成为集群的单点故障，Namenode不可用时，整个文件系统是不可用的。HDFS针对单点故障提供了2种解决机制：<br>1）<strong>备份持久化元数据</strong><br>将文件系统的元数据同时写到多个文件系统， 例如同时将元数据写到本地文件系统及NFS。这些备份操作都是同步的、原子的。</p>
<p>2）<strong>Secondary Namenode</strong><br>Secondary节点定期合并主Namenode的namespace image和edit log， 避免edit log过大，通过创建检查点checkpoint来合并。它会维护一个合并后的namespace image副本， 可用于在Namenode完全崩溃时恢复数据。</p>
<p>Secondary Namenode通常运行在另一台机器，因为合并操作需要耗费大量的CPU和内存。其数据落后于Namenode，因此当Namenode完全崩溃时，会出现数据丢失。 通常做法是拷贝NFS中的备份元数据到Second，将其作为新的主Namenode。<br>在HA（High Availability高可用性）中可以运行一个Hot Standby，作为热备份，在Active Namenode故障之后，替代原有Namenode成为Active Namenode。</p>
<h4 id="1-3-SecondaryNameNode工作原理"><a href="#1-3-SecondaryNameNode工作原理" class="headerlink" title="1.3 SecondaryNameNode工作原理"></a>1.3 <strong>SecondaryNameNode工作原理</strong></h4><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u010848845/article/details/118491365">别扯了，Secondary NameNode工作原理就看这家</a></p>
<p><img src="https://img-blog.csdnimg.cn/20210705153740561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA4NDg4NDU=,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>1 ）<strong>第一阶段： NameNode 启动</strong></p>
<p>（ 1 ）第一次启动 NameNode 格式化后，创建 Fsimage 和 Edits 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。<br>（ 2 ）客户端对元数据进行增删改的请求。<br>（ 3 ） NameNode 记录操作日志，更新滚动日志。<br>（ 4 ） NameNode 在内存中对元数据进行增删改。</p>
<p>2 ）<strong>第二阶段： Secondary NameNode 工作</strong></p>
<p>（ 1 ） Secondary NameNode 询问 NameNode 是否需要 CheckPoint 。直接带回 NameNode<br>是否检查结果。<br>（ 2 ） Secondary NameNode 请求执行 CheckPoint 。<br>（ 3 ） NameNode 滚动正在写的 Edits 日志。<br>（ 4 ）将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode 。<br>（ 5 ） Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。<br>（ 6 ）生成新的镜像文件 fsimage.chkpoint 。<br>（ 7 ）拷贝 fsimage.chkpoint 到 NameNode 。<br>（ 8 ） NameNode 将 fsimage.chkpoint 重新命名成 fsimage 。</p>
<h3 id="1-4-DataNode"><a href="#1-4-DataNode" class="headerlink" title="1.4 DataNode"></a>1.4 DataNode</h3><p>数据节点负责存储和提取Block，读写请求可能来自nameNode，也可能直接来自客户端。数据节点周期性向NameNode汇报自己节点上所有存储的BLock相关信息。</p>
<h3 id="1-5-HDFS-设计目标"><a href="#1-5-HDFS-设计目标" class="headerlink" title="1.5 HDFS 设计目标"></a>1.5 HDFS 设计目标</h3><ul>
<li>存储非常大的文件</li>
<li>采用流式的数据访问方式</li>
</ul>
<p><strong>不适合的应用类型：</strong></p>
<ul>
<li>低延时的数据访问<ul>
<li>对延时要求在毫秒级别的应用，不适合采用HDFS。</li>
</ul>
</li>
<li>大量小文件<ul>
<li>文件的元数据（如目录结构，文件block的节点列表，block-node mapping）保存在NameNode的内存中，整个文件系统的文件数量会受限于NameNode的内存大小。</li>
</ul>
</li>
<li>多方读写，需要任意的文件修改<ul>
<li>HDFS采用追加的方法写入数据，不支持文件任意offset修改。不支持多个写入器。</li>
</ul>
</li>
</ul>
<h3 id="1-6-HDFS的文件存储格式"><a href="#1-6-HDFS的文件存储格式" class="headerlink" title="1.6 HDFS的文件存储格式"></a>1.6 HDFS的文件存储格式</h3><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wqbin/p/14635480.html">HDFS的文件存储格式</a></p>
<p>可分为<strong>行式存储</strong>和<strong>列式存储</strong>两大类。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/6450093-0c5b3f7a2eceaaef.jpg" alt="img"></p>
<h4 id="行式存储"><a href="#行式存储" class="headerlink" title="行式存储"></a>行式存储</h4><p>同一行的数据存储在一起，即连续存储。例如：<code>SequenceFile</code>,  <code>MapFile</code>, <code>Avro</code>, <code>Datafile</code>等格式都是使用行式存储的。</p>
<p>如果只需要访问行的一小部分列数据，也需要将整行的数据读入内存。举个例子：一行中有十列的数据，取数的时候只需要取两列的数据，那么就需要把整个行中的所有数据都需要读取出来。</p>
<ul>
<li><strong>SequenceFile</strong></li>
<li><strong>MapFile</strong></li>
<li><strong>Avro</strong></li>
<li><strong>DataFile</strong></li>
</ul>
<h4 id="列式存储"><a href="#列式存储" class="headerlink" title="列式存储"></a>列式存储</h4><p>整个文件被切割为若干列数据，每一列数据一起存储。<code>Parquet</code>, <code>RCFile</code>, <code>ORCFile</code>.面对列式存储的数据，可以跳过不需要的列，适合于只处理行的一小部分字段的情况。但是这种格式的读写需要更多的内存空间，因为需要缓存行在内存中（为了获取多行中的某一列）。</p>
<p>同时不适合流失写入，因为一旦写入失败，当前文件无法恢复，而面对行的数据在写入失败时，可以重新同步到最后一个同步点。</p>
<ul>
<li><strong>Parquet</strong></li>
<li><strong>RCFile</strong></li>
<li><strong>ORCFile</strong></li>
</ul>
<p><img src="https://upload-images.jianshu.io/upload_images/6450093-dbe2595ee1e293b1.png" alt="img"></p>
<p>一般情况下，离线处理中的宽表会有很多的字段，而在进行分析的时候，只需要一小部分字段即可，所以实际生产中，列式存储的情况比较多。</p>
<p><strong>Parquet与ORC的对比</strong></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly95cWZpbGUuYWxpY2RuLmNvbS9lOGI3ODEzNzIyMGM4OTUyOGVjZDA0NDY0NjJiZDI3Y2FmNGRmNTRkLnBuZw?x-oss-process=image/format,png" alt="image"></p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly95cWZpbGUuYWxpY2RuLmNvbS9iZTU0N2YxNGY3YmZhZGZlNGQyMjJkOTFhNDIyMTk4NmJkNzU3ZDk2LnBuZw?x-oss-process=image/format,png" alt="image"></p>
<p>小总结：ORC的压缩能力强，支持ACID，支持更新，删除等操作。但是嵌套式结构实现比较复杂。</p>
<h3 id="1-7-HDFS的读写流程"><a href="#1-7-HDFS的读写流程" class="headerlink" title="1.7 HDFS的读写流程"></a>1.7 HDFS的读写流程</h3><h4 id="1-7-1-HDFS-的-写流程"><a href="#1-7-1-HDFS-的-写流程" class="headerlink" title="1.7.1 HDFS 的 写流程"></a>1.7.1 HDFS 的 写流程</h4><p><img src="https://www.pianshen.com/images/782/2d5555fa9bb1b9bda4614b97abcc7d0e.png" alt="img"></p>
<p>客户端发起写请求到NameNode，NameNode返回可用的资源，客户端根据资源使用情况对要写入的数据分块，逐一上传块到DataNode，DataNode获取上传块数据并写入磁盘，完成后报告给NameNode块信息，同时也告诉客户端写入成功，客户端继续后续块的写入，在此期间NameNode接受到DataNode块写入完成信息之后备份数直到满。</p>
<ol>
<li>首先客户端发起写请求到NameNode，NameNode检查目录是否存在，父目录是否存在。</li>
<li>NameNode通知客户端是否可以上传</li>
<li>client长传时，先对文件进行分块，默认block为128M。client向NameNode请求第一个block需要传输到哪个DataNode上。</li>
<li>NameNode接受到请求，返回可用的DataNode。假设备份副本数为3，那么就返回三个可用的DataNode。（client同机器d1,同机架的另一台服务器的d2， 不同机架的另一台服务器的d3）</li>
<li>client请求一台DataNode建立block传输管道，第一个datanode接受到请求后会继续调用第二个datanode，然后第二个datanode调用第三个datanode，将整个pipeline建立完成，逐级返回客户端（这个过程是串联的）</li>
<li>三个datanode逐级应答客户端。</li>
<li>客户端开始往d1节点上传第一个block，然后上传到d2，接下来是d3</li>
<li>当第一个block传输完后，客户端再次请求namenode上传第二个接收的block的datanode节点，直到最后一个block上传完成为止。</li>
</ol>
<h4 id="1-7-2-HDFS-的-读流程"><a href="#1-7-2-HDFS-的-读流程" class="headerlink" title="1.7.2 HDFS 的 读流程"></a>1.7.2 HDFS 的 读流程</h4><p><img src="https://www.pianshen.com/images/955/28906ddfe55a52f88180366de7c6b3bb.png" alt="img"></p>
<p>客户端发起读请求到NameNode，NameNode返回可使用的DataNode，客户端根据返回的资源到对应的DataNode上读取块数据，客户端合并文件数据。</p>
<ol>
<li>client和namenode通信查询元数据（block所在的datanode节点），找到所在的datanode服务器</li>
<li>挑选一台datanode（就近原则，然后随机）服务器请求建立socket流</li>
<li>datanode发送数据，从磁盘读取数据放入流，以packet为单位来做校验。</li>
<li>客户端以packet为单位接收，先在本地缓存，然后写入目标文件，最后合并文件。</li>
</ol>
<h2 id="2-分布式计算框架MapReduce"><a href="#2-分布式计算框架MapReduce" class="headerlink" title="2. 分布式计算框架MapReduce"></a>2. 分布式计算框架MapReduce</h2><h3 id="2-1-MapReduce工作流程"><a href="#2-1-MapReduce工作流程" class="headerlink" title="2.1 MapReduce工作流程"></a>2.1 MapReduce工作流程</h3><h4 id="2-1-1-MapTask工作机制"><a href="#2-1-1-MapTask工作机制" class="headerlink" title="2.1.1 MapTask工作机制"></a>2.1.1 MapTask工作机制</h4><p><img src="https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726181803473-2052825806.png" alt="img"></p>
<ol>
<li><p><strong>MapTask</strong>收集Mapper中的map()方法每次输出的key-value值，放入到环形缓冲区中。</p>
<p>环形缓冲区默认大小<strong>100M</strong></p>
<p>环形缓冲区双向写入，一侧记录索引值，一侧记录真是的数据</p>
</li>
<li><p>环形缓冲区中的数据达到80%的时候，开始进行反向溢写</p>
</li>
<li><p>从缓冲区溢写出来的数据会根据<strong>分区器</strong>进行分区，且每个分区内，会通过<strong>快速排序</strong>，对key排序，保证每个分区中的数据是有序的。</p>
</li>
<li><p>接下来将缓冲区本次溢写出来的且分区内有序的数据落盘（多临时小文件），待数据都处理完后，多个溢出文件会被合并成大的溢出文件（这个过程通过<strong>归并排序</strong>，使得这个大的溢出文件内部也是有序的）。</p>
</li>
<li><p>如果MapTask开启了<strong>Combiner</strong>预聚合功能，那么在缓冲区溢出数据分区排序完之后，每个分区内会做一次预聚合的操作，将相同key的记录按照一定的规则进行聚合，然后落盘，合并。（开启预聚合功能可在一定程度上缓解数据倾斜带来的问题）</p>
</li>
<li><p>每个MapTask所在机器上都会输出对应的Map阶段的结果。</p>
</li>
</ol>
<h4 id="2-1-2-ReduceTask工作机制"><a href="#2-1-2-ReduceTask工作机制" class="headerlink" title="2.1.2 ReduceTask工作机制"></a>2.1.2 ReduceTask工作机制</h4><p><img src="https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726181824985-212928464.png" alt="img"></p>
<ol>
<li>每个<strong>ReduceTask</strong>从上阶段的各个MapTask所在机器上拷贝<strong>当前ReduceTask负责的分区数据</strong>到自己的缓冲区中（如果数据超过缓存区大小，则写到磁盘上）</li>
<li>对于来自多个MapTask上的数据进行<strong>归并排序</strong>，合并成一个文件，将具有相同key的数据排列在一起，这样就实现了按照key进行分组，也可称之为局部排序。</li>
<li>每组数据经过reduce()方法进行处理。</li>
<li>最终将计算结果写到HDFS上。</li>
</ol>
<h3 id="2-2-Shuffle机制"><a href="#2-2-Shuffle机制" class="headerlink" title="2.2 Shuffle机制"></a>2.2 Shuffle机制</h3><p><img src="https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726181459479-554913934.png" alt="img"></p>
<p>MapReduce整个Shuffle阶段横跨了MapTask和ReduceTask这两个任务阶段。</p>
<h2 id="3-集群资源管理器Yarn"><a href="#3-集群资源管理器Yarn" class="headerlink" title="3. 集群资源管理器Yarn"></a>3. 集群资源管理器Yarn</h2><h3 id="Yarn的基本架构"><a href="#Yarn的基本架构" class="headerlink" title="Yarn的基本架构"></a>Yarn的基本架构</h3><p><img src="https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726183159871-354972654.png" alt="img"></p>
<p>Yarn主要有ResourceManager、NodeManager、ApplicationMaster和Container等组件</p>
<ol>
<li><strong>ResourceManager</strong><ul>
<li>处理客户端请求</li>
<li>监控NodeManager</li>
<li>启动或监控ApplicationMaster</li>
<li>资源的分配与调度</li>
</ul>
</li>
<li><strong>NodeManager</strong><ul>
<li>管理单个节点上的资源</li>
<li>处理来自ResourceManager的命令</li>
<li>处理来自ApplicationMaster的命令</li>
</ul>
</li>
<li><strong>ApplicationMaster</strong><ul>
<li>负责数据的切分</li>
<li>为应用程序申请资源并分配给内部的任务</li>
<li>任务的监控与容错</li>
</ul>
</li>
<li><strong>Container</strong><ul>
<li>Container是Yarn中的资源抽象，它封装了某个节点上的多维资源，如内存，CPU，磁盘，网络等。</li>
</ul>
</li>
</ol>
<h3 id="Yarn工作机制"><a href="#Yarn工作机制" class="headerlink" title="Yarn工作机制"></a>Yarn工作机制</h3><p><img src="https://img2020.cnblogs.com/blog/1748663/202007/1748663-20200726183647821-1922505507.png" alt="img"></p>
<ol>
<li><font color=gree>作业提交</font><ol>
<li>Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。</li>
<li>Client会向RM申请一个<strong>Application</strong></li>
<li>RM给Client返回该job<strong>资源的提交路径</strong>和<strong>作业id</strong></li>
<li>Client提交<strong>jar包</strong>，<strong>切片信息</strong>和<strong>配置文件</strong>到指定的资源提交路径上。</li>
<li>Client提交完资源后，向RM申请运行<strong>ApplicationMaster</strong>。ApplicationMaster运行在刚刚申请的Container中。</li>
</ol>
</li>
<li><font color=gree>作业初始化</font><ol start="6">
<li>当RM收到Client的请求后，将请求封装成<strong>task</strong>，将该job添加到<strong>容量调度器中</strong></li>
<li>某个空闲的NM领取到该Job，创建Container，并产生<strong>ApplicationMaster</strong></li>
<li>下载Client提交的资源到NM本地</li>
</ol>
</li>
<li><font color=gree>任务分配</font><ol start="9">
<li><strong>ApplicationMaster</strong>向RM申请运行多个<strong>MapTask</strong>任务资源。</li>
<li>RM将运行<strong>MapTask</strong>任务分配给另外两个<strong>NodeManager</strong>，另外两个<strong>NodeManager</strong>分别领取到任务并创建容器。</li>
</ol>
</li>
<li><font color=gree>任务运行</font><ol start="11">
<li>AM向接收到任务的<strong>NodeManager</strong>发送程序启动脚本，这两个<strong>NodeManager</strong>分别启动<strong>MapTask</strong>，<strong>MapTask</strong>对数据分区排序。</li>
<li><strong>ApplicationMaster</strong>等待所有<strong>MapTask</strong>运行完毕后，向RM申请容器，运行ReduceTask</li>
<li><strong>ReduceTask</strong>向<strong>MapTask</strong>获取相应分区的数据。</li>
<li>程序运行完毕后，MR会向RM申请注销自己。</li>
</ol>
</li>
<li><font color=gree>进度和状态更新</font><ol start="15">
<li>Yarn中的任务将其进度和状态（包括counter）返回给应用管理器，客户端每秒向应用管理器请求进度更新，展示给用户。</li>
</ol>
</li>
<li><font color=gree>作业完成</font><ol start="16">
<li>除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。</li>
</ol>
</li>
</ol>
<h2 id="4-补充"><a href="#4-补充" class="headerlink" title="4. 补充"></a>4. 补充</h2><h3 id="4-1-Hadoop中的常用端口号"><a href="#4-1-Hadoop中的常用端口号" class="headerlink" title="4.1 Hadoop中的常用端口号"></a>4.1 Hadoop中的常用端口号</h3><table>
<thead>
<tr>
<th>服务</th>
<th>节点名</th>
<th>默认端口</th>
<th>配置</th>
<th>说明</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>HDFS</td>
<td>NameNode</td>
<td>50070</td>
<td>dfs.namenode.http-address</td>
<td>NameNode Web UI端口</td>
<td></td>
</tr>
<tr>
<td>HDFS</td>
<td>NameNode</td>
<td>8020</td>
<td>fs.defaultFS</td>
<td>NameNode API连接默认端口</td>
<td></td>
</tr>
<tr>
<td>HDFS</td>
<td>NameNode</td>
<td>9870</td>
<td>fs.defaultFS</td>
<td>3.0版本后8020==&gt;9870</td>
<td></td>
</tr>
<tr>
<td>HDFS</td>
<td>DataNode</td>
<td>50010</td>
<td>dfs.datanode.address</td>
<td>DAataNode初始化时向NameNode发送心跳</td>
<td></td>
</tr>
<tr>
<td>HDFS</td>
<td>DataNode</td>
<td>50020</td>
<td>dfs.datanode.ipc.address</td>
<td>DataNode ipc服务器地址和端口</td>
<td></td>
</tr>
<tr>
<td>HDFS</td>
<td>DataNode</td>
<td>50075</td>
<td>dfs.datanode.http.address</td>
<td>DataNode http服务器地址和端口</td>
<td></td>
</tr>
<tr>
<td>HDFS</td>
<td>DataNode</td>
<td>50475</td>
<td>dfs.datanode.https.address</td>
<td>DataNode https服务器地址和端口</td>
<td></td>
</tr>
<tr>
<td>Yarn</td>
<td>ResourceManager</td>
<td>8088</td>
<td>yarn.resourcemanager.webapp.address</td>
<td>RM Web应用程序的http地址和端口</td>
<td></td>
</tr>
<tr>
<td>YARN</td>
<td>JobHistory Server</td>
<td>19888</td>
<td>mapreduce.jobhistory.webapp.address</td>
<td>MapReduce JobHistory服务器WebUI的IP和端口</td>
<td></td>
</tr>
</tbody></table>
<h2 id="高可用"><a href="#高可用" class="headerlink" title="高可用"></a>高可用</h2><h1 id="Spark复习"><a href="#Spark复习" class="headerlink" title="Spark复习"></a>Spark复习</h1><h2 id="1-Spark-Core"><a href="#1-Spark-Core" class="headerlink" title="1. Spark Core"></a>1. Spark Core</h2><h3 id="1-1-转换算子"><a href="#1-1-转换算子" class="headerlink" title="1.1 转换算子"></a>1.1 转换算子</h3><ol>
<li><p><code>map()</code></p>
<ul>
<li>一进一出，每来一个元素执行一次map中的逻辑</li>
</ul>
</li>
<li><p><code>mapPartitions()</code></p>
<ul>
<li>每个分区调用一次，会把每个分区中的元素包装成Iterator</li>
</ul>
</li>
<li><p><code>mapPartitionsWithIndex()</code></p>
<ul>
<li>带有分区信息的<code>mapPartitions()</code></li>
</ul>
</li>
<li><p><code>flatMap()</code></p>
<ul>
<li>功能与 map类似，但是可以将集合进行 扁平化，可实现一进多出的或者不出的map</li>
</ul>
</li>
<li><p><code>glom()</code></p>
<ul>
<li>将每个分区内的元素合并成一个数组</li>
</ul>
</li>
<li><p><code>groupBy(func)</code></p>
<ul>
<li>按照func返回的值进行分组，将对应的值放入一个迭代器中Iterable；</li>
</ul>
</li>
<li><p><code>filter(func)</code></p>
<ul>
<li>根据func返回的布尔值进行过滤</li>
</ul>
</li>
<li><p><code>sample(withReplacement, fraction, seed)</code></p>
<ul>
<li>以指定的随机种子抽样出比例为<code>fraction</code>的数据（抽取到的数是<code>size*fraction</code>）, 需要注意的是得到的结果并不能保证准确的比例。</li>
<li><code>withReplacement</code>表示时抽出的数据是否放回，（<code>true  or  false</code>）</li>
<li><code>seed</code>用于指定随机数生成器 种子。一般用默认的，或者传入的当前的时间戳。</li>
</ul>
</li>
<li><p><code>distinct()</code></p>
<ul>
<li> 对RDD中的元素执行去重操作。</li>
</ul>
</li>
<li><p><code>coalesce(numPartitions)</code></p>
<ul>
<li>缩减分区数到执行的数量，用哦关于大数据集过滤后，提高小数据集的执行效率</li>
</ul>
</li>
<li><p><code>replacePartition(numPartitions)</code></p>
<ul>
<li>重新分区，底层调用coalesce，但是会指定shuffle，而coalesce默认不会进行shuflle。</li>
</ul>
</li>
<li><p><code>sortBy(func,[ascending], [numTasks])</code></p>
<ul>
<li><p>使用func先对数据进行处理，按照处理后的结果进行排序，默认为升序。</p>
</li>
<li><p>```scala<br>rdd1.sortBy(x=&gt;x, false) # 按照x进行降序排序。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">13. `pipe(command)`</span><br><span class="line"></span><br><span class="line">    - 把RDD中的每个元素通过管道的方式传递给shell脚本或命令。一个分区执行一次这个命令。</span><br><span class="line">    - `rdd1.pipe(&quot;/pipe&quot;)`</span><br><span class="line"></span><br><span class="line">14. 双Value型转换算则</span><br><span class="line"></span><br><span class="line">    1. 并集`union`</span><br><span class="line"></span><br><span class="line">       - rdd1.union(rdd2)</span><br><span class="line"></span><br><span class="line">    2. 差集`subtract(otherDataSet)`</span><br><span class="line"></span><br><span class="line">    3. 交集`intersection(otherDataset)`</span><br><span class="line"></span><br><span class="line">    4. 笛卡尔积`cartesian(otherDataset)`</span><br><span class="line"></span><br><span class="line">    5. 拉链`zip(otherDataSet)`</span><br><span class="line"></span><br><span class="line">       - 拉链操作，需要注意的是，在Spark中，两个RDD的元素的数量和分区数都必须相同，否则会抛出异常，其实质上就是要求每个分区的元素的数量相同。</span><br><span class="line"></span><br><span class="line">       - ```scala</span><br><span class="line">         scala&gt; val rdd1 = sc.parallelize(1 to 5)</span><br><span class="line">         rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at &lt;console&gt;:24</span><br><span class="line">         </span><br><span class="line">         scala&gt; val rdd2 = sc.parallelize(11 to 15)</span><br><span class="line">         rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[35] at parallelize at &lt;console&gt;:24</span><br><span class="line">         </span><br><span class="line">         scala&gt; rdd1.zip(rdd2).collect</span><br><span class="line">         res17: Array[(Int, Int)] = Array((1,11), (2,12), (3,13), (4,14), (5,15))</span><br><span class="line">         </span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<p><strong>key-value型转换算子</strong></p>
<ol>
<li><code>reduceByKey(func, [numTasks])</code><ul>
<li>按照key进行聚合运算</li>
</ul>
</li>
<li><code>groupByKey()</code><ul>
<li>按照key进行分组</li>
</ul>
</li>
</ol>
<p><code>reduceByKey 与 groupByKey</code>的区别</p>
<ul>
<li>reduceByKey按照key进行聚合，在shuffle之前有combine（预聚合的操作），结果是RDD[K, V]</li>
<li>groupByKey按照key进行分组，直接进行shuffle，没有预聚合的操作。</li>
</ul>
<ol start="3">
<li><code>foldByKey</code>(默认为left)<ul>
<li>可以指定初始值的聚合操作</li>
<li>返回值的RDD类型与初始值类型保持一致</li>
</ul>
</li>
</ol>
<p>reduceByKey与foldByKey的聚合逻辑（分区内的聚合逻辑和分区间的聚合逻辑都是一致的）</p>
<ol start="4">
<li><p><code>aggregateByKey(zero)(seqOP, combOp,[numTasks])</code></p>
<ul>
<li>可以指定初始值，指定分区内的聚合逻辑和分区间的聚合逻辑</li>
<li><strong>但是初始值还是需要人为来指定，</strong></li>
</ul>
</li>
<li><p><code>combineByKey</code></p>
<ul>
<li><p>既可以动态指定零值，还可以指定分区内的聚合逻辑和分区间的聚合逻辑。</p>
</li>
<li><p>```scala<br>  /**</p>
<ul>
<li>Simplified version of combineByKeyWithClassTag that hash-partitions the resulting RDD using the</li>
<li>existing partitioner/parallelism level. This method is here for backward compatibility. It</li>
<li>does not provide combiner classtag information to the shuffle.</li>
<li></li>
<li>@see [[combineByKeyWithClassTag]]</li>
<li>/<br>def combineByKey[C](<br>  createCombiner: V =&gt; C,<br>  mergeValue: (C, V) =&gt; C,<br>  mergeCombiners: (C, C) =&gt; C): RDD[(K, C)] = self.withScope {<br>combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners)(null)<br>}<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">   - 一个函数是 根据第一个 元素的value的值进行初始值的设定；</span><br><span class="line"></span><br><span class="line">   - 一个函数是，定义分区内元素聚合逻辑；</span><br><span class="line"></span><br><span class="line">   - 一个函数是，定义分区间元素聚合逻辑。</span><br><span class="line"></span><br><span class="line">6. `sortByKey()`</span><br><span class="line">   </span><br><span class="line">   - 按照key值进行排序</span><br><span class="line">7. `cogroup`算子</span><br><span class="line">   </span><br><span class="line">   - cogroup算子 操作两个Key-Value形式的RDD， 最后将两个RDD合并成一个RDD，这个RDD的形式：RDD[(K, (Iterable[V], Iterable[W]))]</span><br><span class="line">   </span><br><span class="line">8. `join`</span><br><span class="line"></span><br><span class="line">   ```scala</span><br><span class="line">   import org.apache.spark.rdd.RDD</span><br><span class="line">   import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">   </span><br><span class="line">   object Join &#123;</span><br><span class="line">   	def main(args: Array[String]): Unit = &#123;</span><br><span class="line">   		val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;AggregateByKey&quot;)</span><br><span class="line">   		val sc = new SparkContext(conf)</span><br><span class="line">   		val rdd1: RDD[(Int, String)] = sc.parallelize(Array((1, &quot;a&quot;), (1, &quot;b&quot;), (2, &quot;c&quot;), (4, &quot;d&quot;)))</span><br><span class="line">   		val rdd2: RDD[(Int, String)] = sc.parallelize(Array((1, &quot;aa&quot;), (1, &quot;dd&quot;), (3, &quot;bb&quot;), (2, &quot;cc&quot;)))</span><br><span class="line">   		//TODO 1.内连接</span><br><span class="line">   		val rdd_join: RDD[(Int, (String, String))] = rdd1.join(rdd2)</span><br><span class="line">   		//TODO 2.左外连接</span><br><span class="line">   		val rdd_leftOuterJoin: RDD[(Int, (String, Option[String]))] = rdd1.leftOuterJoin(rdd2)</span><br><span class="line">   		//TODO 3.右外连接</span><br><span class="line">   		val rdd_rightOuterJoin: RDD[(Int, (Option[String], String))] = rdd1.rightOuterJoin(rdd2)</span><br><span class="line">   		//TODO 4.全外连接</span><br><span class="line">   		val rdd_fullOuterJoin: RDD[(Int, (Option[String], Option[String]))] = rdd1.fullOuterJoin(rdd2)</span><br><span class="line">   		println(rdd_join.collect().toList)</span><br><span class="line">   		println(rdd_leftOuterJoin.collect().toList)</span><br><span class="line">   		println(rdd_rightOuterJoin.collect().toList)</span><br><span class="line">   		println(rdd_fullOuterJoin.collect().toList)</span><br><span class="line">   	&#125;</span><br><span class="line">   &#125;</span><br><span class="line">   结果：</span><br><span class="line">   内连接：</span><br><span class="line">   List((1,(a,aa)), (1,(a,dd)), (1,(b,aa)), (1,(b,dd)), (2,(c,cc)))</span><br><span class="line">   左外连接：</span><br><span class="line">   List((1,(a,Some(aa))), (1,(a,Some(dd))), (1,(b,Some(aa))), (1,(b,Some(dd))), (2,(c,Some(cc))), (4,(d,None)))</span><br><span class="line">   右外连接：</span><br><span class="line">   List((1,(Some(a),aa)), (1,(Some(a),dd)), (1,(Some(b),aa)), (1,(Some(b),dd)), (2,(Some(c),cc)), (3,(None,bb)))</span><br><span class="line">   外连接（全外连接）：</span><br><span class="line">   List((1,(Some(a),Some(aa))), (1,(Some(a),Some(dd))), (1,(Some(b),Some(aa))), (1,(Some(b),Some(dd))), (2,(Some(c),Some(cc))), (3,(None,Some(bb))), (4,(Some(d),None)))</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="1-2-行动算子"><a href="#1-2-行动算子" class="headerlink" title="1.2 行动算子"></a>1.2 行动算子</h3><ol>
<li><code>collect()</code><ul>
<li>以数组的形式返回RDD中所有的元素</li>
</ul>
</li>
<li><code>count()</code><ul>
<li>n返回RDD中元素的个数</li>
</ul>
</li>
<li><code>take(n)</code><ul>
<li>返回RDD中前n个元素组成的数组</li>
</ul>
</li>
<li><code>first</code><ul>
<li>返回RDD中的第一个元素，类似于<code>take(1)</code></li>
</ul>
</li>
<li><code>takeOrder(n, [ordering])</code><ul>
<li>返回排序后的前n个元素，默认是升序排列</li>
</ul>
</li>
<li><code>foreach</code>与<code>foreachPartition</code><ul>
<li>foreach用一般用于与外部存储进行通讯， 这里的foreach与scala中的foreach函数是不同的，Spark中的foreach是在Executor中进行遍历的， 而不是Driver端。</li>
</ul>
</li>
<li><code>countByKey()</code><ul>
<li>统计每个key 的个数，底层是将key转换成(key,1)的形式</li>
</ul>
</li>
<li><code>reduce(func)</code><ul>
<li>通过func函数聚集RDD中的所有元素，先聚集分区内数据，再聚合分区间数据。</li>
</ul>
</li>
<li><code>fold(zero)(func)</code></li>
<li><code>aggregate(zero)(分区内逻辑，分区间逻辑)</code><ul>
<li>行动算子aggregate与转换算子aggregateByKey最大的区别在于，aggregate中的零值参与计算的次数不同，分区内会参与一次，分区间也会参与一次。</li>
</ul>
</li>
<li>各种saveAs…</li>
</ol>
<h2 id="2-RDD的依赖关系"><a href="#2-RDD的依赖关系" class="headerlink" title="2. RDD的依赖关系"></a>2. RDD的依赖关系</h2><ol>
<li>窄依赖</li>
<li>宽依赖</li>
</ol>
<h3 id="1-窄依赖"><a href="#1-窄依赖" class="headerlink" title="1. 窄依赖"></a>1. 窄依赖</h3><p><img src="https://img2.baidu.com/it/u=1912645117,1546810615&fm=15&fmt=auto&gp=0.jpg" alt="img"></p>
<p>父RDD中的一个分区，至多只有一个子RDD的分区使用。</p>
<h3 id="2-宽依赖"><a href="#2-宽依赖" class="headerlink" title="2. 宽依赖"></a>2. 宽依赖</h3><p><img src="https://img1.baidu.com/it/u=3492215315,2225793381&fm=26&fmt=auto&gp=0.jpg" alt="img"></p>
<p>父RDD中的一个分区，被子RDD的多个分区使用，这种依赖关系称为宽依赖。</p>
<p>会引起宽依赖的算子:<code>groupByKey</code>, <code>reduceByKey</code>, <code>join</code>,  <code>sortByKey</code></p>
<h3 id="3-Spark-Job的划分"><a href="#3-Spark-Job的划分" class="headerlink" title="3. Spark Job的划分"></a>3. Spark Job的划分</h3><p>Spark的应用程序都是懒加载的，每调用一个action算子之后，调度器就创建一个执行图和启动一个Spark Job，每个job由多个stage组成。每个stage由多个tasks组成。而task就表示每个并行计算，并且会在多个执行器上执行。</p>
<h4 id="job"><a href="#job" class="headerlink" title="job"></a>job</h4><p>Spark job 处于 Spark 执行层级结构中的最高层. 每个 Spark job 对应一个 action, 每个 action 被 Spark 应用中的驱动所程序调用.</p>
<p>可以把 Action 理解成把数据从 RDD 的数据带到其他存储系统的组件(通常是带到驱动程序所在的位置或者写到稳定的存储系统中)</p>
<p>只要一个 action 被调用, Spark 就不会再向这个 job 增加新的东西.</p>
<h4 id="stages"><a href="#stages" class="headerlink" title="stages"></a>stages</h4><p>从整体来看, 一个 stage 可以任务是“计算(task)”的集合, 这些每个“计算”在各自的 Executor 中进行运算, 而不需要同其他的执行器或者驱动进行网络通讯. 换句话说, 当任何两个 workers 之间开始需要网络通讯的时候, 这时候一个新的 stage 就产生了, 例如: shuffle 的时候.</p>
<p>这些创建 stage 边界的依赖称为 <em>ShuffleDependencies</em>. shuffle 是由宽依赖所引起的, 比如: sort, groupBy, 因为他们需要在分区中重新分发数据. 那些窄依赖的转换会被分到同一个 stage 中.</p>
<h4 id="Tasks"><a href="#Tasks" class="headerlink" title="Tasks"></a>Tasks</h4><p>stage 由 tasks 组成. 在执行层级中, task 是最小的执行单位. 每一个 task 表现为一个本地计算.</p>
<p><strong>一个 stage 中的所有 tasks 会对不同的数据执行相同的代码.(程序代码一样, 只是作用在了不同的数据上)</strong></p>
<p>一个 task 不能被多个执行器来执行, 但是, 每个执行器会动态的分配多个 slots 来执行 tasks, 并且在整个生命周期内会并行的运行多个 task. 每个 stage 的 task 的数量对应着分区的数量, 即每个 Partition 都被分配一个 Task .</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210808112547253.png" alt="image-20210808112547253"></p>
<p>在大多数情况下，每个stage的所有task在下一个stage开启之前必须全部完成。</p>
<h3 id="4-RDD的持久化"><a href="#4-RDD的持久化" class="headerlink" title="4. RDD的持久化"></a>4. RDD的持久化</h3><p>每碰到一个Action就会产生一个job，每个job开始计算的时候，总是从这个job最开始的RDD开始计算。</p>
<p>每个job总是从它血缘的开始计算，难免会有计算过程重复执行的情况。比如（中间过程产生了一些列的RDD，最终有两个action的时候，这两个action依赖同一个RDD，这样每次执行action的时候，就会重复计算）</p>
<p>如果整个RDD依赖的DAG图的血缘关系过长，就很可能出现分区数据损坏或丢失，则又要从头开始计算来达到容错的目的。</p>
<p><font color=red>每个 job 都会重新进行计算, 在有些情况下是没有必要, 如何解决这个问题呢?</font></p>
<p>Spark 一个重要能力就是可以持久化数据集在内存中. 当我们持久化一个 RDD 时, 每个节点都会存储它在内存中计算的那些分区, 然后在其他的 action 中可以重用这些数据. 这个特性会让将来的 action 计算起来更快(通常快 10 倍). 对于迭代算法和快速交互式查询来说, 缓存(<strong>Caching</strong>)是一个关键工具.</p>
<p>可以使用方法<code>persist()</code>或者<code>cache()</code>来持久化一个 RDD. 在第一个 action 会计算这个 RDD, 然后把结果的存储到他的节点的内存中. Spark 的 <code>Cache</code> 也是容错: 如果 RDD 的任何一个分区的数据丢失了, Spark 会自动的重新计算.</p>
<p>RDD 的各个 <code>Partition</code> 是相对独立的, 因此只需要计算丢失的部分即可, 并不需要重算全部 Partition</p>
<p>另外, 允许我们对持久化的 RDD 使用不同的存储级别.</p>
<p>例如: 可以存在磁盘上, 存储在内存中(堆内存中), 跨节点做复本.</p>
<p>可以给<code>persist()</code>来传递存储级别. <code>cache()</code>方法是使用默认存储级别(<code>StorageLevel.MEMORY_ONLY</code>)的简写方法.</p>
<p><strong>RDD的持久化级别</strong></p>
<table>
<thead>
<tr>
<th>Storage Level</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td><code>MEMORY_ONLY</code></td>
<td>Store RDD as  deserialized Java objects in the JVM. If the RDD does not fit in memory, some  partitions will not be cached and will be recomputed on the fly each time  they’re needed. This is the default level.</td>
</tr>
<tr>
<td><code>MEMORY_AND_DISK</code></td>
<td>Store RDD as  deserialized Java objects in the JVM. If the RDD does not fit in memory,  store the partitions that don’t fit on disk, and read them from there when  they’re needed.</td>
</tr>
<tr>
<td><code>MEMORY_ONLY_SER  </code>(Java and Scala)</td>
<td>Store RDD as <em>serialized</em> Java objects (one byte  array per partition). This is generally more space-efficient than  deserialized objects, especially when using a <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.1.1/tuning.html">fast serializer</a>,  but more CPU-intensive to read.</td>
</tr>
<tr>
<td><code>MEMORY_AND_DISK_SER</code>  (Java and Scala)</td>
<td>Similar to  MEMORY_ONLY_SER, but spill partitions that don’t fit in memory to disk  instead of recomputing them on the fly each time they’re needed.</td>
</tr>
<tr>
<td><code>DISK_ONLY</code></td>
<td>Store the RDD  partitions only on disk.</td>
</tr>
<tr>
<td><code>MEMORY_ONLY_2</code>,  <code>MEMORY_AND_DISK_2</code>, etc.</td>
<td>Same as the  levels above, but replicate each partition on two cluster nodes.</td>
</tr>
<tr>
<td><code>OFF_HEAP</code>(experimental)</td>
<td>Similar to  MEMORY_ONLY_SER, but store the data in <a target="_blank" rel="noopener" href="http://spark.apache.org/docs/2.1.1/configuration.html#memory-management">off-heap   memory</a>. This requires off-heap memory to be enabled.</td>
</tr>
</tbody></table>
<p>有一点需要说明的是, 即使我们不手动设置持久化, Spark 也会自动的对一些 shuffle 操作的中间数据做持久化操作(比如: <code>reduceByKey</code>). 这样做的目的是为了当一个节点 <code>shuffle</code> 失败了避免重新计算整个输入. 当时, 在实际使用的时候, 如果想重用数据, 仍然建议调用 <code>persist</code> 或 <code>cache</code></p>
<h4 id="检查点checkpoint"><a href="#检查点checkpoint" class="headerlink" title="检查点checkpoint"></a>检查点checkpoint</h4><p>Spark 中对于数据的保存除了持久化操作之外，还提供了一种检查点的机制,检查点（本质是通过将RDD写入Disk做检查点）是为了通过 Lineage 做容错的辅助</p>
<p><strong>Lineage 过长会造成容错成本过高</strong>，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的 RDD 开始重做 Lineage，就会减少开销。</p>
<p>检查点通过将数据写入到 HDFS 文件系统实现了 RDD 的检查点功能。</p>
<p>为当前 RDD 设置检查点。该函数将会创建一个二进制的文件，并存储到 checkpoint 目录中，该目录是用 SparkContext.setCheckpointDir()设置的。在 checkpoint 的过程中，该RDD 的所有依赖于父 RDD中 的信息将全部被移除。</p>
<p>对 RDD 进行 checkpoint 操作并不会马上被执行，必须执行 Action 操作才能触发, 在触发的时候需要对这个 RDD 重新计算.</p>
<h4 id="持久化与checkpoint的区别"><a href="#持久化与checkpoint的区别" class="headerlink" title="持久化与checkpoint的区别"></a>持久化与checkpoint的区别</h4><ol>
<li>持久化只是将数据保存在BlockManager中，而RDD的Lineage是不变的。但是<code>checkpoint</code>执行完后，RDD已经没有之前所谓的依赖RDD了，而只有一个强行为其设置的<code>checkpoint</code>，RDD的Lineage改变了。</li>
<li>持久化的数据丢失可能性更大，磁盘，内存都有可能存在数据丢失的情况。但是<code>checkpoint</code>的数据通常是存储在如<strong>HDFS等容错、高可用的文件系统，数据丢失的可能性较小。</strong></li>
<li> <strong>注意:</strong> 默认情况下，如果某个 RDD 没有持久化，但是设置了checkpoint，会存在问题. 本来这个 job 都执行结束了，但是由于中间 RDD 没有持久化，checkpoint job 想要将 RDD 的数据写入外部文件系统的话，需要全部重新计算一次，再将计算出来的 RDD 数据 checkpoint到外部文件系统。 所以，<strong>建议对 checkpoint()的 RDD 使用持久化, 这样 RDD 只需要计算一次就可以了</strong>.</li>
</ol>
<h2 id="3-Key-Value类型RDD的数据分区器"><a href="#3-Key-Value类型RDD的数据分区器" class="headerlink" title="3. Key-Value类型RDD的数据分区器"></a>3. Key-Value类型RDD的数据分区器</h2><h3 id="3-1-HashPartitioner"><a href="#3-1-HashPartitioner" class="headerlink" title="3.1 HashPartitioner"></a>3.1 HashPartitioner</h3><p>对于给定的key，计算key的hashCode，并除以分区的个数取余，最后返回值就是这个key所属的分区的ID</p>
<p><strong>HashPartitioner的弊端</strong>： 可能导致每个分区中的数据量不均匀。容易出现数据倾斜。</p>
<h3 id="3-2-RangePartitioner范围分区器"><a href="#3-2-RangePartitioner范围分区器" class="headerlink" title="3.2 RangePartitioner范围分区器"></a>3.2 RangePartitioner范围分区器</h3><p>是将一定范围内的数据映射到某一个分区内，尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大。但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。实现过程为：</p>
<ol>
<li>先从整个RDD中抽取样本数据（每个分区都要进行抽样），将样本数据排序，计算出每个分区的最大key值，形成一个Array[KEY]类型的数组变量rangeBounds（边界数组）。</li>
<li>判断key在rangeBounds中所处的范围，给出该key值在下一个RDD中的分区id的下标；该分区器要求RDD中的KEY类型必须是可以排序的。</li>
</ol>
<p><strong>范围分区器的核心</strong>：</p>
<ul>
<li>蓄水池抽样算法</li>
<li>边界数组</li>
<li>分区号的计算</li>
</ul>
<p><strong>蓄水池抽样算法：</strong></p>
<p>给定一个数据流，数据流长度N很大，且N知道处理完所有数据之前都不可知，请问如何在只遍历一遍数据（O(N)）的情况下，能够随机选取出m个不重复的数据。</p>
<p>核心代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span>[] reservoir = <span class="keyword">new</span> <span class="title class_">int</span>[m];</span><br><span class="line"><span class="comment">//init, 先读取前m个数据</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;m;i++)&#123;</span><br><span class="line">    reservoir[i] = dataStream[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 接下来处理剩余数据</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=m;i&lt;dataStream.length;i++)&#123;</span><br><span class="line">    <span class="comment">// 先获取一个[0,i]内的随机整数</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">d</span> <span class="operator">=</span> rand.nextInt(i+<span class="number">1</span>);</span><br><span class="line">    <span class="comment">//如果随机整数落在了[0, m-1]范围内，则替换蓄水池中的元素</span></span><br><span class="line">    <span class="keyword">if</span>(d&lt;m)&#123;</span><br><span class="line">        reservoir[d] = dataStream[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>边界数组RangeBounds</strong></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>假设一个数组中有4个元素，且有序。例如上图中的10,30,50,70。<br>那么根据这个数组中的4个元素，我们可以划分出5个区间：</p>
<blockquote>
<p>0：小于10<br>        1：10-30<br>        2：30-50<br>        3：50-70<br>        4：大于70</p>
</blockquote>
<p>这样，我们就分出了5个相应的分区。这样的数组我们就叫它为边界数组。<br>一个边界数组中的元素<strong>必须是有序的</strong>，因此在RangePartitioner范围分区内需要对边界数组进行排序。<br>其 <strong>边界数组长度+1 = 最后的分区数</strong></p>
<p><strong>分区号的计算</strong></p>
<ol>
<li>边界数组的长度小于等于128<ul>
<li>这个时候，每来一个key，轮询的方式遍历数组中的每个元素，然后确定分区号</li>
</ul>
</li>
<li>边界数组的长度大于128的时候<ul>
<li>通过<strong>二分查找</strong>来快速定位分区号。</li>
</ul>
</li>
</ol>
<h2 id="4-共享变量"><a href="#4-共享变量" class="headerlink" title="4. 共享变量"></a>4. 共享变量</h2><p>正常情况下，传递给spark算子的（比如map，reduce等）的函数都是在远程的集群节点上执行，函数中用到的所有变量都是独立的拷贝。这些变量被拷贝到集群上的每个节点上 ，这些变量的更改不会传递回驱动程序。</p>
<p>但是Spark提供了两个可以跨task的共享变量：</p>
<ul>
<li>累加器</li>
<li>广播变量</li>
</ul>
<h4 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h4><p>累加器用来对信息进行聚合，通常在向 Spark 传递函数时，比如使用 map() 函数或者用 filter() 传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，所以更新这些副本的值不会影响驱动器中的对应变量。</p>
<p>如果我们想实现所有分片处理时更新共享变量的功能，那么累加器可以实现我们想要的效果。</p>
<p>累加器是一种变量, 仅仅支持“add”, 支持并发. 累加器用于去实现计数器或者求和. Spark 内部已经支持数字类型的累加器, 开发者可以添加其他类型的支持.</p>
<ol>
<li>内置累加器<code>sc.LongAccumulator</code></li>
<li>自定义累加器<code>继承AccumulatorV2</code></li>
</ol>
<h4 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h4><p>广播变量是在每个节点上保存一个只读的变量缓存，而不用给每个task来传一个copy。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> broadcastVar = sc.broadcast(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">broadcastVar: org.apache.spark.broadcast.<span class="type">Broadcast</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Broadcast</span>(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; broadcastVar.value</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<ol>
<li><p>通过对一个类型T的对象调用SparkContext.broadcast创建出一个Broadcast[T]对象。任何可序列化的类型都可以这么实现。</p>
</li>
<li><p>通过value属性访问该对象的值(在Java中为value()方法)。</p>
</li>
<li><p>变量只会被发到各个节点一次，应作为只读值处理(修改这个值不会影响到别的节点)</p>
</li>
</ol>
<h2 id="2-SparkSQL"><a href="#2-SparkSQL" class="headerlink" title="2. SparkSQL"></a>2. SparkSQL</h2><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16284015946612" alt="在这里插入图片描述"></p>
<h3 id="RDD，-DataFrame和DataSet之间的关系"><a href="#RDD，-DataFrame和DataSet之间的关系" class="headerlink" title="RDD， DataFrame和DataSet之间的关系"></a>RDD， DataFrame和DataSet之间的关系</h3><p>在SparkSQL中，Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet。</p>
<p>RDD (Spark1.0) —&gt; Dataframe(Spark1.3) —&gt; Dataset(Spark1.6)</p>
<p>同样的数据得到这三种不同的数据结构，经过计算得到的结果相同，但是执行效率和执行方式不同。</p>
<h4 id="RDD，DataFrame和DataSet三者的共性"><a href="#RDD，DataFrame和DataSet三者的共性" class="headerlink" title="RDD，DataFrame和DataSet三者的共性"></a>RDD，DataFrame和DataSet三者的共性</h4><ol>
<li>RDD、DataFrame、DataSet全都是Spark平台下的分布式弹性数据集，为处理超大型数据提供便利。</li>
<li>三者都是惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算。</li>
<li>三者都会根据Spark的内存情况自动缓存运算，这样即使数据量跟大，也不会担心内存溢出。</li>
<li>三者都有partition的概念</li>
<li>三者有许多共同的函数，如map，filter，排序等。</li>
<li>在对Dataframe和DataSet进行操作许多操作都需要**<code>import spark.implicits._</code>**这个包的支持</li>
<li>DataFrame和DataSet均可使用模式匹配获取各个字段的值和类型。</li>
</ol>
<h4 id="三者的区别"><a href="#三者的区别" class="headerlink" title="三者的区别"></a>三者的区别</h4><p><strong>RDD</strong></p>
<ol>
<li>RDD一般和spark mlib同时使用</li>
<li>RDD不支持sparkSql操作</li>
</ol>
<p><strong>DataFrame</strong></p>
<ol>
<li>与RDD和DataSet不同，DataFrame每一行的类型固定为Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值。</li>
<li>DataFrame与DataSet一般不与spark mlib同时使用</li>
<li>DataFrame与DataSet均支持SparkSql的操作，比如select， groupby之类，还能注册临时表/视图，进行sql语句操作</li>
<li>DataFrame与DataSet支持一些特别方便的保存方式，比如保存csv，可以带上表头，这样每一列的字段名一目了然</li>
</ol>
<p><strong>DataSet</strong></p>
<p>DataSet和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同。DataFrame其实是DataSet的一个特例。<br>DataFrame也可以叫DataSet[Row]，每一行的类型都是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面的getAs方法或者模式匹配拿住特定的字段。而DataSet中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息。</p>
<h3 id="Spark-on-Hive-与-Hive-on-Spark"><a href="#Spark-on-Hive-与-Hive-on-Spark" class="headerlink" title="Spark on Hive 与 Hive on Spark"></a><strong>Spark on Hive 与 Hive on Spark</strong></h3><ul>
<li>Spark on Hive通过Spark-SQL使用hive语句，操作hive，底层运行的还是spark rdd</li>
<li>Hive on Spark是把hive查询从mapreduce 的mr (Hadoop计算引擎)操作替换为spark rdd（spark 执行引擎） 操作。</li>
</ul>
<h3 id="Spark-SQL-Text-转化为实际的物理任务的流程"><a href="#Spark-SQL-Text-转化为实际的物理任务的流程" class="headerlink" title="Spark SQL Text 转化为实际的物理任务的流程"></a>Spark SQL Text 转化为实际的物理任务的流程</h3><p><img src="https://img-blog.csdnimg.cn/20200518170149838.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E1Njg1MjYz,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<ol>
<li>SQL语句经过<strong>SqlParse（解析器）</strong>解析成<strong>Unresolved LogicalPlan</strong></li>
<li><strong>分析器Analyzer</strong>结合<strong>数据字典Catalog</strong>进行绑定，生成<strong>Resolved LogicalPlan</strong></li>
<li><strong>优化器Optimizer</strong>对<strong>Resolved LogicalPlan</strong>进行优化，生成优化后的<strong>Optimizer LogicalPlan</strong></li>
<li><strong>SparkPlan</strong>将上述的<strong>Optimizer LogicalPlan</strong>转换成<strong>PhysicPlan</strong></li>
<li>使用<strong>prepareForExecption</strong>将<strong>PhysicPlan</strong>转换成可执行的物理计划</li>
<li>使用<strong>execute()<strong>执行可执行的物理计划，生成</strong>DataFrame</strong></li>
</ol>
<h3 id="Spark-SQL-源码分析（version：Spark2-amp-amp-Spark3-1-2）"><a href="#Spark-SQL-源码分析（version：Spark2-amp-amp-Spark3-1-2）" class="headerlink" title="Spark SQL 源码分析（version：Spark2 &amp;&amp; Spark3.1.2）"></a>Spark SQL 源码分析（version：Spark2 &amp;&amp; Spark3.1.2）</h3><blockquote>
<p>补充时间：2022年4月30日</p>
<p>备注： 详细了解一下Spark SQL的部分关键源码，明白一条 SQL 语句是如何一步步转换成最终的RDD任务</p>
</blockquote>
<h4 id="前置知识：Antlr4-语法生成器工具"><a href="#前置知识：Antlr4-语法生成器工具" class="headerlink" title="前置知识：Antlr4 语法生成器工具"></a>前置知识：Antlr4 语法生成器工具</h4><p>Spark SQL内部是通过第三方插件Antlr进行SQL语句的语法分析的。在了解Spark SQL源码之前，首先要了解Antlr的相关知识。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/antlr/antlr4/blob/master/doc/index.md">Antlr4 GitHub doc地址</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/apache/spark/tree/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser">Spark SQL中使用的SQL语法文件：sqlBase.g4文件</a></p>
<h4 id="Spark2源码分析"><a href="#Spark2源码分析" class="headerlink" title="Spark2源码分析"></a>Spark2源码分析</h4><p>Spark SQL模块在处理SQL时，内部是通过一个名为<code>Catalyst</code>的优化器（我更喜欢称之为：处理器）将SQL转换成最终的逻辑执行计划。</p>
<h2 id="3-Spark-内核"><a href="#3-Spark-内核" class="headerlink" title="3. Spark 内核"></a>3. Spark 内核</h2><h3 id="Spark提交任务流程"><a href="#Spark提交任务流程" class="headerlink" title="Spark提交任务流程"></a>Spark提交任务流程</h3><h4 id="1-Yarn-Cluster-模式"><a href="#1-Yarn-Cluster-模式" class="headerlink" title="1. Yarn Cluster 模式"></a>1. Yarn Cluster 模式</h4><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210808163959670-16284120015104.png" alt="image-20210808163959670"></p>
<ol>
<li>执行<code>spark-submit</code>脚本提交任务，实际是启动一个SparkSubmit的JVM进程；</li>
<li>SparkSubmit类中的main方法反射调用Client的main方法；</li>
<li>Client创建Yarn客户端，然后想Yarn发送执行指令<code>bin/java ApplicationMaster</code> 请求启动一个AM；</li>
<li>Yarn框架收到指令后，RM会选择一台可用的NM，并在其中启动ApplicationMaster（进程）；</li>
<li><code>ApplicationMaster进程运行(此时RM还不知道AM启动与否，所以AM需要向RM注册)</code>，同时会启动一个<code>Driver子线程</code>，用于执行用户的作业（执行代码，初始化sc，任务切分）；</li>
<li>AM向RM进行注册（证明AM已经启动了），AM还要向RM申请资源<code>Container</code></li>
<li>获取到资源后，AM会向资源所在的NM（获取到的资源，可能在不同的机器上）发送指令<code>bin/java CoarseGrainedExecutorBackend</code>,启动一个粗粒度的ExecutorBackend</li>
<li>相应的NodeManager上会启动相应的<code>ExecutorBackend进程</code>, 并向<code>Driver</code>进行反向注册</li>
<li><code>Driver</code>上注册成功后，会向相应的NM返回注册成功信息，然后<code>ExecutorBackend</code>进程会创建一个<code>Executor</code>对象。</li>
<li>Driver内部指定用户提交作业的main方法，初始化sc，并进行任务的切分，然后分配给ExecutorBackend任务，并监控任务的执行。</li>
</ol>
<h4 id="Yarn-Client模式"><a href="#Yarn-Client模式" class="headerlink" title="Yarn Client模式"></a>Yarn Client模式</h4><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210808171159136-16284139206945.png" alt="image-20210808171159136"></p>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><ol>
<li>执行spark-submit脚本提交任务，实际启动一个SparkSubmit的JVM进程</li>
<li>SparkSubmit伴生对象中的main方法反射调用用户代码（就是我们自己所编写的代码）的main方法。</li>
<li>启动Driver（在SparkSubmit进程的main线程中运行，此时的Driver就不是一个子线程了），执行用户的作业，并创建<code>ScheduleBackend</code>与Yarn进行通信。</li>
<li><code>YarnClientScheduleBackend</code>向RM发送指令<code>bin/java ExecutorLauncher</code>(底层本质就是<code>ApplicationMaster</code>)启动ExecutorLauncher进程</li>
<li>RM收到指令后会在指定的NM中启动ExecutorLauncher，实质上还是调用的ApplicationMaster的main方法。</li>
<li>ExecutorLauncher向AM注册，申请资源</li>
<li>获取资源后ExecutorLauncher向相应的NM发送指令<code>bin/java CoarseGrainedExecutorBackend</code>启动一个粗粒度的ExecutorBackend；</li>
<li>后面和cluster模式一致</li>
</ol>
<p><strong>注意：</strong></p>
<p>driver不是一个子线程了，而是直接运行在SparkSubmit进程的main线程中，所以sparkSubmit进程不能退出。</p>
<p>而Cluster模式下，Driver是运行在远程集群上的，SparkSubmit进程提交完作业后即可以关闭。</p>
<h2 id="4-Spark任务调度机制"><a href="#4-Spark任务调度机制" class="headerlink" title="4. Spark任务调度机制"></a>4. Spark任务调度机制</h2><h3 id="4-1-Spark-任务调度概述"><a href="#4-1-Spark-任务调度概述" class="headerlink" title="4.1 Spark 任务调度概述"></a>4.1 Spark 任务调度概述</h3><p>在介绍任务调度之前，先来明确一下Spark中几个重要的概念。</p>
<ul>
<li><strong>Job</strong><ul>
<li>Job是以Action算子为界限，遇到一个Action算子则触发一个Job</li>
</ul>
</li>
<li><strong>Stage</strong><ul>
<li>Stage是Job的子集，以RDD宽依赖（即Shuffle）为界限，遇到Shuffle做一次划分</li>
</ul>
</li>
<li><strong>Task</strong><ul>
<li>Task是Stage的子集，以并行度（分区数）来衡量，这个Stage的分区数有多少，则这个Stage就有多少个Task，每个Stage中的多个Task运行同样的逻辑，但是作用在不同的数据上。</li>
</ul>
</li>
</ul>
<p>Spark的任务调度总体来说分为两路进行：一路是Stage级别的调度；一路是Task级别的调度。</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210809095832482-8474331.png" alt="image-20210809095832482"></p>
<p>Spark RDD通过其Transactions操作，形成了RDD血缘关系图DAG，最后通过Action的调用，发出Job并调执行。</p>
<p><code>DAGScheduler</code>负责Stage级别的调度，主要是将Job切分成若干<code>Stages</code>，并将每个Stage打包成<code>TaskSet</code>交给<code>TaskScheduler</code>调度。</p>
<p><code>TaskScheduler</code>负责Task级别的调度，将DAGScheduler传过来的<code>TaskSet</code>按照指定的调度策略分发到<code>Executor</code>上执行，调度过程中<code>SchedulerBackend</code>负责提供可用资源，其中<code>SchedulerBackend</code>有多种实现，分别对接不同的资源管理系统。</p>
<p><code>Driver</code>初始化<code>SparkContext</code>过程中，会分别初始化<code>DAGScheduler</code>、<code>TaskScheduler</code>、<code>SchedulerBackend</code>以及<code>HeartbeatReceiver</code>，并启动<code>SchedulerBackend</code>以及<code>HeartbeatReceiver</code>。</p>
<p><code>SchedulerBackend</code>通过<code>ApplicationMaster</code>申请资源，并不断从<code>TaskScheduler</code>中拿到合适的Task分发到<code>Executor</code>执行。</p>
<p><code>HeartBeatReceiver</code>负责接收<code>Executor</code>的心跳信息，监控<code>Executor</code>的存活状态，并通知到<code>TaskScheduler</code>。</p>
<h3 id="4-2-Spark-Stage级别的调度"><a href="#4-2-Spark-Stage级别的调度" class="headerlink" title="4.2 Spark Stage级别的调度"></a>4.2 Spark Stage级别的调度</h3><p>Spark的任务调度是从DAG切割开始，主要是由<code>DAGScheduler</code>来完成。当遇到一个Action操作后就会触发一个Job的计算，并交给<code>DAGScheduler</code>来提交。</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210809102313917-8475795.png" alt="image-20210809102313917"></p>
<ol>
<li><p>Job由最终的RDD和Action方法封装而成；</p>
</li>
<li><p><code>SparkContext</code>将Job交给<code>DAGScheduler</code>提交，它会根据RDD的血缘关系构成的DAG进行切分，将一个Job划分为若干Stages；</p>
<ul>
<li>具体划分策略是：由最终的RDD不断通过依赖回溯判断父依赖是否是<strong>宽依赖</strong>，即以<code>Shuffle</code>为界，划分Stage，窄依赖的RDD之间被划分到同一个Stage中，可以进行pipeline式的计算。</li>
<li><strong>划分的Stages分两类</strong>：<ul>
<li>一类叫做<code>ResultStage</code>，为DAG最下游的Stage，由Action方法决定；</li>
<li>一类叫做<code>ShuffleMapStage</code>，为下游的Stage准备数据。</li>
</ul>
</li>
</ul>
</li>
<li><p>Submit stage 提交阶段</p>
<ul>
<li><code>DAGScheduler.handleJobSubmitted</code>方法创建好<code>ResultStage</code>后会提交这个stage（submitStage方法），在提交一个<code>stage</code>的时候，会要先提交它的<code>parent stage</code>，也是通过递归的形式，直到一个<code>stage</code>的所有父阶段-<code>parent stage</code>都被提交了，才会提交本阶段，如果一个stage的parent还没有完成，则会把这个stage加入到<code>waitingStages</code>。也就是说，DAG图中前面的stage会被先提交。当一个stage的parent都准备好了，也就是执行完毕之后，它才会进入<code>submitMissingTasks</code>的环节。</li>
<li>stage的划分是<font color=green>从后向前</font>进行划分的，而真正的任务执行是<font color=green>从前向后</font>执行的。</li>
</ul>
</li>
<li><p>Subimit task</p>
<ul>
<li><p><code>Stage</code>中的<code>Task</code>是在<code>DAGScheduler</code>（不是TaskScheduler）的<code>submitMissingTasks</code>方法中创建的，包括<code>ShuffleMapTask</code>和<code>ResultTask</code>，与<code>Stage</code>对应。归属于同一个stage的这批Task组成一个<code>TaskSet集合</code>,最后提交给<code>TaskScheduler</code>的就是这个<code>TaskSet</code>。</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp" alt="img"></p>
</li>
</ul>
</li>
</ol>
<p>Stage级调度的整体流程：</p>
<p><img src="./source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/DAGScheduler.png" alt="DAGScheduler.png"></p>
<p><font color=red>注意:</font></p>
<p>task的创建是在DAGScheduler的submitMissingTasks方法中创建的，而<code>TaskScheduler</code>负责Task的调度，不负责Task 的创建。</p>
<h3 id="4-3-Spark-Task级别的调度"><a href="#4-3-Spark-Task级别的调度" class="headerlink" title="4.3 Spark Task级别的调度"></a>4.3 Spark Task级别的调度</h3><p>Task的调度是由<code>TaskScheduler</code>与<code>SchedulerBackend</code>紧密合作，共同完成的。</p>
<p><code>TaskScheduler</code>是task级别的调度器，主要作用是管理task的调度和提交，是Spark底层的调度器。</p>
<p><code>SchedulerBackend</code>是<code>TaskScheduler</code>的后端服务，有独立的线程，所有的<code>Executor</code>都会注册到<code>SchedulerBackend</code>，主要作用是进行资源的分配、将<code>Task</code>分配给<code>Executor</code>等。</p>
<p><img src="./source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/TaskScheduler.png" alt="TaskScheduler.png"></p>
<p><strong>TaskSetManager</strong>负责监控管理同一个stage中的Tasks，TaskScheduler就是以TaskSetManager为单元来调度任务。包括<strong>任务推断</strong>、<strong>Task本地化调度</strong>，并对<strong>Task进行资源分配</strong>。</p>
<p><strong>具体工作流程</strong></p>
<p><img src="./source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/TaskScheduler%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png" alt="TaskScheduler.png"></p>
<ol>
<li><p><code>DAGScheduler</code>(<code>submitMissingTasks</code>方法中)调用<code>TaskScheduler.submitTask()</code>创建并TaskSet给<code>TaskScheduler</code>；</p>
</li>
<li><p><code>TaskScheduler</code>拿到<code>TaskSet</code>后会创建一个<code>TaskSetManager</code>来管理它，并且把<code>TaskSetManager</code>添加到<strong>rootPool</strong>调度池中；</p>
</li>
<li><p>调用<code>SchedulerBackend.revivieOffers()</code>方法</p>
</li>
<li><p><code>SchedulerBackend</code>发送<code>ReviveOffers</code>消息给<code>DriverEndPoint</code>；</p>
</li>
<li><p><code>DriverEndPoint</code>接收到<code>ReviveOffers</code>消息后，会调用<code>makeOffers()方法</code>创建<code>WorkerOffer</code>，并通过<code>TaskScheduler.resourceOffer()</code>返回Offer；</p>
</li>
<li><p><strong>TaskScheduler</strong>会从<strong>rootPool</strong>资源调度池中按照特定的调度算法取出一个<strong>TaskSetManager</strong>，逐个给<strong>TaskSet</strong>的<strong>Task</strong>分配<strong>WorkerOffer</strong>，并将其封装成<strong>TaskDescription</strong>（包含Offer信息）</p>
</li>
<li><p>调用<code>SchedulerBackend.DriverEndPoint</code>的<strong>launchTasks</strong>方法，将<strong>TaskDescription</strong>序列化并封装在<strong>LaunchTask</strong>消息中，发送给<strong>Offer</strong>指定的<strong>Executor</strong>。</p>
<p><strong>LaunchTask</strong>消息被<strong>ExecutorBackend</strong>接收到后，会将<strong>Task</strong>信息反序列化，传给<code>Executor.launTask()</code>,最后使用<strong>Executor的线程池</strong>中的线程来执行这个<strong>Task</strong>。</p>
</li>
</ol>
<h3 id="4-4-调度策略"><a href="#4-4-调度策略" class="headerlink" title="4.4 调度策略"></a>4.4 调度策略</h3><p>TaskScheduler支持两种调度策略：<strong>FIFO</strong>和<strong>FAIR</strong></p>
<ul>
<li><p><strong>FIFO</strong>:先进入到rooPool资源池中的TaskSetManager优先被调度</p>
</li>
<li><p><strong>FAIR</strong>：公平调度</p>
</li>
</ul>
<h3 id="4-5-本地化调度"><a href="#4-5-本地化调度" class="headerlink" title="4.5 本地化调度"></a>4.5 本地化调度</h3><p>因为每个Stage中的task负责处理不同分区的数据，所以在task被分配到Executor上时，尽量保证Task与处理的数据保持较近的距离，这样可以避免一定的数据传输开销。</p>
<p><strong>TaskScheduler</strong>从调度队列中拿到<strong>TaskSetManager</strong>后，那么接下来的工作就是<strong>TaskSetManager</strong>按照一定的规则一个个取出task给<strong>TaskScheduler</strong>，<strong>TaskScheduler</strong>再交给<strong>SchedulerBackend</strong>去发到<strong>Executor</strong>上执行。</p>
<p><strong>TaskSetManager</strong>会根据每个Task的优先位置，确定<code>Task的本地化调度级别：Locality</code>， Locality一共有五种，优先级由高到低顺序：</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>名称</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>PROCESS_LOCAL</td>
<td>进程本地化</td>
<td>task和数据在同一个Executor中，性能最好。</td>
</tr>
<tr>
<td>NODE_LOCAL</td>
<td>节点本地化</td>
<td>task和数据在同一个节点中，但是task和数据不在同一个Executor中，数据需要在进程间进行传输</td>
</tr>
<tr>
<td>RACK_LOCAL</td>
<td>机架本地化</td>
<td>task和数据在同一个机架的两个不同的节点上，数据需要通过网络在节点之间进行传输。</td>
</tr>
<tr>
<td>NO_PREF</td>
<td></td>
<td>对于task来说，从哪里获取数据都一样，没有好坏之分</td>
</tr>
<tr>
<td>ANY</td>
<td></td>
<td>task和数据可以在集群的任何地方，而且不在一个机架中，性能最差</td>
</tr>
</tbody></table>
<p>在调度执行时，Spark调度总是会尽量让每个task以最高的本地级别来启动，当一个task以本地性级别启动，但是该本地性级别对应的所有节点都没有空闲资源而启动失败，此时并不会立马降低本地性级别启动，而是在某个时间长度内再次以本地性级别来启动该task，若超过限时时间则降级启动，去尝试下一个本地性级别。</p>
<p>可以通过调大每个类别的最大容忍延迟时间，在等待阶段对应的Executor可能就会有相应的资源去执行次task，这就在一定程度上提升了运行性能。</p>
<p><strong>失败重试和黑名单</strong></p>
<p>除了选择合适的Task调度运行外，还需要监控Task的执行状态，前面也提到，与外部打交道的是SchedulerBackend，Task被提交到Executor启动执行后，Executor会将执行状态上报给SchedulerBackend，SchedulerBackend则告诉TaskScheduler，TaskScheduler找到该Task对应的TaskSetManager，并通知到该TaskSetManager，这样TaskSetManager就知道Task的失败与成功状态，对于失败的Task，会记录它失败的次数，如果失败次数还没有超过最大重试次数，那么就把它放回待调度的Task池子中，否则整个Application失败。</p>
<p>在记录Task失败次数过程中，会记录它上一次失败所在的Executor Id和Host，这样下次再调度这个Task时，会使用黑名单机制，避免它被调度到上一次失败的节点上，起到一定的容错作用。黑名单记录Task上一次失败所在的Executor Id和Host，以及其对应的“拉黑”时间，</p>
<p>“拉黑”时间是指这段时间内不要再往这个节点上调度这个Task了。</p>
<h2 id="5-Spark-Shuffle解析"><a href="#5-Spark-Shuffle解析" class="headerlink" title="5. Spark Shuffle解析"></a>5. Spark Shuffle解析</h2><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/format,png.png" alt="ShuffleMapStage与ResultStage"></p>
<p>Spark中会根据俄宽依赖来划分Stage。</p>
<p>在划分Stage时，最后一个Stage称为<code>ResultStage</code>, 前面所有的Stage被称为<code>ShuffleMapStage</code></p>
<p><strong>ShuffleMapStage</strong>的结束伴随着shuffle文件的写磁盘（<strong>ShuffleWrite</strong>）</p>
<p><strong>ResultStage</strong>的开始会先进行shuffle文件的读磁盘（<strong>ShuffleRead</strong>）</p>
<p>（在MapReduce计算框架中，只要发生Shuffle就会伴随着数据落盘，而在Spark中，只有ShuffleMapStage结束时才会伴随着数据落盘）</p>
<h3 id="5-1-HashShuffle"><a href="#5-1-HashShuffle" class="headerlink" title="5.1 HashShuffle"></a>5.1 HashShuffle</h3><p>在spark-1.6之前默认的shuffle方式是hash，在spark-1.6版本之后使用sort-BaseShuflle，因为HashShuffle存在不足所以就替换了HashShuffle。Spark2.0之后，从源码中完全移除了HashShuffle。</p>
<h4 id="5-1-1-未优化的HashShuffle"><a href="#5-1-1-未优化的HashShuffle" class="headerlink" title="5.1.1 未优化的HashShuffle"></a>5.1.1 未优化的HashShuffle</h4><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%9C%AA%E4%BC%98%E5%8C%96%E7%9A%84HashShuffle.jpeg" alt="img" style="zoom:80%;" />

<p>如上图所示：假设共有三个ReduceTask在等待MapTask落盘的数据文件。</p>
<p>每个MapTask处理完后的数据，会根据key的hash被分到不同的缓冲区中，进行将数据落盘。</p>
<p>如果采用未优化的HashShuffle，那么每个MapTask都会产生相应ReduceTask数据量的小文件。</p>
<p><strong>缺点</strong></p>
<ol>
<li>map任务的中间结果首先存入内存（缓存），然后才写入磁盘，这对于内存的开销很大，当一个节点上的Map任务的输出结果集很大时，很容易导致内存紧张，发生OOM；</li>
<li>生成很多的小文件。假设有M个MapTask，有N个ReduceTask，则会创建M*N个小文件，磁盘I/O将成为性能的瓶颈。</li>
</ol>
<h4 id="5-1-2-优化后的HashShuffle"><a href="#5-1-2-优化后的HashShuffle" class="headerlink" title="5.1.2 优化后的HashShuffle"></a>5.1.2 优化后的HashShuffle</h4><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/优化的HashShuffle.png" alt="img" style="zoom:60%;" />



<p>优化的HashShuffle过程就是启用合并机制，合并机制就是复用Buffer。在同一个进程中（一个core中），无论是有多少个task，都只会有相应ReduceTask数量的一份数据（例如上述就是3个）</p>
<p>每一个MapTask所在的进程中，分别写入共同进程中的3分本地文件，上述图中有4个Task，但是只有两个Core，所以总共输出是 <code>2个Core * 3个分类文件 = 6 个本地小文件</code></p>
<h3 id="5-2-SortShuffle"><a href="#5-2-SortShuffle" class="headerlink" title="5.2 SortShuffle"></a>5.2 SortShuffle</h3><h4 id="5-2-1-普通的SortShuffle"><a href="#5-2-1-普通的SortShuffle" class="headerlink" title="5.2.1 普通的SortShuffle"></a>5.2.1 普通的SortShuffle</h4><p>每个进程中只有一块缓冲区</p>
<img src="./source/2021年8月份秋招复习笔记/普通SortShuffle.png" alt="img" style="zoom:80%;" />







<p>在该模式下，数据会先写入一个数据结构，reduceByKey写入Map，一边通过Map局部聚合，一边写入内存。</p>
<p>如果内存中的数据达到<strong>阈值</strong>，就会将内存中的数据结构写入到磁盘，清空内存数据结构。</p>
<p>在<strong>溢写磁盘</strong>前，现根据<strong>key进行排序</strong>，排序过后的数据，会分批写入到磁盘文件中。默认批次为10000条，数据会以每批一万条写入到磁盘文件。写入磁盘文件通过缓冲区溢写的方式，每次溢写都会产生一个磁盘文件，也就是说一个Task过程会产生多个临时文件。</p>
<p>最后在每个Task中，将<strong>所有的临时文件合并</strong>，这就是<strong>merge</strong>过程，此过程将所有临时文件读取出来，一次写入到最终文件。意味着一个<strong>Task</strong>的所有数据都在这一个文件中。同时单独写一份索引文件，标识下游各个Task的数据在文件中的索引，Start offset和End offset。</p>
<h4 id="5-2-2-bypassSortShuffle"><a href="#5-2-2-bypassSortShuffle" class="headerlink" title="5.2.2 bypassSortShuffle"></a>5.2.2 bypassSortShuffle</h4><p>每个进程中有多个缓冲区</p>
<img src="./source/2021年8月份秋招复习笔记/byPassSortShuffle.png" alt="img" style="zoom:80%;" />

<p>bypass运行机制的出发条件（<font color=red>必须同时满足</font>）：</p>
<ol>
<li>shuffle map task数量小于<code>spark.shuffle.sort.bypassMergeThreshold=200</code>参数的值，默认为200；</li>
<li>不是聚合类的<code>shuffle</code>算子（<font color=red>没有预聚合功能的</font>，比如：groupByKey）</li>
</ol>
<p>该过程的磁盘写机制其实跟未经优化的<code>HashShuffleManager</code>是一模一样的，因为都要创建数据量惊人的磁盘文件，只是在最后一次溢写，进行了一次磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，Shuffle read的性能会更好。（<strong>同时也是采用缓冲区溢写的方法落盘小文件，解决了HashShuffle中内存的问题</strong>）</p>
<p>而该机制与普通的SortShuffleManager运行机制的不同在于：不会进行排序。也就是说，启动该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开</p>
<h2 id="6-Spark内存管理"><a href="#6-Spark内存管理" class="headerlink" title="6. Spark内存管理"></a>6. Spark内存管理</h2><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/src=http%253A%252F%252Fwx1.sinaimg.cn%252Fmw690%252F63918611gy1fe7btgzmz8j20le0fidhc.jpg&refer=http%253A%252F%252Fwx1.sinaimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg" alt="img"></p>
<p>Spark将内存从逻辑上区分为堆内内存和堆外内存，称为内存模型（MemoryMode）</p>
<ul>
<li>这里的堆内内存不能与JVM中的Java堆直接画等号，它只是JVM堆内存的一部分，由JVM统一管理。</li>
<li>堆外内存则是Spark使用<code>sun.misc.Unsafe</code>的API直接在工作节点（Executor）的系统内存中开辟的空间。</li>
</ul>
<p>内存池：对上述两种内存进行资源管理</p>
<p><strong>堆外内存</strong>：为了进一步优化内存的使用以及提高Shuffle时排序的效率，Spark引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，存储经过序列化的二进制数据。</p>
<p>堆外内存意味着把 内存对象分配在Java虚拟机以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。</p>
<p>利用 JDK Unsafe API，Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。</p>
<p>堆外内存可以被精确地申请和释放（堆外内存之所以能够被精确的申请和释放，是由于内存的申请和释放不再通过JVM机制，而是直接向操作系统申请，JVM对于内存的清理是无法准确指定时间点的，因此无法实现精确的释放），而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。</p>
<h3 id="内存空间分配"><a href="#内存空间分配" class="headerlink" title="内存空间分配"></a>内存空间分配</h3><h4 id="1-静态内存管理"><a href="#1-静态内存管理" class="headerlink" title="1.  静态内存管理"></a>1.  静态内存管理</h4><p>在 Spark1.6之前采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的，但用户可以在应用程序启动前进行配置.</p>
<h5 id="1-1-堆内内存管理"><a href="#1-1-堆内内存管理" class="headerlink" title="1.1 堆内内存管理"></a>1.1 堆内内存管理</h5><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/src=http%253A%252F%252Fupload-images.jianshu.io%252Fupload_images%252F9175374-a2a527f62646d62b.png&refer=http%253A%252F%252Fupload-images.jianshu.io&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg" alt="img"></p>
<ul>
<li><p><strong>Storage内存</strong>（存储内存）：主要用于存储Spark的cache数据，例如RDD的缓存、Broadcast变量，Unroll数据等。（**默认占用系统内存的60%**）</p>
</li>
<li><p><strong>Execution内存</strong>（执行内存）：主要用于存放Shuffle、Join、Sort、Aggregation等计算过程中的临时数据。（**默认占用系统内存的20%**）</p>
</li>
<li><p><strong>Other</strong>（有时也叫做用户内存）：主要用于存储RDD转换操作所需的数据，例如RDD依赖等信息。（**默认占用系统内存的20%**）</p>
</li>
<li><p><strong>预留内存</strong>（Reserved Memory）：系统预留内存，会用来存储Spark内部对象，防止OO</p>
</li>
</ul>
<h5 id="1-2-堆外内存管理"><a href="#1-2-堆外内存管理" class="headerlink" title="1.2 堆外内存管理"></a>1.2 堆外内存管理</h5><p>堆外的空间分配较为简单，只有存储内存和执行内存。</p>
<p>可用的执行内存和存储内存占用的空间大小直接由参数 spark.memory.storageFraction 决定，由于堆外内存占用的空间可以被精确计算，所以无需再设定保险区域。</p>
<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210809202402983-8511844.png" alt="image-20210809202402983" style="zoom:50%;" />

<p>静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成“一半海水，一半火焰”的局面，即存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。</p>
<p>由于新的内存管理机制的出现，这种方式目前已经很少有开发者使用，出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。</p>
<h4 id="2-统一内存管理"><a href="#2-统一内存管理" class="headerlink" title="2. 统一内存管理"></a>2. 统一内存管理</h4><p>Spark 1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域.</p>
<h5 id="2-1-统一堆内内存管理"><a href="#2-1-统一堆内内存管理" class="headerlink" title="2.1 统一堆内内存管理"></a>2.1 统一堆内内存管理</h5><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86.jpeg"></p>
<p><strong>存储内存</strong>与<strong>执行内存</strong>共占系统内存的<strong>60%<strong>，Other（用户内存占</strong>40%<strong>），预留内存：</strong>300M</strong></p>
<p>统一内存管理最重要的优化在于<strong>动态占用机制</strong>，其规则如此下：</p>
<ol>
<li>设定基本的存储内存和执行内存区域<code>spark.storage.storageFraction</code>,该设定确定了双方各自拥有的空间的范围。</li>
<li>双方空间都不足时，则存储到硬盘。若己方内存空间不足而对方空余时，可借用对方的空间。</li>
<li>执行内存的空间被存储内存占用后，可让存储内存将占用的部分数据转存到硬盘，然后“归还”借用的空间。</li>
<li>存储内存的空间被执行内存占用后，无法让运行内存“归还”，因为需要考虑Shuffle过程中的诸多因素，实现起来比较复杂。（<font color=red>执行优先，不能执行一半就把内存还回去了，得等此次执行完后才可返还内存</font>）</li>
</ol>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/1228818-20180426212853794-858627420.png" alt="img"></p>
<h5 id="2-2-统一堆外内存管理"><a href="#2-2-统一堆外内存管理" class="headerlink" title="2.2 统一堆外内存管理"></a>2.2 统一堆外内存管理</h5><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210809202631142-8511994.png" alt="image-20210809202631142" style="zoom:50%;" />

<h3 id="存储内存管理"><a href="#存储内存管理" class="headerlink" title="存储内存管理"></a>存储内存管理</h3><p>RDD 作为 Spark 最基本的数据抽象, 是分区记录(partition)的只读集合, 只能基于在稳定物理存储中的数据集上创建, 或者在其他已有的 RDD 上执行转换(Transformation)操作产生一个新的 RDD.</p>
<p>转换后的 RDD 与原始的 RDD 之间产生的依赖关系, 构成了血统(Lineage). 凭借血统, Spark 可以保证每一个 RDD 都可以被重新恢复.</p>
<p>但 RDD 的所有转换都是惰性的, 即只有当行动(Action)发生时, Spark 才会创建任务读取 RDD, 然后才会真正的执行转换操作.</p>
<p>Task 在启动之初读取一个分区的时, 会先判断这个分区是否已经被持久化, 如果没有则需要检查 Checkpoint 或按照血统重新计算.</p>
<p>如果要在一个 RDD 上执行多次行动, 可以在第一次行动中使用 persis 或 cache 方法, 在内存或磁盘中持久化或缓存这个 RDD, 从而在后面的Action 时提示计算速度.</p>
<p>事实上, cache 方法是使用默认的 MEMORY_ONLY的存储级别将 RDD 持久化到内存, 所以缓存是一种特殊的持久化.</p>
<p>堆内内存和堆外内存的设计, 便可以对缓存 RDD 时使用的内存做统一的规划和管理。</p>
<h2 id="7-Spark性能优化"><a href="#7-Spark性能优化" class="headerlink" title="7. Spark性能优化"></a>7. Spark性能优化</h2><h3 id="7-1-Spark-常用配置参数"><a href="#7-1-Spark-常用配置参数" class="headerlink" title="7.1 Spark 常用配置参数"></a>7.1 Spark 常用配置参数</h3><p>执行<code>submit</code>脚本的时候，可以指定一些配置参数，</p>
<p>例如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/spark/bin/spark-submit\</span><br><span class="line">--class spark.WordCount \</span><br><span class="line">--num-executor 80 \</span><br><span class="line">...</span><br><span class="line">/usr/locla/spark/spark.jar</span><br></pre></td></tr></table></figure>



<p>下面列举几个常用的参数：</p>
<table>
<thead>
<tr>
<th>参数名</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>num-executors</code></td>
<td>设置executor的数量</td>
</tr>
<tr>
<td><code>driver-memory</code></td>
<td>设置driver端的内存大小</td>
</tr>
<tr>
<td><code>executor-memory</code></td>
<td>设置每个executor的内存大小</td>
</tr>
<tr>
<td><code>executor-cores</code></td>
<td>设置每个executor的cpu核心数</td>
</tr>
<tr>
<td><code>master yarn</code></td>
<td>设置集群模式为Yarn</td>
</tr>
<tr>
<td><code>deploy-mode cluster</code></td>
<td>设置yarn的部署模式为cluster（client）</td>
</tr>
</tbody></table>
<h3 id="7-2-常规性能调优一：RDD复用"><a href="#7-2-常规性能调优一：RDD复用" class="headerlink" title="7.2 常规性能调优一：RDD复用"></a>7.2 常规性能调优一：RDD复用</h3><ul>
<li><p>在对RDD进行算子时，要避免相同的算子和计算逻辑之下对 RDD 进行重复的计算:</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810095712565-8560634-8561548.png" alt="image-20210810095712565">                               </p>
<p>对上图中的RDD计算架构进行修改:</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810095721948-8560643-8561543.png" alt="image-20210810095721948"></p>
</li>
</ul>
<h3 id="7-2-常规性能调优二：RDD持久化"><a href="#7-2-常规性能调优二：RDD持久化" class="headerlink" title="7.2 常规性能调优二：RDD持久化"></a>7.2 常规性能调优二：RDD持久化</h3><ul>
<li><ul>
<li>在Spark中，当多次对同一个 RDD 执行算子操作时，每一次都会对这个 RDD 的祖先 RDD 重新计算一次，这种情况是必须要避免的，对同一个RDD的重复计算是对资源的极大浪费，因此，必须对多次使用的RDD进行持久化，通过持久化将公共RDD的数据缓存到内存/磁盘中，之后对于公共RDD的计算都会从内存/磁盘中直接获取RDD数据。 对于RDD的持久化，有两点需要说明： </li>
<li>RDD的持久化是可以进行序列化的，当内存无法将RDD的数据完整的进行存放的时候，可以考虑使用序列化的方式减小数据体积，将数据完整存储在内存中。</li>
<li>如果对于数据的可靠性要求很高，并且内存充足，可以使用副本机制，对RDD数据进行持久化。当持久化启用了复本机制时，对于持久化的每个数据单元都存储一个副本，放在其他节点上面，由此实现数据的容错，一旦一个副本数据丢失，不需要重新计算，还可以使用另外一个副本。</li>
</ul>
</li>
<li><strong>RDD尽可能尽早的filter操作</strong></li>
</ul>
<h3 id="7-3-常规性能调优三：并行度调节"><a href="#7-3-常规性能调优三：并行度调节" class="headerlink" title="7.3 常规性能调优三：并行度调节"></a>7.3 常规性能调优三：并行度调节</h3><p><code>set spark.default.parallelism=500;</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().set(<span class="string">&quot;spark.default.parallelism&quot;</span>, <span class="string">&quot;500&quot;</span>);</span><br></pre></td></tr></table></figure>

<p><font color=green>一般情况下，task数量应该设置为Spark作业总CPU Core数量的2~3倍。</font></p>
<p>之所以没有推荐task数量与CPU core总数相等，是因为task的执行时间不同，有的task执行速度快而有的task执行速度慢，如果task数量与CPU core总数相等，那么执行快的task执行完成后，会出现CPU core空闲的情况。如果task数量设置为CPU core总数的2~3倍，那么一个task执行完毕后，CPU core会立刻执行下一个task，降低了资源的浪费，同时提升了Spark作业运行的效率。</p>
<h3 id="7-4-常规性能调优四：广播大变量"><a href="#7-4-常规性能调优四：广播大变量" class="headerlink" title="7.4 常规性能调优四：广播大变量"></a>7.4 常规性能调优四：广播大变量</h3><p>默认情况下，task 中的算子中如果使用了外部的变量，每个 task 都会获取一份变量的复本，这就造成了内存的极大消耗。 - 一方面，如果后续对 RDD 进行持久化，可能就无法将 RDD 数据存入内存，只能写入磁盘，磁盘IO将会严重消耗性能； - 另一方面，task在创建对象的时候，也许会发现堆内存无法存放新创建的对象，这就会导致频繁的GC，GC会导致工作线程停止，进而导致Spark暂停工作一段时间，严重影响Spark性能。</p>
<p>假设当前任务配置了20个Executor，指定500个task，有一个20M的变量被所有task共用，此时会在500个task中产生500个副本，耗费集群10G的内存，如果使用了广播变量， 那么每个Executor保存一个副本，一共消耗400M内存，内存消耗减少了5倍。</p>
<p>广播变量在每个Executor保存一个副本，此Executor的所有task共用此广播变量，这让变量产生的副本数量大大减少。</p>
<p>在初始阶段，广播变量只在Driver中有一份副本。task在运行的时候，想要使用广播变量中的数据，此时首先会在自己本地的Executor对应的BlockManager中尝试获取变量，如果本地没有，BlockManager就会从Driver或者其他节点的BlockManager上远程拉取变量的复本，并由本地的BlockManager进行管理；之后此Executor的所有task都会直接从本地的BlockManager中获取变量。</p>
<h3 id="7-5-算子调优"><a href="#7-5-算子调优" class="headerlink" title="7.5 算子调优"></a>7.5 算子调优</h3><h4 id="1-mapPartitions"><a href="#1-mapPartitions" class="headerlink" title="1. mapPartitions"></a>1. mapPartitions</h4><p>普通的 map 算子对 RDD 中的每一个元素进行操作，而 mapPartitions 算子对 RDD 中每一个分区进行操作。</p>
<p>如果是普通的map算子，假设一个 partition 有 1 万条数据，那么 map 算子中的 function 要执行1万次，也就是对每个元素进行操作。</p>
<p>如果是 mapPartition 算子，由于一个 task 处理一个 RDD 的partition，那么一个task只会执行一次function，function一次接收所有的partition数据，效率比较高。</p>
<p>mapPartitions算子也存在一些缺点：对于普通的map操作，一次处理一条数据，如果在处理了2000条数据后内存不足，那么可以将已经处理完的2000条数据从内存中垃圾回收掉；但是如果使用mapPartitions算子，但数据量非常大时，function一次处理一个分区的数据，如果一旦内存不足，此时无法回收内存，就可能会OOM，即内存溢出。</p>
<p>因此，mapPartitions算子适用于数据量不是特别大的时候，此时使用mapPartitions算子对性能的提升效果还是不错的。（当数据量很大的时候，一旦使用mapPartitions算子，就会直接OOM） 在项目中，应该首先估算一下RDD的数据量、每个partition的数据量，以及分配给每个Executor的内存资源，如果资源允许，可以考虑使用mapPartitions算子代替map。</p>
<h4 id="2-foreachPartition优化数据库操作"><a href="#2-foreachPartition优化数据库操作" class="headerlink" title="2. foreachPartition优化数据库操作"></a>2. foreachPartition优化数据库操作</h4><p>使用了foreachPartition算子后，可以获得以下的性能提升：</p>
<ol>
<li>对于我们写的function函数，一次处理一整个分区的数据</li>
<li>对于一个分区的数据，创建唯一的数据库连接</li>
<li>只需要向数据发送一次SQL语句和多组参数</li>
</ol>
<p>在生产环境中，全部都会使用foreachPartition算子完成数据库操作。foreachPartition算子存在一个问题，与mapPartitions算子类似，如果一个分区的数据量特别大，可能会造成OOM，即内存溢出。</p>
<h4 id="3-filter与coalesce的配合使用"><a href="#3-filter与coalesce的配合使用" class="headerlink" title="3. filter与coalesce的配合使用"></a>3. filter与coalesce的配合使用</h4><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810102102079-8562063.png" alt="image-20210810102102079" style="zoom:50%;" />

<p>在Spark任务中我们经常会使用filter算子完成RDD中数据的过滤，在任务初始阶段，从各个分区中加载到的数据量是相近的，但是一旦进过filter过滤后，每个分区的数据量有可能会存在较大差异。</p>
<p>在上图中, 第二个分区的数据过滤后只剩100条，而第三个分区的数据过滤后剩下800条，在相同的处理逻辑下，第二个分区对应的task处理的数据量与第三个分区对应的task处理的数据量差距达到了8倍，这也会导致运行速度可能存在数倍的差距，这也就是<strong>数据倾斜问题</strong>。</p>
<p>可以通过repartition与coalesce都可以进行重分区，其中repartition只是coalesce接口中shuffle为true的简易实现，coalesce默认情况下不进行shuffle。</p>
<blockquote>
<p>可以在filter操作之后，使用coalesce算子针对每个partition的数据量各不相同的情况，压缩partition的数量，而且让每个partition的数据量尽量均匀紧凑，以便于后面的task进行计算操作，在某种程度上能够在一定程度上提升性能。</p>
</blockquote>
<h4 id="4-repartition-解决SparkSQL低并行度问题"><a href="#4-repartition-解决SparkSQL低并行度问题" class="headerlink" title="4. repartition 解决SparkSQL低并行度问题"></a>4. repartition 解决SparkSQL低并行度问题</h4><p><code>set spark.default.parallelism=500;</code>对于SparkSQL是不生效的，用户设置的并行度只对于SparkSQL以外的所有Spark的stage生效。</p>
<p>Spark SQL的并行度不允许用户自己指定，Spark SQL自己会默认根据 hive 表对应的 HDFS 文件的 split 个数自动设置 Spark SQL 所在的那个 stage 的并行度，用户自己通spark.default.parallelism参数指定的并行度，只会在没Spark SQL的stage中生效。</p>
<p>由于Spark SQL所在stage的并行度无法手动设置，如果数据量较大，并且此stage中后续的transformation操作有着复杂的业务逻辑，而Spark SQL自动设置的task数量很少，这就意味着每个task要处理为数不少的数据量，然后还要执行非常复杂的处理逻辑，这就可能表现为第一个有 Spark SQL 的 stage 速度很慢，而后续的没有 Spark SQL 的 stage 运行速度非常快。</p>
<p>为了解决SparkSQL无法设置并行度和task数量的问题，可以使用repartition算子。</p>
<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810102807976-8562489.png" alt="image-20210810102807976" style="zoom:50%;" />

<p>Spark SQL这一步的并行度和task数量肯定是没有办法去改变了，但是，对于Spark SQL查询出来的RDD，立即使用repartition算子，去重新进行分区，这样可以重新分区为多个partition，从repartition之后的RDD操作，由于不再涉及 Spark SQL，因此 stage 的并行度就会等于你手动设置的值，这样就避免了 Spark SQL 所在的 stage 只能用少量的 task 去处理大量数据并执行复杂的算法逻辑。</p>
<h4 id="5-reduceByKey预聚合"><a href="#5-reduceByKey预聚合" class="headerlink" title="5. reduceByKey预聚合"></a>5. reduceByKey预聚合</h4><p>实际生产中，尽量使用带有预聚合的算子，而避免使用（groupByKey这类没有预聚合的算子）</p>
<p>所谓预聚合：就是在map端就进行一次combine操作。</p>
<h3 id="7-6-Shuffle调优"><a href="#7-6-Shuffle调优" class="headerlink" title="7.6 Shuffle调优"></a>7.6 Shuffle调优</h3><h4 id="1-调节map端缓冲区大小"><a href="#1-调节map端缓冲区大小" class="headerlink" title="1. 调节map端缓冲区大小"></a>1. 调节map端缓冲区大小</h4><p>在 Spark 任务运行过程中，如果 shuffle 的map端处理的数据量比较大，但是map端缓冲的大小是固定的，可能会出现map端缓冲数据频繁<strong>spill溢写到磁盘</strong>文件中的情况，使得性能非常低下，通过调节map端缓冲的大小，可以避免频繁的磁盘 IO 操作，进而提升 Spark 任务的整体性能。</p>
<p>map端缓冲的默认配置是<code>32KB</code>, 如果每个task处理640KB的数据，那么会发生640/32 = 20次溢写，如果每个task处理64000KB的数据，机会发生64000/32=2000此溢写，这对于性能的影响是非常严重的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().set(<span class="string">&quot;spark.shuffle.file.buffer&quot;</span>, <span class="string">&quot;64&quot;</span>);</span><br></pre></td></tr></table></figure>

<h4 id="2-调节reduce端缓冲区大小"><a href="#2-调节reduce端缓冲区大小" class="headerlink" title="2. 调节reduce端缓冲区大小"></a>2. 调节reduce端缓冲区大小</h4><p>Spark Shuffle 过程中，shuffle reduce task 的 buffer缓冲区大小决定了reduce task 每次能够缓冲的数据量，也就是每次能够拉取的数据量，如果内存资源较为充足，<strong>适当增加拉取数据缓冲区的大小，可以减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。</strong></p>
<p>reduce端数据拉取缓冲区的大小可以通过spark.reducer.maxSizeInFlight参数进行设置，默认为48MB</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().set(<span class="string">&quot;spark.reducer.maxSizeInFlight&quot;</span>, <span class="string">&quot;96&quot;</span>);</span><br></pre></td></tr></table></figure>

<h4 id="3-调节reduce端拉取数据重试次数"><a href="#3-调节reduce端拉取数据重试次数" class="headerlink" title="3. 调节reduce端拉取数据重试次数"></a>3. 调节reduce端拉取数据重试次数</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().set(<span class="string">&quot;spark.shuffle.io.maxRetries&quot;</span>, <span class="string">&quot;6&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>Spark Shuffle 过程中，reduce task 拉取属于自己的数据时，如果因为网络异常等原因导致失败会自动进行重试。对于那些包含了特别耗时的 shuffle 操作的作业，建议增加重试最大次数（比如60次），以避免由于 JVM 的full gc 或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle 过程，调节该参数可以大幅度提升稳定性。</p>
<p>reduce 端拉取数据重试次数可以通过spark.shuffle.io.maxRetries参数进行设置，该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败，默认为3，</p>
<h4 id="4-调节reduce端拉取数据等待间隔"><a href="#4-调节reduce端拉取数据等待间隔" class="headerlink" title="4. 调节reduce端拉取数据等待间隔"></a>4. 调节reduce端拉取数据等待间隔</h4><p>Spark Shuffle 过程中，reduce task 拉取属于自己的数据时，如果因为网络异常等原因导致失败会自动进行重试，在一次失败后，会等待一定的时间间隔再进行重试，可以通过加大间隔时长（比如60s），以增加shuffle操作的稳定性。</p>
<p>reduce端拉取数据等待间隔可以通过spark.shuffle.io.retryWait参数进行设置，默认值为5s，</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">  .set(<span class="string">&quot;spark.shuffle.io.retryWait&quot;</span>, <span class="string">&quot;60s&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="5-调节SortShuffle排序操作阈值"><a href="#5-调节SortShuffle排序操作阈值" class="headerlink" title="5. 调节SortShuffle排序操作阈值"></a>5. 调节SortShuffle排序操作阈值</h4><p>对于SortShuffleManager，如果shuffle reduce task的数量小于某一阈值则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</p>
<p>当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量，那么此时map-side就不会进行排序了，减少了排序的性能开销，但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。 SortShuffleManager排序操作阈值的设置可以通过spark.shuffle.sort. bypassMergeThreshold这一参数进行设置，默认值为200，</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().set(<span class="string">&quot;spark.shuffle.sort.bypassMergeThreshold&quot;</span>, <span class="string">&quot;400&quot;</span>);</span><br></pre></td></tr></table></figure>

<h3 id="7-7-内存调优"><a href="#7-7-内存调优" class="headerlink" title="7.7 内存调优"></a>7.7 内存调优</h3><p>增大堆外内存<code>--conf spark.executor.memoryOverhead=2048;</code></p>
<h2 id="8-Spark数据倾斜解决方案"><a href="#8-Spark数据倾斜解决方案" class="headerlink" title="8. Spark数据倾斜解决方案"></a>8. Spark数据倾斜解决方案</h2><p>Spark 中的数据倾斜问题主要指shuffle过程中出现的数据倾斜问题，是由于不同的key对应的数据量不同导致的不同task所处理的数据量不同的问题。</p>
<p>例如，reduce点一共要处理100万条数据，第一个和第二个task分别被分配到了1万条数据，计算5分钟内完成，第三个task分配到了98万数据，此时第三个task可能需要10个小时完成，这使得整个Spark作业需要10个小时才能运行完成，这就是数据倾斜所带来的后果。</p>
<p>注意，要区分开数据倾斜与数据量过量这两种情况，数据倾斜是指少数task被分配了绝大多数的数据，因此少数task运行缓慢；数据过量是指所有task被分配的数据量都很大，相差不多，所有task都运行缓慢。</p>
<p>数据倾斜的表现：</p>
<ol>
<li><p>Spark 作业的大部分 task 都执行迅速，只有有限的几个task执行的非常慢，此时可能出现了数据倾斜，作业可以运行，但是运行得非常慢；</p>
</li>
<li><p>Spark 作业的大部分task都执行迅速，但是有的task在运行过程中会突然报出OOM，反复执行几次都在某一个task报出OOM错误，此时可能出现了数据倾斜，作业无法正常运行。</p>
</li>
</ol>
<h3 id="8-1-聚合原数据"><a href="#8-1-聚合原数据" class="headerlink" title="8.1 聚合原数据"></a>8.1 聚合原数据</h3><ol>
<li>避免shuffle过程<ul>
<li>可以在hive端直接将每个key对应的数据拼接成一个大大的字符串，避免shuffle</li>
</ul>
</li>
<li>缩小key的粒度（增大数据倾斜的可能性，降低每个task的数据量）</li>
<li>增大key的粒度（减少数据倾斜的可能性，增大每个task的数据量）</li>
</ol>
<h3 id="8-2-过滤掉导致倾斜的key"><a href="#8-2-过滤掉导致倾斜的key" class="headerlink" title="8.2 过滤掉导致倾斜的key"></a>8.2 过滤掉导致倾斜的key</h3><p>有些时候，导致数据倾斜的key可能为null，提前将这个无用的导致的倾斜的key过滤掉。</p>
<p>如果有真实的key导致了数据倾斜，我们可以单独抽取出来这个key，通过拼接随机数后缀，将key进行打散，进而可以在一定程度上缓解数据倾斜所带来的问题。</p>
<h3 id="8-3-提高shuffle操作中的reduce并行度"><a href="#8-3-提高shuffle操作中的reduce并行度" class="headerlink" title="8.3 提高shuffle操作中的reduce并行度"></a>8.3 提高shuffle操作中的reduce并行度</h3><p>当方案一和方案二对于数据倾斜的处理没有很好的效果时，可以考虑提高shuffle过程中的reduce端并行度，reduce端并行度的提高就增加了reduce端task的数量，那么每个task分配到的数据量就会相应减少，由此缓解数据倾斜问题。</p>
<p>对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即<code>spark.sql.shuffle.partitions</code>，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p>
<p>提高reduce端并行度并没有从根本上改变数据倾斜的本质和问题（方案一和方案二从根本上避免了数据倾斜的发生），只是尽可能地去缓解和减轻shuffle reduce task的数据压力，以及数据倾斜的问题，适用于有较多key对应的数据量都比较大的情况。</p>
<p>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p>
<p>在理想情况下，reduce端并行度提升后，会在一定程度上减轻数据倾斜的问题，甚至基本消除数据倾斜；但是，在一些情况下，只会让原来由于数据倾斜而运行缓慢的task运行速度稍有提升，或者避免了某些task的OOM问题，但是，仍然运行缓慢，此时，要及时放弃方案三，开始尝试后面的方案。</p>
<h3 id="8-4-使用随机key实现双重聚合"><a href="#8-4-使用随机key实现双重聚合" class="headerlink" title="8.4 使用随机key实现双重聚合"></a>8.4 使用随机key实现双重聚合</h3><p>当使用了类似于groupByKey、reduceByKey这样的算子时，可以考虑使用随机key实现双重聚合</p>
<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810111030982-8565032.png" alt="image-20210810111030982" style="zoom:50%;" />



<p>首先，通过map算子给每个数据的key添加随机数前缀，对key进行打散，将原先一样的key变成不一样的key，然后进行第一次聚合，这样就可以让原本被一个task处理的数据分散到多个task上去做局部聚合；</p>
<p>随后，去除掉每个key的前缀，再次进行聚合。</p>
<p>此方法对于有<code>groupByKey,reduceByKey</code>这类算子造成的数据倾斜有比较好的效果，仅仅适用于聚合类的shuffle操作，使用范围相对较窄。</p>
<h3 id="8-5-将reduce-join-转换成map-join"><a href="#8-5-将reduce-join-转换成map-join" class="headerlink" title="8.5 将reduce join 转换成map join"></a>8.5 将reduce join 转换成map join</h3><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810111500233-8565301.png" alt="image-20210810111500233" style="zoom:50%;" />

<p>正常情况下，join操作都会执行shuffle过程，并且执行的是reduce join，也就是先将所有相同的key和对应的value汇聚到一个reduce task中，然后再进行join。</p>
<p>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。</p>
<p>但是一个数据量特别大，一个数据量特别小，这两个数据进行join，就很有可能产生数据倾斜，导致大量的数据key都会聚集到一小部分reduce task中。</p>
<p><strong>解决办法</strong></p>
<p>通过将小数据量RDD中的数据 broadcast广播出去，然后通过map join的方式实现与reduce join相同的效果。此时就不会发生shuffle操作，也就不会发生数据倾斜了。</p>
<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810111914642-8565556.png" alt="image-20210810111914642" style="zoom:50%;" />

<h3 id="8-6-sample采样对倾斜key单独进行join"><a href="#8-6-sample采样对倾斜key单独进行join" class="headerlink" title="8.6 sample采样对倾斜key单独进行join"></a>8.6 sample采样对倾斜key单独进行join</h3><h3 id="8-7-使用随机数以及扩容进行join"><a href="#8-7-使用随机数以及扩容进行join" class="headerlink" title="8.7 使用随机数以及扩容进行join"></a>8.7 使用随机数以及扩容进行join</h3><p>如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了，对于join操作，我们可以考虑对其中一个RDD数据进行扩容，另一个RDD进行稀释后再join。</p>
<p>我们会将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。</p>
<p>这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，需要对整个RDD进行数据扩容，对内存资源要求很高。</p>
<p><strong>核心思想</strong></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210810153402154-8580843.png" alt="image-20210810153402154"></p>
<p>选择一个RDD，使用flatMap进行扩容，对每条数据的key添加数值前缀（1~N的数值），将一条数据映射为多条数据；（扩容）</p>
<p>选择另外一个RDD，进行map映射操作，每条数据的key都打上一个随机数作为前缀（1~N的随机数）；（稀释）</p>
<h3 id="8-8-更换分区器（RangePartitioner）"><a href="#8-8-更换分区器（RangePartitioner）" class="headerlink" title="8.8 更换分区器（RangePartitioner）"></a>8.8 更换分区器（RangePartitioner）</h3><h3 id="8-9-两张大的表进行join（无法广播），产生数据倾斜"><a href="#8-9-两张大的表进行join（无法广播），产生数据倾斜" class="headerlink" title="8.9 两张大的表进行join（无法广播），产生数据倾斜"></a>8.9 两张大的表进行join（无法广播），产生数据倾斜</h3><p>两个RDD进行join的时候，如果数据量都比较大，那么此时可以sample看下两个RDD中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD中的少数几个key的数据量过大，而另一个RDD中的所有key都分布比较均匀，此时可以考虑采用本解决方案。</p>
<p><strong><font color=red>解决方案</font></strong></p>
<ol>
<li><strong>对数据倾斜的那个RDD，使用sample算子采样出一份样本数据，统计下每个key的数量，看看导致数据倾斜的数据量最大的是哪几个key。</strong></li>
<li>然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给<strong>每个key</strong>都打上<code>0~n</code>以内的随机前缀；不会导致数据倾斜的大部分key形成另外一个RDD。</li>
<li>接着将join的另一个RDD过滤出来2中所提取出来的那些导致数据倾斜的key，也单独形成一个RDD。这个RDD中的key是少量的，将每条数据膨胀成n倍，这n条数据都按照顺序添加一个<code>0~1</code>的前缀；那些不会导致数据倾斜的key单独形成另外一个RDD。</li>
<li>再将随机附加了随机前缀的RDD与另一个膨胀了n倍的RDD进行join，这样就可以将原先相同的key打散成n分，分散到多个task中去进行join</li>
<li>而另外两个普通的RDD就照常join即可</li>
<li>最后将两次join的结果使用union算子合并起来即可，最终得到join的结果。</li>
</ol>
<blockquote>
<p>方案优点</p>
</blockquote>
<p>对于两个大RDD进行join时的数据倾斜，如果只是某一个key导致了数据倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容，避免了占用过多内存。</p>
<blockquote>
<p>方法局限性</p>
</blockquote>
<p>如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜了，就不能使用本解决方案了。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/meihao5/article/details/81084876">Spark数据倾斜的解决方案：参考</a></p>
<h3 id="8-10-增大并行度，增大资源"><a href="#8-10-增大并行度，增大资源" class="headerlink" title="8.10 增大并行度，增大资源"></a>8.10 增大并行度，增大资源</h3><h2 id="9-Spark中涉及到的join"><a href="#9-Spark中涉及到的join" class="headerlink" title="9. Spark中涉及到的join"></a>9. Spark中涉及到的join</h2><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210917191421213-16318772632891.png" alt="image-20210917191421213"></p>
<h3 id="Hash-Join"><a href="#Hash-Join" class="headerlink" title="Hash Join"></a>Hash Join</h3><p>以一个简单的SQL语句为例：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">		t1.<span class="operator">*</span></span><br><span class="line">		,t2.<span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">        (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">                    <span class="operator">*</span></span><br><span class="line">            <span class="keyword">from</span>	<span class="keyword">order</span></span><br><span class="line">        ) t1</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">        (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">                    <span class="operator">*</span></span><br><span class="line">            <span class="keyword">from</span>	item</span><br><span class="line">        ) t2</span><br><span class="line"><span class="keyword">on</span> t1.id<span class="operator">=</span>t2.id;</span><br></pre></td></tr></table></figure>

<p>参与join的两张表是<code>item</code>和<code>order</code>，join key分别是<code>item.id</code>以及<code>order.id</code>，假设这个Join采用的是Hash Join算法，整个过程会经历三步：</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-16318778783212-16318778797984.webp" alt="img"></p>
<ol>
<li>确定<strong>Build Table</strong>（映射表、小表）以及<strong>Probe Table</strong>（探查表、大表）。其中<strong>Build Table</strong>用于构建<strong>Hash Table</strong>，而<strong>Probe</strong>会遍历自身所有<strong>Key</strong>，映射到所生成的<strong>Hash Table</strong>上去匹配。</li>
<li><strong>Build Table</strong>构建<strong>Hash Table</strong>。依次读取<strong>Build Table</strong>（Item）的数据，对于每一行数据根据Join的<strong>key</strong>（item.id）进行<strong>hash</strong>，<strong>hash</strong>到对应的<strong>Bucket</strong>，生成<strong>hash table</strong>中的一条记录。数据缓存在内存中，如果内存放不下需要<strong>dump</strong>到外存。</li>
<li><strong>Probe Table</strong>探测。依次扫描<strong>Probe Table</strong>（order）的数据，使用相同的<strong>hash函数</strong>进行映射，如果成功<strong>映射到Hash Table</strong>之后，再<strong>检查join条件</strong>（item.id=order.id），如果匹配成功就可以将两者join在一起。</li>
</ol>
<p><strong>补充点</strong></p>
<blockquote>
<ol>
<li>Hash Join 的性能。从上面的原理图可以看出，hash join对两张表基本只扫描依次，**算法效率是O(a+b)*<em>，比起蛮力的笛卡尔积算法的a</em>b快了很多数量级。</li>
<li>Build Table要尽量选择小表：<ul>
<li>从原理上可以看出，构建的Hash Table是需要被频繁访问的，所以<strong>Hash Table最好能全部加载到内存中</strong></li>
<li>这也决定了Hash Table只适合至少一个小表join的场景</li>
</ul>
</li>
</ol>
</blockquote>
<p>上述介绍的Hash join 是在单机版中运行的join算法，而在大数据分布式情况下，目前成熟的有两套Hash Join算法：``broadcast hash join<code>和</code>shuffle hash join`</p>
<h4 id="Broadcast-Hash-Join"><a href="#Broadcast-Hash-Join" class="headerlink" title="Broadcast Hash Join"></a>Broadcast Hash Join</h4><p><code>broadcast hash join</code>是将其中一张小表广播分发到另一张大表所在的分区节点上，分别并发地与上的分区记录进行<code>hash join</code>。broadcast适用于小表很小，可以直接广播的场景。</p>
<p>需要注意的是，Spark中对于可以广播的小表，默认限制是10M以下。（参数是<code>spark.sql.autoBroadcastJoinThreshold</code>）</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-16318798352397-16318798366029.webp" alt="img"></p>
<p>在执行上，主要可以分为以下两步：</p>
<ol>
<li>broadcast阶段：将小表广播分发到大表所在的所有主机。分发方式可以是driver分发，或者是p2p的方式。</li>
<li>hash join阶段：在每个executor上执行单机版的hash join，<strong>小表映射，大表试探</strong></li>
</ol>
<h4 id="Shuffle-Hash-Join"><a href="#Shuffle-Hash-Join" class="headerlink" title="Shuffle Hash Join"></a>Shuffle Hash Join</h4><p>当join的一张表很小的时候，使用broadcast hash join，无疑效率最高。但是随着小表逐渐变大，广播所需内存，带宽等资源必然就会太大，所以才会有默认10M的资源限制。</p>
<p>所以，当小表逐渐变大时，就需要采用另一种Hash Join来处理：<code>Shuffle Hash Join</code>。</p>
<p><code>Shuffle Hash Join</code>按照join key进行分区，根据<strong>key相同必然分区相同的原理</strong>，将大表join分而治之，划分为小表的join，充分利用集群资源并行化执行。</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-163188039067910-163188039254812.webp" alt="img"></p>
<ol>
<li><strong>Shuffle阶段</strong><ul>
<li>分别将两个表按照join key进行分区，将相同join key的记录重分布到同一节点，两张表的数据会被分布到集群中所有节点。这个过程称为shuffle</li>
</ul>
</li>
<li><strong>Hash Join阶段</strong><ul>
<li>每个分区节点上的数据单独执行单机hash join算法</li>
</ul>
</li>
</ol>
<p>如果两张小表join，可以直接使用单机版的hash join；如果一张大表join一张极小表，可以选择broadcast hash join算法；而如果是一张大表join一张小表，小表不能广播超过10M了，则可以选择shuffle hash join算法；</p>
<p><strong>如果两张大表进行join呢？</strong></p>
<h3 id="Sort-Merge-Join"><a href="#Sort-Merge-Join" class="headerlink" title="Sort-Merge Join"></a>Sort-Merge Join</h3><p><strong>没有使用HashJoin，即单机join的时候，没有HashTable的生成。</strong></p>
<p>Spark SQL对两张大表join采用了全新的算法：Sort-Merge Join，整个过程分为三步步骤：</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-16319326499621-16319326513313.webp" alt="img"></p>
<ol>
<li><strong>Shuffle阶段</strong><ul>
<li>将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式并行处理</li>
</ul>
</li>
<li><strong>Sort阶段</strong><ul>
<li>对单个分区节点的两表数据，分别进行排序</li>
</ul>
</li>
<li><strong>Merge阶段</strong><ul>
<li>对排好序的两张分区表数据执行join操作。join操作很简单，分别遍历两个有序序列，碰到相同join key就merge输出，否则取更小一边，（<font color=red>类似于双指针合并两个有序序列</font>）,如下图所示：</li>
<li><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-16319328669854-16319328684116.webp" alt="img"></li>
</ul>
</li>
</ol>
<p>仔细分析可以发现，Sort-Merge Join的单价并不比Shuffle hash join小，反而是多了排序。<strong>那么为什么SparkSQL还会在两张大表的场景下选择使用Sort-Merge Join算法呢？</strong></p>
<p>当Spark采用<strong>Sort Shuffle</strong>时，在经过Shuffle之后，Partition的数据都是按照key排序的，因此理论上可以认为数据经过Shuffle之后是不需要Sort的，可以直接Merge。</p>
<h3 id="Join时通过BloomFilter布隆过滤器进行-进行谓词下推"><a href="#Join时通过BloomFilter布隆过滤器进行-进行谓词下推" class="headerlink" title="Join时通过BloomFilter布隆过滤器进行 进行谓词下推"></a>Join时通过BloomFilter布隆过滤器进行 进行谓词下推</h3><p>谓词下推是将过滤条件从计算进程下推到存储进程先行执行，注意这里有两种类型进程：计算进程以及存储进程。计算与存储分离思想，这在大数据领域相当常见，比如最常见的计算进程有SparkSQL、Hive、impala等，负责SQL解析优化、数据计算聚合等，存储进程有HDFS（DataNode）、Kudu、HBase，负责数据存储。正常情况下应该是将所有数据从存储进程加载到计算进程，再进行过滤计算。谓词下推是说将一些过滤条件下推到存储进程，直接让存储进程将数据过滤掉。这样的好处显而易见，过滤的越早，数据量越少，序列化开销、网络开销、计算开销这一系列都会减少，性能自然会提高。</p>
<blockquote>
<p>在逻辑执行计划优化层面，比如SQL语句：select * from order ,item where item.id = order.item_id and item.category = ‘book’，正常情况语法解析之后应该是先执行Join操作，再执行Filter操作。通过谓词下推，可以将Filter操作下推到Join操作之前执行。即将where item.category = ‘book’下推到 item.id = order.item_id之前先行执行。</p>
</blockquote>
<h2 id="10-Spark源码分析"><a href="#10-Spark源码分析" class="headerlink" title="10. Spark源码分析"></a>10. Spark源码分析</h2><h3 id="1-环境准备（Yarn-集群）"><a href="#1-环境准备（Yarn-集群）" class="headerlink" title="1. 环境准备（Yarn 集群）"></a>1. 环境准备（Yarn 集群）</h3><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210922152340903-16322954231991.png" alt="image-20210922152340903"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-submit.cmd脚本下最终会执行一个指令</span><br><span class="line"></span><br><span class="line">java org.apache.spark.deploy.SparkSubmit 执行</span><br></pre></td></tr></table></figure>

<p><strong>SparkSubmit.scala</strong></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210922155639198-16322974007352.png" alt="image-20210922155639198"></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210922155810394-16322974914213.png" alt="image-20210922155810394"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doSubmit</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// Initialize logging if it hasn&#x27;t been done yet. Keep track of whether logging needs to</span></span><br><span class="line">  <span class="comment">// be reset before the application starts.</span></span><br><span class="line">  <span class="keyword">val</span> uninitLog = initializeLogIfNecessary(<span class="literal">true</span>, silent = <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> appArgs = parseArguments(args)   <span class="comment">// 解析参数</span></span><br><span class="line">  <span class="keyword">if</span> (appArgs.verbose) &#123;</span><br><span class="line">    logInfo(appArgs.toString)</span><br><span class="line">  &#125;</span><br><span class="line">  appArgs.action <span class="keyword">match</span> &#123;  <span class="comment">// 根据解析好的参数对象的action属性值，匹配具体的动作：是submit，还是kill，还是requestStatus，还是printVersion</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs, uninitLog)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">KILL</span> =&gt; kill(appArgs)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">PRINT_VERSION</span> =&gt; printVersion()</span><br><span class="line">  &#125;</span><br><span class="line">    <span class="comment">// action的默认动作为SUBMIT</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210922160232506-16322977552994.png" alt="image-20210922160232506"></p>
<p>parse方法具体解析<code>spark-sumbit</code>脚本传过来的参数</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210922160803410-16322980847205.png" alt="image-20210922160803410"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tailrec</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submit</span></span>(args: <span class="type">SparkSubmitArguments</span>, uninitLog: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">doRunMain</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (args.proxyUser != <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="keyword">val</span> proxyUser = <span class="type">UserGroupInformation</span>.createProxyUser(args.proxyUser,</span><br><span class="line">                                                                 <span class="type">UserGroupInformation</span>.getCurrentUser())</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                proxyUser.doAs(<span class="keyword">new</span> <span class="type">PrivilegedExceptionAction</span>[<span class="type">Unit</span>]() &#123;</span><br><span class="line">                    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">                        runMain(args, uninitLog)</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">                <span class="comment">// Hadoop&#x27;s AuthorizationException suppresses the exception&#x27;s stack trace, which</span></span><br><span class="line">                <span class="comment">// makes the message printed to the output by the JVM not very helpful. Instead,</span></span><br><span class="line">                <span class="comment">// detect exceptions with empty stack traces here, and treat them differently.</span></span><br><span class="line">                <span class="keyword">if</span> (e.getStackTrace().length == <span class="number">0</span>) &#123;</span><br><span class="line">                    error(<span class="string">s&quot;ERROR: <span class="subst">$&#123;e.getClass().getName()&#125;</span>: <span class="subst">$&#123;e.getMessage()&#125;</span>&quot;</span>)</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">throw</span> e</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            runMain(args, uninitLog)   <span class="comment">// 执行主程序</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runMain</span></span>(args: <span class="type">SparkSubmitArguments</span>, uninitLog: <span class="type">Boolean</span>): <span class="type">Unit</span> = </span><br><span class="line">    	<span class="comment">// 通过准备提交环境方法，模式匹配，返回四个参数，其中重要的是childMainClass</span></span><br><span class="line">        <span class="keyword">val</span> (childArgs, childClasspath, sparkConf, childMainClass) = prepareSubmitEnvironment(args)</span><br><span class="line">        <span class="comment">// Let the main class re-initialize the logging system once it starts.</span></span><br><span class="line">        <span class="keyword">if</span> (uninitLog) &#123;</span><br><span class="line">            <span class="type">Logging</span>.uninitialize()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (args.verbose) &#123;</span><br><span class="line">            logInfo(<span class="string">s&quot;Main class:\n<span class="subst">$childMainClass</span>&quot;</span>)</span><br><span class="line">            logInfo(<span class="string">s&quot;Arguments:\n<span class="subst">$&#123;childArgs.mkString(&quot;\n&quot;)&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="comment">// sysProps may contain sensitive information, so redact before printing</span></span><br><span class="line">            logInfo(<span class="string">s&quot;Spark config:\n<span class="subst">$&#123;Utils.redact(sparkConf.getAll.toMap).mkString(&quot;\n&quot;)&#125;</span>&quot;</span>)</span><br><span class="line">            logInfo(<span class="string">s&quot;Classpath elements:\n<span class="subst">$&#123;childClasspath.mkString(&quot;\n&quot;)&#125;</span>&quot;</span>)</span><br><span class="line">            logInfo(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> loader = getSubmitClassLoader(sparkConf)</span><br><span class="line">        <span class="keyword">for</span> (jar &lt;- childClasspath) &#123;</span><br><span class="line">            addJarToClasspath(jar, loader)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">var</span> mainClass: <span class="type">Class</span>[_] = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 通过上述返回的childMainClass，反射出具体的类</span></span><br><span class="line">            mainClass = <span class="type">Utils</span>.classForName(childMainClass)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">ClassNotFoundException</span> =&gt;</span><br><span class="line">            logError(<span class="string">s&quot;Failed to load class <span class="subst">$childMainClass</span>.&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> (childMainClass.contains(<span class="string">&quot;thriftserver&quot;</span>)) &#123;</span><br><span class="line">                logInfo(<span class="string">s&quot;Failed to load main class <span class="subst">$childMainClass</span>.&quot;</span>)</span><br><span class="line">                logInfo(<span class="string">&quot;You need to build Spark with -Phive and -Phive-thriftserver.&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkUserAppException</span>(<span class="type">CLASS_NOT_FOUND_EXIT_STATUS</span>)</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">NoClassDefFoundError</span> =&gt;</span><br><span class="line">            logError(<span class="string">s&quot;Failed to load <span class="subst">$childMainClass</span>: <span class="subst">$&#123;e.getMessage()&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> (e.getMessage.contains(<span class="string">&quot;org/apache/hadoop/hive&quot;</span>)) &#123;</span><br><span class="line">                logInfo(<span class="string">s&quot;Failed to load hive class.&quot;</span>)</span><br><span class="line">                logInfo(<span class="string">&quot;You need to build Spark with -Phive and -Phive-thriftserver.&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkUserAppException</span>(<span class="type">CLASS_NOT_FOUND_EXIT_STATUS</span>)</span><br><span class="line">        &#125;</span><br><span class="line">		<span class="comment">// 判断childMainClass反射出的类是否继承了SparkApplication类，如果继承了直接返回该类的示例，如果没有继承，直接创建mainClass</span></span><br><span class="line">        <span class="keyword">val</span> app: <span class="type">SparkApplication</span> = <span class="keyword">if</span> (classOf[<span class="type">SparkApplication</span>].isAssignableFrom(mainClass)) &#123;</span><br><span class="line">            mainClass.getConstructor().newInstance().asInstanceOf[<span class="type">SparkApplication</span>]</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">JavaMainApplication</span>(mainClass)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// Following constants are visible for testing.</span></span><br><span class="line">  <span class="keyword">private</span>[deploy] <span class="keyword">val</span> <span class="type">YARN_CLUSTER_SUBMIT_CLASS</span> =</span><br><span class="line">    <span class="string">&quot;org.apache.spark.deploy.yarn.YarnClusterApplication&quot;</span>    </span><br><span class="line"></span><br><span class="line"><span class="comment">// In yarn-cluster mode, use yarn.Client as a wrapper around the user class</span></span><br><span class="line">    <span class="keyword">if</span> (isYarnCluster) &#123;</span><br><span class="line">      childMainClass = <span class="type">YARN_CLUSTER_SUBMIT_CLASS</span></span><br><span class="line">      <span class="keyword">if</span> (args.isPython) &#123;</span><br><span class="line">        childArgs += (<span class="string">&quot;--primary-py-file&quot;</span>, args.primaryResource)</span><br><span class="line">        childArgs += (<span class="string">&quot;--class&quot;</span>, <span class="string">&quot;org.apache.spark.deploy.PythonRunner&quot;</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (args.isR) &#123;</span><br><span class="line">        <span class="keyword">val</span> mainFile = <span class="keyword">new</span> <span class="type">Path</span>(args.primaryResource).getName</span><br><span class="line">        childArgs += (<span class="string">&quot;--primary-r-file&quot;</span>, mainFile)</span><br><span class="line">        childArgs += (<span class="string">&quot;--class&quot;</span>, <span class="string">&quot;org.apache.spark.deploy.RRunner&quot;</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (args.primaryResource != <span class="type">SparkLauncher</span>.<span class="type">NO_RESOURCE</span>) &#123;</span><br><span class="line">          childArgs += (<span class="string">&quot;--jar&quot;</span>, args.primaryResource)</span><br><span class="line">        &#125;</span><br><span class="line">        childArgs += (<span class="string">&quot;--class&quot;</span>, args.mainClass)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (args.childArgs != <span class="literal">null</span>) &#123;</span><br><span class="line">        args.childArgs.foreach &#123; arg =&gt; childArgs += (<span class="string">&quot;--arg&quot;</span>, arg) &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      app.start(childArgs.toArray, sparkConf)  <span class="comment">// org.apache.spark.deploy.yarn.YarnClusterApplication的start方法</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        <span class="keyword">throw</span> findCause(t)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>从上述代码中可以看出，最终<code>childMainClass=org.apache.spark.deploy.yarn.YarnClusterApplication</code></p>
<p><strong>Client类中</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">YarnClusterApplication</span> <span class="keyword">extends</span> <span class="title">SparkApplication</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>], conf: <span class="type">SparkConf</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// SparkSubmit would use yarn cache to distribute files &amp; jars in yarn mode,</span></span><br><span class="line">    <span class="comment">// so remove them from sparkConf here for yarn mode.</span></span><br><span class="line">    conf.remove(<span class="type">JARS</span>)</span><br><span class="line">    conf.remove(<span class="type">FILES</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Client</span>(<span class="keyword">new</span> <span class="type">ClientArguments</span>(args), conf, <span class="literal">null</span>).run() <span class="comment">// 创建一个Client，并执行run方法</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">	<span class="comment">// amClass集群模式和Client模式下具体的取值</span></span><br><span class="line">    <span class="keyword">val</span> amClass =</span><br><span class="line">      <span class="keyword">if</span> (isClusterMode) &#123;</span><br><span class="line">        <span class="type">Utils</span>.classForName(<span class="string">&quot;org.apache.spark.deploy.yarn.ApplicationMaster&quot;</span>).getName</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">Utils</span>.classForName(<span class="string">&quot;org.apache.spark.deploy.yarn.ExecutorLauncher&quot;</span>).getName</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> amArgs =</span><br><span class="line">      <span class="type">Seq</span>(amClass) ++ userClass ++ userJar ++ primaryPyFile ++ primaryRFile ++ userArgs ++</span><br><span class="line">      <span class="type">Seq</span>(<span class="string">&quot;--properties-file&quot;</span>,</span><br><span class="line">        buildPath(<span class="type">Environment</span>.<span class="type">PWD</span>.$$(), <span class="type">LOCALIZED_CONF_DIR</span>, <span class="type">SPARK_CONF_FILE</span>)) ++</span><br><span class="line">      <span class="type">Seq</span>(<span class="string">&quot;--dist-cache-conf&quot;</span>,</span><br><span class="line">        buildPath(<span class="type">Environment</span>.<span class="type">PWD</span>.$$(), <span class="type">LOCALIZED_CONF_DIR</span>, <span class="type">DIST_CACHE_CONF_FILE</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Command for the ApplicationMaster</span></span><br><span class="line">    <span class="keyword">val</span> commands = prefixEnv ++</span><br><span class="line">      <span class="type">Seq</span>(<span class="type">Environment</span>.<span class="type">JAVA_HOME</span>.$$() + <span class="string">&quot;/bin/java&quot;</span>, <span class="string">&quot;-server&quot;</span>) ++</span><br><span class="line">      javaOpts ++ amArgs ++</span><br><span class="line">      <span class="type">Seq</span>(</span><br><span class="line">        <span class="string">&quot;1&gt;&quot;</span>, <span class="type">ApplicationConstants</span>.<span class="type">LOG_DIR_EXPANSION_VAR</span> + <span class="string">&quot;/stdout&quot;</span>,</span><br><span class="line">        <span class="string">&quot;2&gt;&quot;</span>, <span class="type">ApplicationConstants</span>.<span class="type">LOG_DIR_EXPANSION_VAR</span> + <span class="string">&quot;/stderr&quot;</span>)</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> it would be nicer to just make sure there are no null commands here</span></span><br><span class="line">    <span class="keyword">val</span> printableCommands = commands.map(s =&gt; <span class="keyword">if</span> (s == <span class="literal">null</span>) <span class="string">&quot;null&quot;</span> <span class="keyword">else</span> s).toList</span><br><span class="line">    amContainer.setCommands(printableCommands.asJava)</span><br><span class="line"></span><br><span class="line">    logDebug(<span class="string">&quot;===============================================================================&quot;</span>)</span><br><span class="line">    logDebug(<span class="string">&quot;YARN AM launch context:&quot;</span>)</span><br><span class="line">    logDebug(<span class="string">s&quot;    user class: <span class="subst">$&#123;Option(args.userClass).getOrElse(&quot;N/A&quot;)&#125;</span>&quot;</span>)</span><br><span class="line">    logDebug(<span class="string">&quot;    env:&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (log.isDebugEnabled) &#123;</span><br><span class="line">      <span class="type">Utils</span>.redact(sparkConf, launchEnv.toSeq).foreach &#123; <span class="keyword">case</span> (k, v) =&gt;</span><br><span class="line">        logDebug(<span class="string">s&quot;        <span class="subst">$k</span> -&gt; <span class="subst">$v</span>&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    logDebug(<span class="string">&quot;    resources:&quot;</span>)</span><br><span class="line">    localResources.foreach &#123; <span class="keyword">case</span> (k, v) =&gt; logDebug(<span class="string">s&quot;        <span class="subst">$k</span> -&gt; <span class="subst">$v</span>&quot;</span>)&#125;</span><br><span class="line">    logDebug(<span class="string">&quot;    command:&quot;</span>)</span><br><span class="line">    logDebug(<span class="string">s&quot;        <span class="subst">$&#123;printableCommands.mkString(&quot; &quot;)&#125;</span>&quot;</span>)</span><br><span class="line">    logDebug(<span class="string">&quot;===============================================================================&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// send the acl settings into YARN to control who has access via YARN interfaces</span></span><br><span class="line">    <span class="keyword">val</span> securityManager = <span class="keyword">new</span> <span class="type">SecurityManager</span>(sparkConf)</span><br><span class="line">    amContainer.setApplicationACLs(</span><br><span class="line">      <span class="type">YarnSparkHadoopUtil</span>.getApplicationAclsForYarn(securityManager).asJava)</span><br><span class="line">    setupSecurityToken(amContainer)</span><br><span class="line">    amContainer</span><br></pre></td></tr></table></figure>



<p>ResourceManager接收命令，在可用的节点上启动<code>ApplicationMaster</code></p>
<p><strong>ApplicationMaster.scala</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">SignalUtils</span>.registerLogger(log)</span><br><span class="line">    <span class="keyword">val</span> amArgs = <span class="keyword">new</span> <span class="type">ApplicationMasterArguments</span>(args)</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    <span class="keyword">if</span> (amArgs.propertiesFile != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="type">Utils</span>.getPropertiesFromFile(amArgs.propertiesFile).foreach &#123; <span class="keyword">case</span> (k, v) =&gt;</span><br><span class="line">        sparkConf.set(k, v)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Set system properties for each config entry. This covers two use cases:</span></span><br><span class="line">    <span class="comment">// - The default configuration stored by the SparkHadoopUtil class</span></span><br><span class="line">    <span class="comment">// - The user application creating a new SparkConf in cluster mode</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// Both cases create a new SparkConf object which reads these configs from system properties.</span></span><br><span class="line">    sparkConf.getAll.foreach &#123; <span class="keyword">case</span> (k, v) =&gt;</span><br><span class="line">      sys.props(k) = v</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> yarnConf = <span class="keyword">new</span> <span class="type">YarnConfiguration</span>(<span class="type">SparkHadoopUtil</span>.newConfiguration(sparkConf))</span><br><span class="line">    master = <span class="keyword">new</span> <span class="type">ApplicationMaster</span>(amArgs, sparkConf, yarnConf)  <span class="comment">// 创建ApplicationMaster</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ugi = sparkConf.get(<span class="type">PRINCIPAL</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="comment">// We only need to log in with the keytab in cluster mode. In client mode, the driver</span></span><br><span class="line">      <span class="comment">// handles the user keytab.</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(principal) <span class="keyword">if</span> master.isClusterMode =&gt;</span><br><span class="line">        <span class="keyword">val</span> originalCreds = <span class="type">UserGroupInformation</span>.getCurrentUser().getCredentials()</span><br><span class="line">        <span class="type">SparkHadoopUtil</span>.get.loginUserFromKeytab(principal, sparkConf.get(<span class="type">KEYTAB</span>).orNull)</span><br><span class="line">        <span class="keyword">val</span> newUGI = <span class="type">UserGroupInformation</span>.getCurrentUser()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (master.appAttemptId == <span class="literal">null</span> || master.appAttemptId.getAttemptId &gt; <span class="number">1</span>) &#123;</span><br><span class="line">          <span class="comment">// Re-obtain delegation tokens if this is not a first attempt, as they might be outdated</span></span><br><span class="line">          <span class="comment">// as of now. Add the fresh tokens on top of the original user&#x27;s credentials (overwrite).</span></span><br><span class="line">          <span class="comment">// Set the context class loader so that the token manager has access to jars</span></span><br><span class="line">          <span class="comment">// distributed by the user.</span></span><br><span class="line">          <span class="type">Utils</span>.withContextClassLoader(master.userClassLoader) &#123;</span><br><span class="line">            <span class="keyword">val</span> credentialManager = <span class="keyword">new</span> <span class="type">HadoopDelegationTokenManager</span>(sparkConf, yarnConf, <span class="literal">null</span>)</span><br><span class="line">            credentialManager.obtainDelegationTokens(originalCreds)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Transfer the original user&#x27;s tokens to the new user, since it may contain needed tokens</span></span><br><span class="line">        <span class="comment">// (such as those user to connect to YARN).</span></span><br><span class="line">        newUGI.addCredentials(originalCreds)</span><br><span class="line">        newUGI</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        <span class="type">SparkHadoopUtil</span>.get.createSparkUser()</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">// master.run()  ApplicationMaster运行</span></span><br><span class="line">    ugi.doAs(<span class="keyword">new</span> <span class="type">PrivilegedExceptionAction</span>[<span class="type">Unit</span>]() &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">System</span>.exit(master.run())</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">sparkContextInitialized</span></span>(sc: <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    master.sparkContextInitialized(sc)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">getAttemptId</span></span>(): <span class="type">ApplicationAttemptId</span> = &#123;</span><br><span class="line">    master.appAttemptId</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">getHistoryServerAddress</span></span>(</span><br><span class="line">      sparkConf: <span class="type">SparkConf</span>,</span><br><span class="line">      yarnConf: <span class="type">YarnConfiguration</span>,</span><br><span class="line">      appId: <span class="type">String</span>,</span><br><span class="line">      attemptId: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    sparkConf.get(<span class="type">HISTORY_SERVER_ADDRESS</span>)</span><br><span class="line">      .map &#123; text =&gt; <span class="type">SparkHadoopUtil</span>.get.substituteHadoopVariables(text, yarnConf) &#125;</span><br><span class="line">      .map &#123; address =&gt; <span class="string">s&quot;<span class="subst">$&#123;address&#125;</span><span class="subst">$&#123;HistoryServer.UI_PATH_PREFIX&#125;</span>/<span class="subst">$&#123;appId&#125;</span>/<span class="subst">$&#123;attemptId&#125;</span>&quot;</span> &#125;</span><br><span class="line">      .getOrElse(<span class="string">&quot;&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>master.run()</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Int</span> = &#123;</span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line">     <span class="keyword">val</span> attemptID = <span class="keyword">if</span> (isClusterMode) &#123;</span><br><span class="line">       <span class="comment">// Set the web ui port to be ephemeral for yarn so we don&#x27;t conflict with</span></span><br><span class="line">       <span class="comment">// other spark processes running on the same box</span></span><br><span class="line">       <span class="type">System</span>.setProperty(<span class="type">UI_PORT</span>.key, <span class="string">&quot;0&quot;</span>)</span><br><span class="line"></span><br><span class="line">       <span class="comment">// Set the master and deploy mode property to match the requested mode.</span></span><br><span class="line">       <span class="type">System</span>.setProperty(<span class="string">&quot;spark.master&quot;</span>, <span class="string">&quot;yarn&quot;</span>)</span><br><span class="line">       <span class="type">System</span>.setProperty(<span class="type">SUBMIT_DEPLOY_MODE</span>.key, <span class="string">&quot;cluster&quot;</span>)</span><br><span class="line"></span><br><span class="line">       <span class="comment">// Set this internal configuration if it is running on cluster mode, this</span></span><br><span class="line">       <span class="comment">// configuration will be checked in SparkContext to avoid misuse of yarn cluster mode.</span></span><br><span class="line">       <span class="type">System</span>.setProperty(<span class="string">&quot;spark.yarn.app.id&quot;</span>, appAttemptId.getApplicationId().toString())</span><br><span class="line"></span><br><span class="line">       <span class="type">Option</span>(appAttemptId.getAttemptId.toString)</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       <span class="type">None</span></span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     <span class="keyword">new</span> <span class="type">CallerContext</span>(</span><br><span class="line">       <span class="string">&quot;APPMASTER&quot;</span>, sparkConf.get(<span class="type">APP_CALLER_CONTEXT</span>),</span><br><span class="line">       <span class="type">Option</span>(appAttemptId.getApplicationId.toString), attemptID).setCurrentContext()</span><br><span class="line"></span><br><span class="line">     logInfo(<span class="string">&quot;ApplicationAttemptId: &quot;</span> + appAttemptId)</span><br><span class="line"></span><br><span class="line">     <span class="comment">// This shutdown hook should run *after* the SparkContext is shut down.</span></span><br><span class="line">     <span class="keyword">val</span> priority = <span class="type">ShutdownHookManager</span>.<span class="type">SPARK_CONTEXT_SHUTDOWN_PRIORITY</span> - <span class="number">1</span></span><br><span class="line">     <span class="type">ShutdownHookManager</span>.addShutdownHook(priority) &#123; () =&gt;</span><br><span class="line">       <span class="keyword">val</span> maxAppAttempts = client.getMaxRegAttempts(sparkConf, yarnConf)</span><br><span class="line">       <span class="keyword">val</span> isLastAttempt = appAttemptId.getAttemptId() &gt;= maxAppAttempts</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> (!finished) &#123;</span><br><span class="line">         <span class="comment">// The default state of ApplicationMaster is failed if it is invoked by shut down hook.</span></span><br><span class="line">         <span class="comment">// This behavior is different compared to 1.x version.</span></span><br><span class="line">         <span class="comment">// If user application is exited ahead of time by calling System.exit(N), here mark</span></span><br><span class="line">         <span class="comment">// this application as failed with EXIT_EARLY. For a good shutdown, user shouldn&#x27;t call</span></span><br><span class="line">         <span class="comment">// System.exit(0) to terminate the application.</span></span><br><span class="line">         finish(finalStatus,</span><br><span class="line">           <span class="type">ApplicationMaster</span>.<span class="type">EXIT_EARLY</span>,</span><br><span class="line">           <span class="string">&quot;Shutdown hook called before final status was reported.&quot;</span>)</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> (!unregistered) &#123;</span><br><span class="line">         <span class="comment">// we only want to unregister if we don&#x27;t want the RM to retry</span></span><br><span class="line">         <span class="keyword">if</span> (finalStatus == <span class="type">FinalApplicationStatus</span>.<span class="type">SUCCEEDED</span> || isLastAttempt) &#123;</span><br><span class="line">           unregister(finalStatus, finalMsg)</span><br><span class="line">           cleanupStagingDir(<span class="keyword">new</span> <span class="type">Path</span>(<span class="type">System</span>.getenv(<span class="string">&quot;SPARK_YARN_STAGING_DIR&quot;</span>)))</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line"><span class="comment">// 集群模式下，运行runDriver()</span></span><br><span class="line">     <span class="keyword">if</span> (isClusterMode) &#123;</span><br><span class="line">       runDriver()</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         <span class="comment">// 如果是Client模式运行 runExecutorLauncher()方法</span></span><br><span class="line">       runExecutorLauncher()</span><br><span class="line">     &#125;</span><br><span class="line">   &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">     <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">       <span class="comment">// catch everything else if not specifically handled</span></span><br><span class="line">       logError(<span class="string">&quot;Uncaught exception: &quot;</span>, e)</span><br><span class="line">       finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>,</span><br><span class="line">         <span class="type">ApplicationMaster</span>.<span class="type">EXIT_UNCAUGHT_EXCEPTION</span>,</span><br><span class="line">         <span class="string">&quot;Uncaught exception: &quot;</span> + <span class="type">StringUtils</span>.stringifyException(e))</span><br><span class="line">   &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">     <span class="keyword">try</span> &#123;</span><br><span class="line">       metricsSystem.foreach &#123; ms =&gt;</span><br><span class="line">         ms.report()</span><br><span class="line">         ms.stop()</span><br><span class="line">       &#125;</span><br><span class="line">     &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">       <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">         logWarning(<span class="string">&quot;Exception during stopping of the metric system: &quot;</span>, e)</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   exitCode</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p><code>runDriver</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runDriver</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  addAmIpFilter(<span class="type">None</span>, <span class="type">System</span>.getenv(<span class="type">ApplicationConstants</span>.<span class="type">APPLICATION_WEB_PROXY_BASE_ENV</span>))</span><br><span class="line">  userClassThread = startUserApplication() <span class="comment">// 启动用户应用</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// This a bit hacky, but we need to wait until the spark.driver.port property has</span></span><br><span class="line">  <span class="comment">// been set by the Thread executing the user class.</span></span><br><span class="line">  logInfo(<span class="string">&quot;Waiting for spark context initialization...&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> totalWaitTime = sparkConf.get(<span class="type">AM_MAX_WAIT_TIME</span>)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 等待上限文环境初始化成功后才可以往下运行：SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="type">ThreadUtils</span>.awaitResult(sparkContextPromise.future,</span><br><span class="line">      <span class="type">Duration</span>(totalWaitTime, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>))</span><br><span class="line">    <span class="keyword">if</span> (sc != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> rpcEnv = sc.env.rpcEnv</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> userConf = sc.getConf</span><br><span class="line">      <span class="keyword">val</span> host = userConf.get(<span class="type">DRIVER_HOST_ADDRESS</span>)</span><br><span class="line">      <span class="keyword">val</span> port = userConf.get(<span class="type">DRIVER_PORT</span>)</span><br><span class="line">        <span class="comment">//ApplicationMaster 向ResourceManager进行注册（目的：通信，申请资源）</span></span><br><span class="line">      registerAM(host, port, userConf, sc.ui.map(_.webUrl), appAttemptId)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> driverRef = rpcEnv.setupEndpointRef(</span><br><span class="line">        <span class="type">RpcAddress</span>(host, port),</span><br><span class="line">        <span class="type">YarnSchedulerBackend</span>.<span class="type">ENDPOINT_NAME</span>)</span><br><span class="line">        <span class="comment">// ApplicationMaster向RM注册成功之后，RM会通过client给ApplicationMaster分配可用的资源</span></span><br><span class="line">      createAllocator(driverRef, userConf, rpcEnv, appAttemptId, distCacheConf)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Sanity check; should never happen in normal operation, since sc should only be null</span></span><br><span class="line">      <span class="comment">// if the user app did not create a SparkContext.</span></span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">&quot;User did not initialize spark context!&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    resumeDriver()</span><br><span class="line">    userClassThread.join()</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">SparkException</span> <span class="keyword">if</span> e.getCause().isInstanceOf[<span class="type">TimeoutException</span>] =&gt;</span><br><span class="line">      logError(</span><br><span class="line">        <span class="string">s&quot;SparkContext did not initialize after waiting for <span class="subst">$totalWaitTime</span> ms. &quot;</span> +</span><br><span class="line">         <span class="string">&quot;Please check earlier log output for errors. Failing the application.&quot;</span>)</span><br><span class="line">      finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>,</span><br><span class="line">        <span class="type">ApplicationMaster</span>.<span class="type">EXIT_SC_NOT_INITED</span>,</span><br><span class="line">        <span class="string">&quot;Timed out waiting for SparkContext.&quot;</span>)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    resumeDriver()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>startUserApplication</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startUserApplication</span></span>(): <span class="type">Thread</span> = &#123;</span><br><span class="line">  logInfo(<span class="string">&quot;Starting the user application in a separate Thread&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> userArgs = args.userArgs</span><br><span class="line">  <span class="keyword">if</span> (args.primaryPyFile != <span class="literal">null</span> &amp;&amp; args.primaryPyFile.endsWith(<span class="string">&quot;.py&quot;</span>)) &#123;</span><br><span class="line">    <span class="comment">// When running pyspark, the app is run using PythonRunner. The second argument is the list</span></span><br><span class="line">    <span class="comment">// of files to add to PYTHONPATH, which Client.scala already handles, so it&#x27;s empty.</span></span><br><span class="line">    userArgs = <span class="type">Seq</span>(args.primaryPyFile, <span class="string">&quot;&quot;</span>) ++ userArgs</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (args.primaryRFile != <span class="literal">null</span> &amp;&amp;</span><br><span class="line">      (args.primaryRFile.endsWith(<span class="string">&quot;.R&quot;</span>) || args.primaryRFile.endsWith(<span class="string">&quot;.r&quot;</span>))) &#123;</span><br><span class="line">    <span class="comment">// TODO(davies): add R dependencies here</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用类加载器加载用户class，这个class就是Spark-submit时所指定的具体的class， 获取main方法</span></span><br><span class="line">  <span class="keyword">val</span> mainMethod = userClassLoader.loadClass(args.userClass)</span><br><span class="line">    .getMethod(<span class="string">&quot;main&quot;</span>, classOf[<span class="type">Array</span>[<span class="type">String</span>]])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> userThread = <span class="keyword">new</span> <span class="type">Thread</span> &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (!<span class="type">Modifier</span>.isStatic(mainMethod.getModifiers)) &#123;</span><br><span class="line">          logError(<span class="string">s&quot;Could not find static main method in object <span class="subst">$&#123;args.userClass&#125;</span>&quot;</span>)</span><br><span class="line">          finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>, <span class="type">ApplicationMaster</span>.<span class="type">EXIT_EXCEPTION_USER_CLASS</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// invoke调用用户应用的main方法</span></span><br><span class="line">            <span class="comment">// 用户的应用程序运行时，就会初始化SparkContext</span></span><br><span class="line">          mainMethod.invoke(<span class="literal">null</span>, userArgs.toArray)</span><br><span class="line">          finish(<span class="type">FinalApplicationStatus</span>.<span class="type">SUCCEEDED</span>, <span class="type">ApplicationMaster</span>.<span class="type">EXIT_SUCCESS</span>)</span><br><span class="line">          logDebug(<span class="string">&quot;Done running user class&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">InvocationTargetException</span> =&gt;</span><br><span class="line">          e.getCause <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> _: <span class="type">InterruptedException</span> =&gt;</span><br><span class="line">              <span class="comment">// Reporter thread can interrupt to stop user class</span></span><br><span class="line">            <span class="keyword">case</span> <span class="type">SparkUserAppException</span>(exitCode) =&gt;</span><br><span class="line">              <span class="keyword">val</span> msg = <span class="string">s&quot;User application exited with status <span class="subst">$exitCode</span>&quot;</span></span><br><span class="line">              logError(msg)</span><br><span class="line">              finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>, exitCode, msg)</span><br><span class="line">            <span class="keyword">case</span> cause: <span class="type">Throwable</span> =&gt;</span><br><span class="line">              logError(<span class="string">&quot;User class threw exception: &quot;</span> + cause, cause)</span><br><span class="line">              finish(<span class="type">FinalApplicationStatus</span>.<span class="type">FAILED</span>,</span><br><span class="line">                <span class="type">ApplicationMaster</span>.<span class="type">EXIT_EXCEPTION_USER_CLASS</span>,</span><br><span class="line">                <span class="string">&quot;User class threw exception: &quot;</span> + <span class="type">StringUtils</span>.stringifyException(cause))</span><br><span class="line">          &#125;</span><br><span class="line">          sparkContextPromise.tryFailure(e.getCause())</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="comment">// Notify the thread waiting for the SparkContext, in case the application did not</span></span><br><span class="line">        <span class="comment">// instantiate one. This will do nothing when the user code instantiates a SparkContext</span></span><br><span class="line">        <span class="comment">// (with the correct master), or when the user code throws an exception (due to the</span></span><br><span class="line">        <span class="comment">// tryFailure above).</span></span><br><span class="line">        sparkContextPromise.trySuccess(<span class="literal">null</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  userThread.setContextClassLoader(userClassLoader)</span><br><span class="line">  userThread.setName(<span class="string">&quot;Driver&quot;</span>)  <span class="comment">// 此处说明ApplicationMaster中创建了Driver子线程</span></span><br><span class="line">  userThread.start()</span><br><span class="line">  userThread</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>createAllocator</code>，RM向AM分配资源</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createAllocator</span></span>(</span><br><span class="line">     driverRef: <span class="type">RpcEndpointRef</span>,</span><br><span class="line">     _sparkConf: <span class="type">SparkConf</span>,</span><br><span class="line">     rpcEnv: <span class="type">RpcEnv</span>,</span><br><span class="line">     appAttemptId: <span class="type">ApplicationAttemptId</span>,</span><br><span class="line">     distCacheConf: <span class="type">SparkConf</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="comment">// In client mode, the AM may be restarting after delegation tokens have reached their TTL. So</span></span><br><span class="line">   <span class="comment">// always contact the driver to get the current set of valid tokens, so that local resources can</span></span><br><span class="line">   <span class="comment">// be initialized below.</span></span><br><span class="line">   <span class="keyword">if</span> (!isClusterMode) &#123;</span><br><span class="line">     <span class="keyword">val</span> tokens = driverRef.askSync[<span class="type">Array</span>[<span class="type">Byte</span>]](<span class="type">RetrieveDelegationTokens</span>)</span><br><span class="line">     <span class="keyword">if</span> (tokens != <span class="literal">null</span>) &#123;</span><br><span class="line">       <span class="type">SparkHadoopUtil</span>.get.addDelegationTokens(tokens, _sparkConf)</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> appId = appAttemptId.getApplicationId().toString()</span><br><span class="line">   <span class="keyword">val</span> driverUrl = <span class="type">RpcEndpointAddress</span>(driverRef.address.host, driverRef.address.port,</span><br><span class="line">     <span class="type">CoarseGrainedSchedulerBackend</span>.<span class="type">ENDPOINT_NAME</span>).toString</span><br><span class="line">   <span class="keyword">val</span> localResources = prepareLocalResources(distCacheConf)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Before we initialize the allocator, let&#x27;s log the information about how executors will</span></span><br><span class="line">   <span class="comment">// be run up front, to avoid printing this out for every single executor being launched.</span></span><br><span class="line">   <span class="comment">// Use placeholders for information that changes such as executor IDs.</span></span><br><span class="line">   logInfo &#123;</span><br><span class="line">     <span class="keyword">val</span> executorMemory = _sparkConf.get(<span class="type">EXECUTOR_MEMORY</span>).toInt</span><br><span class="line">     <span class="keyword">val</span> executorCores = _sparkConf.get(<span class="type">EXECUTOR_CORES</span>)</span><br><span class="line">     <span class="keyword">val</span> dummyRunner = <span class="keyword">new</span> <span class="type">ExecutorRunnable</span>(<span class="type">None</span>, yarnConf, _sparkConf, driverUrl, <span class="string">&quot;&lt;executorId&gt;&quot;</span>,</span><br><span class="line">       <span class="string">&quot;&lt;hostname&gt;&quot;</span>, executorMemory, executorCores, appId, securityMgr, localResources,</span><br><span class="line">       <span class="type">ResourceProfile</span>.<span class="type">DEFAULT_RESOURCE_PROFILE_ID</span>)</span><br><span class="line">     dummyRunner.launchContextDebugInfo()</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//通过client，RM向ApplicationMaster分配可用的资源</span></span><br><span class="line">   allocator = client.createAllocator(</span><br><span class="line">     yarnConf,</span><br><span class="line">     _sparkConf,</span><br><span class="line">     appAttemptId,</span><br><span class="line">     driverUrl,</span><br><span class="line">     driverRef,</span><br><span class="line">     securityMgr,</span><br><span class="line">     localResources)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Initialize the AM endpoint *after* the allocator has been initialized. This ensures</span></span><br><span class="line">   <span class="comment">// that when the driver sends an initial executor request (e.g. after an AM restart),</span></span><br><span class="line">   <span class="comment">// the allocator is ready to service requests.</span></span><br><span class="line">   rpcEnv.setupEndpoint(<span class="string">&quot;YarnAM&quot;</span>, <span class="keyword">new</span> <span class="type">AMEndpoint</span>(rpcEnv, driverRef))</span><br><span class="line"></span><br><span class="line">   allocator.allocateResources()</span><br><span class="line">   <span class="keyword">val</span> ms = <span class="type">MetricsSystem</span>.createMetricsSystem(<span class="type">MetricsSystemInstances</span>.<span class="type">APPLICATION_MASTER</span>,</span><br><span class="line">     sparkConf, securityMgr)</span><br><span class="line">   <span class="keyword">val</span> prefix = _sparkConf.get(<span class="type">YARN_METRICS_NAMESPACE</span>).getOrElse(appId)</span><br><span class="line">   ms.registerSource(<span class="keyword">new</span> <span class="type">ApplicationMasterSource</span>(prefix, allocator))</span><br><span class="line">   <span class="comment">// do not register static sources in this case as per SPARK-25277</span></span><br><span class="line">   ms.start(<span class="literal">false</span>)</span><br><span class="line">   metricsSystem = <span class="type">Some</span>(ms)</span><br><span class="line">   reporterThread = launchReporterThread()</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>Container申请完成后启动相应的Container，</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Launches executors in the allocated containers.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runAllocatedContainers</span></span>(containersToUse: <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">for</span> (container &lt;- containersToUse) &#123;</span><br><span class="line">    executorIdCounter += <span class="number">1</span></span><br><span class="line">    <span class="keyword">val</span> executorHostname = container.getNodeId.getHost</span><br><span class="line">    <span class="keyword">val</span> containerId = container.getId</span><br><span class="line">    <span class="keyword">val</span> executorId = executorIdCounter.toString</span><br><span class="line">    assert(container.getResource.getMemory &gt;= resource.getMemory)</span><br><span class="line">    logInfo(<span class="string">s&quot;Launching container <span class="subst">$containerId</span> on host <span class="subst">$executorHostname</span> &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;for executor with ID <span class="subst">$executorId</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">updateInternalState</span></span>(): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">      runningExecutors.add(executorId)</span><br><span class="line">      numExecutorsStarting.decrementAndGet()</span><br><span class="line">      executorIdToContainer(executorId) = container</span><br><span class="line">      containerIdToExecutorId(container.getId) = executorId</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> containerSet = allocatedHostToContainersMap.getOrElseUpdate(executorHostname,</span><br><span class="line">        <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">ContainerId</span>])</span><br><span class="line">      containerSet += containerId</span><br><span class="line">      allocatedContainerToHostMap.put(containerId, executorHostname)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (runningExecutors.size() &lt; targetNumExecutors) &#123;</span><br><span class="line">      numExecutorsStarting.incrementAndGet()</span><br><span class="line">      <span class="keyword">if</span> (launchContainers) &#123;</span><br><span class="line">          <span class="comment">// 通过线程池的方式运行Container</span></span><br><span class="line">        launcherPool.execute(() =&gt; &#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">              <span class="comment">// ExecutorRunnable是运行时具体要干些什么？</span></span><br><span class="line">            <span class="keyword">new</span> <span class="type">ExecutorRunnable</span>(</span><br><span class="line">              <span class="type">Some</span>(container),</span><br><span class="line">              conf,</span><br><span class="line">              sparkConf,</span><br><span class="line">              driverUrl,</span><br><span class="line">              executorId,</span><br><span class="line">              executorHostname,</span><br><span class="line">              executorMemory,</span><br><span class="line">              executorCores,</span><br><span class="line">              appAttemptId.getApplicationId.toString,</span><br><span class="line">              securityMgr,</span><br><span class="line">              localResources,</span><br><span class="line">              <span class="type">ResourceProfile</span>.<span class="type">DEFAULT_RESOURCE_PROFILE_ID</span> <span class="comment">// use until fully supported</span></span><br><span class="line">            ).run()<span class="comment">// run方法启动</span></span><br><span class="line">            updateInternalState()</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">              numExecutorsStarting.decrementAndGet()</span><br><span class="line">              <span class="keyword">if</span> (<span class="type">NonFatal</span>(e)) &#123;</span><br><span class="line">                logError(<span class="string">s&quot;Failed to launch executor <span class="subst">$executorId</span> on container <span class="subst">$containerId</span>&quot;</span>, e)</span><br><span class="line">                <span class="comment">// Assigned container should be released immediately</span></span><br><span class="line">                <span class="comment">// to avoid unnecessary resource occupation.</span></span><br><span class="line">                amClient.releaseAssignedContainer(containerId)</span><br><span class="line">              &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">throw</span> e</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// For test only</span></span><br><span class="line">        updateInternalState()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      logInfo((<span class="string">&quot;Skip launching executorRunnable as running executors count: %d &quot;</span> +</span><br><span class="line">        <span class="string">&quot;reached target executors count: %d.&quot;</span>).format(</span><br><span class="line">        runningExecutors.size, targetNumExecutors))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><code>ExecutorRunnable</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">   logDebug(<span class="string">&quot;Starting Executor Container&quot;</span>)</span><br><span class="line">     <span class="comment">// 创建与NodeManager通信的Client客户端</span></span><br><span class="line">   nmClient = <span class="type">NMClient</span>.createNMClient()</span><br><span class="line">   nmClient.init(conf)</span><br><span class="line">   nmClient.start()</span><br><span class="line">     <span class="comment">//启动容器</span></span><br><span class="line">   startContainer()</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">startContainer</span></span>(): java.util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">ByteBuffer</span>] = &#123;</span><br><span class="line">     <span class="comment">// 获取上下文环境</span></span><br><span class="line">   <span class="keyword">val</span> ctx = <span class="type">Records</span>.newRecord(classOf[<span class="type">ContainerLaunchContext</span>])</span><br><span class="line">     .asInstanceOf[<span class="type">ContainerLaunchContext</span>]</span><br><span class="line">     </span><br><span class="line">   <span class="keyword">val</span> env = prepareEnvironment().asJava</span><br><span class="line"></span><br><span class="line">   ctx.setLocalResources(localResources.asJava)</span><br><span class="line">   ctx.setEnvironment(env)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> credentials = <span class="type">UserGroupInformation</span>.getCurrentUser().getCredentials()</span><br><span class="line">   <span class="keyword">val</span> dob = <span class="keyword">new</span> <span class="type">DataOutputBuffer</span>()</span><br><span class="line">   credentials.writeTokenStorageToStream(dob)</span><br><span class="line">   ctx.setTokens(<span class="type">ByteBuffer</span>.wrap(dob.getData()))</span><br><span class="line"><span class="comment">// 准备命令</span></span><br><span class="line">   <span class="keyword">val</span> commands = prepareCommand()  <span class="comment">// 封装命令，并向相应的Container所在机器上 发送YarnCoarseGrandExecutorBackend</span></span><br><span class="line"></span><br><span class="line">   ctx.setCommands(commands.asJava)</span><br><span class="line">   ctx.setApplicationACLs(</span><br><span class="line">     <span class="type">YarnSparkHadoopUtil</span>.getApplicationAclsForYarn(securityMgr).asJava)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// If external shuffle service is enabled, register with the Yarn shuffle service already</span></span><br><span class="line">   <span class="comment">// started on the NodeManager and, if authentication is enabled, provide it with our secret</span></span><br><span class="line">   <span class="comment">// key for fetching shuffle files later</span></span><br><span class="line">   <span class="keyword">if</span> (sparkConf.get(<span class="type">SHUFFLE_SERVICE_ENABLED</span>)) &#123;</span><br><span class="line">     <span class="keyword">val</span> secretString = securityMgr.getSecretKey()</span><br><span class="line">     <span class="keyword">val</span> secretBytes =</span><br><span class="line">       <span class="keyword">if</span> (secretString != <span class="literal">null</span>) &#123;</span><br><span class="line">         <span class="comment">// This conversion must match how the YarnShuffleService decodes our secret</span></span><br><span class="line">         <span class="type">JavaUtils</span>.stringToBytes(secretString)</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         <span class="comment">// Authentication is not enabled, so just provide dummy metadata</span></span><br><span class="line">         <span class="type">ByteBuffer</span>.allocate(<span class="number">0</span>)</span><br><span class="line">       &#125;</span><br><span class="line">     ctx.setServiceData(<span class="type">Collections</span>.singletonMap(<span class="string">&quot;spark_shuffle&quot;</span>, secretBytes))</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Send the start request to the ContainerManager</span></span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line">     nmClient.startContainer(container.get, ctx)</span><br><span class="line">   &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">     <span class="keyword">case</span> ex: <span class="type">Exception</span> =&gt;</span><br><span class="line">       <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Exception while starting container <span class="subst">$&#123;container.get.getId&#125;</span>&quot;</span> +</span><br><span class="line">         <span class="string">s&quot; on host <span class="subst">$hostname</span>&quot;</span>, ex)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">prepareCommand</span></span>(): <span class="type">List</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">   <span class="comment">// Extra options for the JVM</span></span><br><span class="line">   <span class="keyword">val</span> javaOpts = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Set the JVM memory</span></span><br><span class="line">   <span class="keyword">val</span> executorMemoryString = executorMemory + <span class="string">&quot;m&quot;</span></span><br><span class="line">   javaOpts += <span class="string">&quot;-Xmx&quot;</span> + executorMemoryString</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Set extra Java options for the executor, if defined</span></span><br><span class="line">   sparkConf.get(<span class="type">EXECUTOR_JAVA_OPTIONS</span>).foreach &#123; opts =&gt;</span><br><span class="line">     <span class="keyword">val</span> subsOpt = <span class="type">Utils</span>.substituteAppNExecIds(opts, appId, executorId)</span><br><span class="line">     javaOpts ++= <span class="type">Utils</span>.splitCommandString(subsOpt).map(<span class="type">YarnSparkHadoopUtil</span>.escapeForShell)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Set the library path through a command prefix to append to the existing value of the</span></span><br><span class="line">   <span class="comment">// env variable.</span></span><br><span class="line">   <span class="keyword">val</span> prefixEnv = sparkConf.get(<span class="type">EXECUTOR_LIBRARY_PATH</span>).map &#123; libPath =&gt;</span><br><span class="line">     <span class="type">Client</span>.createLibraryPathPrefix(libPath, sparkConf)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   javaOpts += <span class="string">&quot;-Djava.io.tmpdir=&quot;</span> +</span><br><span class="line">     <span class="keyword">new</span> <span class="type">Path</span>(<span class="type">Environment</span>.<span class="type">PWD</span>.$$(), <span class="type">YarnConfiguration</span>.<span class="type">DEFAULT_CONTAINER_TEMP_DIR</span>)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Certain configs need to be passed here because they are needed before the Executor</span></span><br><span class="line">   <span class="comment">// registers with the Scheduler and transfers the spark configs. Since the Executor backend</span></span><br><span class="line">   <span class="comment">// uses RPC to connect to the scheduler, the RPC settings are needed as well as the</span></span><br><span class="line">   <span class="comment">// authentication settings.</span></span><br><span class="line">   sparkConf.getAll</span><br><span class="line">     .filter &#123; <span class="keyword">case</span> (k, v) =&gt; <span class="type">SparkConf</span>.isExecutorStartupConf(k) &#125;</span><br><span class="line">     .foreach &#123; <span class="keyword">case</span> (k, v) =&gt; javaOpts += <span class="type">YarnSparkHadoopUtil</span>.escapeForShell(<span class="string">s&quot;-D<span class="subst">$k</span>=<span class="subst">$v</span>&quot;</span>) &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Commenting it out for now - so that people can refer to the properties if required. Remove</span></span><br><span class="line">   <span class="comment">// it once cpuset version is pushed out.</span></span><br><span class="line">   <span class="comment">// The context is, default gc for server class machines end up using all cores to do gc - hence</span></span><br><span class="line">   <span class="comment">// if there are multiple containers in same node, spark gc effects all other containers</span></span><br><span class="line">   <span class="comment">// performance (which can also be other spark containers)</span></span><br><span class="line">   <span class="comment">// Instead of using this, rely on cpusets by YARN to enforce spark behaves &#x27;properly&#x27; in</span></span><br><span class="line">   <span class="comment">// multi-tenant environments. Not sure how default java gc behaves if it is limited to subset</span></span><br><span class="line">   <span class="comment">// of cores on a node.</span></span><br><span class="line">   <span class="comment">/*</span></span><br><span class="line"><span class="comment">       else &#123;</span></span><br><span class="line"><span class="comment">         // If no java_opts specified, default to using -XX:+CMSIncrementalMode</span></span><br><span class="line"><span class="comment">         // It might be possible that other modes/config is being done in</span></span><br><span class="line"><span class="comment">         // spark.executor.extraJavaOptions, so we don&#x27;t want to mess with it.</span></span><br><span class="line"><span class="comment">         // In our expts, using (default) throughput collector has severe perf ramifications in</span></span><br><span class="line"><span class="comment">         // multi-tenant machines</span></span><br><span class="line"><span class="comment">         // The options are based on</span></span><br><span class="line"><span class="comment">         // http://www.oracle.com/technetwork/java/gc-tuning-5-138395.html#0.0.0.%20When%20to%20Use</span></span><br><span class="line"><span class="comment">         // %20the%20Concurrent%20Low%20Pause%20Collector|outline</span></span><br><span class="line"><span class="comment">         javaOpts += &quot;-XX:+UseConcMarkSweepGC&quot;</span></span><br><span class="line"><span class="comment">         javaOpts += &quot;-XX:+CMSIncrementalMode&quot;</span></span><br><span class="line"><span class="comment">         javaOpts += &quot;-XX:+CMSIncrementalPacing&quot;</span></span><br><span class="line"><span class="comment">         javaOpts += &quot;-XX:CMSIncrementalDutyCycleMin=0&quot;</span></span><br><span class="line"><span class="comment">         javaOpts += &quot;-XX:CMSIncrementalDutyCycle=10&quot;</span></span><br><span class="line"><span class="comment">       &#125;</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">// For log4j configuration to reference</span></span><br><span class="line">   javaOpts += (<span class="string">&quot;-Dspark.yarn.app.container.log.dir=&quot;</span> + <span class="type">ApplicationConstants</span>.<span class="type">LOG_DIR_EXPANSION_VAR</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> userClassPath = <span class="type">Client</span>.getUserClasspath(sparkConf).flatMap &#123; uri =&gt;</span><br><span class="line">     <span class="keyword">val</span> absPath =</span><br><span class="line">       <span class="keyword">if</span> (<span class="keyword">new</span> <span class="type">File</span>(uri.getPath()).isAbsolute()) &#123;</span><br><span class="line">         <span class="type">Client</span>.getClusterPath(sparkConf, uri.getPath())</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         <span class="type">Client</span>.buildPath(<span class="type">Environment</span>.<span class="type">PWD</span>.$(), uri.getPath())</span><br><span class="line">       &#125;</span><br><span class="line">     <span class="type">Seq</span>(<span class="string">&quot;--user-class-path&quot;</span>, <span class="string">&quot;file:&quot;</span> + absPath)</span><br><span class="line">   &#125;.toSeq</span><br><span class="line"></span><br><span class="line">   <span class="type">YarnSparkHadoopUtil</span>.addOutOfMemoryErrorArgument(javaOpts)</span><br><span class="line">   <span class="keyword">val</span> commands = prefixEnv ++</span><br><span class="line">     <span class="type">Seq</span>(<span class="type">Environment</span>.<span class="type">JAVA_HOME</span>.$$() + <span class="string">&quot;/bin/java&quot;</span>, <span class="string">&quot;-server&quot;</span>) ++</span><br><span class="line">     javaOpts ++</span><br><span class="line">     <span class="type">Seq</span>(<span class="string">&quot;org.apache.spark.executor.YarnCoarseGrainedExecutorBackend&quot;</span>,</span><br><span class="line">       <span class="string">&quot;--driver-url&quot;</span>, masterAddress,</span><br><span class="line">       <span class="string">&quot;--executor-id&quot;</span>, executorId,</span><br><span class="line">       <span class="string">&quot;--hostname&quot;</span>, hostname,</span><br><span class="line">       <span class="string">&quot;--cores&quot;</span>, executorCores.toString,</span><br><span class="line">       <span class="string">&quot;--app-id&quot;</span>, appId,</span><br><span class="line">       <span class="string">&quot;--resourceProfileId&quot;</span>, resourceProfileId.toString) ++</span><br><span class="line">     userClassPath ++</span><br><span class="line">     <span class="type">Seq</span>(</span><br><span class="line">       <span class="string">s&quot;1&gt;<span class="subst">$&#123;ApplicationConstants.LOG_DIR_EXPANSION_VAR&#125;</span>/stdout&quot;</span>,</span><br><span class="line">       <span class="string">s&quot;2&gt;<span class="subst">$&#123;ApplicationConstants.LOG_DIR_EXPANSION_VAR&#125;</span>/stderr&quot;</span>)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// <span class="doctag">TODO:</span> it would be nicer to just make sure there are no null commands here</span></span><br><span class="line">   commands.map(s =&gt; <span class="keyword">if</span> (s == <span class="literal">null</span>) <span class="string">&quot;null&quot;</span> <span class="keyword">else</span> s).toList</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>AM封装指令<code>bin/java org.apache.spark.executor.YarnCoarseGrainedExecutorBackend</code>到Container所在资源中，启动YarnCoarseGrainedExecutorBackend。</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/spark1%E6%BA%90%E7%A0%81-16328806267071.png" alt="spark1源码"></p>
<h3 id="2-组件通信"><a href="#2-组件通信" class="headerlink" title="2. 组件通信"></a>2. 组件通信</h3><blockquote>
<p><strong>BIO：阻塞式IO</strong></p>
<p><strong>NIO：非阻塞式IO</strong></p>
<p><strong>AIO：异步非阻塞式IO</strong></p>
<p>Linux对AIO支持不够好，Windows对AIO支持的好</p>
<p>Linux采用Epoll方式模仿AIO操作</p>
</blockquote>
<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%BB%84%E4%BB%B6%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86.png" style="zoom:80%;" />



<p>Spark基于Netty新的Rpc框架，借鉴了Akka中的设计，基于Actor模型。</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-16328829051882-16328829071034.webp" alt="img"></p>
<blockquote>
<p>每个Endpoint（Client/Master/Worker）有<strong>1个InBox收件箱</strong>和<strong>N个OutBox发件箱</strong>（N&gt;=1，N取决于当前Endpoint与多少其他的endpoint进行通讯，一个与其通讯的其他Endpoint对应一个OutBox），EndPoint接收到的消息被写入InBox，发送出去的消息写入OutBox并发送其他Endpoint的InBox中</p>
</blockquote>
<p>Driver端：<code>class DriverEndpoint extends IsolatedRpcEndpoint</code></p>
<p>Executor端：<code>class CoarseGrainedExecutorBackend extends IsolatedRpcEndpoint</code></p>
<h3 id="3-应用程序的执行"><a href="#3-应用程序的执行" class="headerlink" title="3. 应用程序的执行"></a>3. 应用程序的执行</h3><p>Driver中执行用户上传的作业，其中首要的是初始化SparkContext对象。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Main entry point for Spark functionality. A SparkContext represents the connection to a Spark</span></span><br><span class="line"><span class="comment"> * cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note Only one `SparkContext` should be active per JVM. You must `stop()` the</span></span><br><span class="line"><span class="comment"> *   active `SparkContext` before creating a new one.</span></span><br><span class="line"><span class="comment"> * @param config a Spark Config object describing the application configuration. Any settings in</span></span><br><span class="line"><span class="comment"> *   this config overrides the default configs as well as system properties.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkContext</span>(<span class="params">config: <span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The call site where this SparkContext was constructed.</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> creationSite: <span class="type">CallSite</span> = <span class="type">Utils</span>.getCallSite()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!config.get(<span class="type">EXECUTOR_ALLOW_SPARK_CONTEXT</span>)) &#123;</span><br><span class="line">    <span class="comment">// In order to prevent SparkContext from being created in executors.</span></span><br><span class="line">    <span class="type">SparkContext</span>.assertOnDriver()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// In order to prevent multiple SparkContexts from being active at the same time, mark this</span></span><br><span class="line">  <span class="comment">// context as having started construction.</span></span><br><span class="line">  <span class="comment">// <span class="doctag">NOTE:</span> this must be placed at the beginning of the SparkContext constructor.</span></span><br><span class="line">  <span class="type">SparkContext</span>.markPartiallyConstructed(<span class="keyword">this</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> startTime = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> stopped: <span class="type">AtomicBoolean</span> = <span class="keyword">new</span> <span class="type">AtomicBoolean</span>(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">assertNotStopped</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">      <span class="keyword">val</span> activeContext = <span class="type">SparkContext</span>.activeContext.get()</span><br><span class="line">      <span class="keyword">val</span> activeCreationSite =</span><br><span class="line">        <span class="keyword">if</span> (activeContext == <span class="literal">null</span>) &#123;</span><br><span class="line">          <span class="string">&quot;(No active SparkContext.)&quot;</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          activeContext.creationSite.longForm</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(</span><br><span class="line">        <span class="string">s&quot;&quot;</span><span class="string">&quot;Cannot call methods on a stopped SparkContext.</span></span><br><span class="line"><span class="string">           |This stopped SparkContext was created at:</span></span><br><span class="line"><span class="string">           |</span></span><br><span class="line"><span class="string">           |$&#123;creationSite.longForm&#125;</span></span><br><span class="line"><span class="string">           |</span></span><br><span class="line"><span class="string">           |The currently active SparkContext was created at:</span></span><br><span class="line"><span class="string">           |</span></span><br><span class="line"><span class="string">           |$activeCreationSite</span></span><br><span class="line"><span class="string">         &quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Create a SparkContext that loads settings from system properties (for instance, when</span></span><br><span class="line"><span class="comment">   * launching with ./bin/spark-submit).</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>() = <span class="keyword">this</span>(<span class="keyword">new</span> <span class="type">SparkConf</span>())</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Alternative constructor that allows setting common Spark properties directly</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).</span></span><br><span class="line"><span class="comment">   * @param appName A name for your application, to display on the cluster web UI</span></span><br><span class="line"><span class="comment">   * @param conf a [[org.apache.spark.SparkConf]] object specifying other Spark parameters</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(master: <span class="type">String</span>, appName: <span class="type">String</span>, conf: <span class="type">SparkConf</span>) =</span><br><span class="line">    <span class="keyword">this</span>(<span class="type">SparkContext</span>.updatedConf(conf, master, appName))</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Alternative constructor that allows setting common Spark properties directly</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).</span></span><br><span class="line"><span class="comment">   * @param appName A name for your application, to display on the cluster web UI.</span></span><br><span class="line"><span class="comment">   * @param sparkHome Location where Spark is installed on cluster nodes.</span></span><br><span class="line"><span class="comment">   * @param jars Collection of JARs to send to the cluster. These can be paths on the local file</span></span><br><span class="line"><span class="comment">   *             system or HDFS, HTTP, HTTPS, or FTP URLs.</span></span><br><span class="line"><span class="comment">   * @param environment Environment variables to set on worker nodes.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(</span><br><span class="line">      master: <span class="type">String</span>,</span><br><span class="line">      appName: <span class="type">String</span>,</span><br><span class="line">      sparkHome: <span class="type">String</span> = <span class="literal">null</span>,</span><br><span class="line">      jars: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span>,</span><br><span class="line">      environment: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>()) = &#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="type">SparkContext</span>.updatedConf(<span class="keyword">new</span> <span class="type">SparkConf</span>(), master, appName, sparkHome, jars, environment))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The following constructors are required when Java code accesses SparkContext directly.</span></span><br><span class="line">  <span class="comment">// Please see SI-4278</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Alternative constructor that allows setting common Spark properties directly</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).</span></span><br><span class="line"><span class="comment">   * @param appName A name for your application, to display on the cluster web UI.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(master: <span class="type">String</span>, appName: <span class="type">String</span>) =</span><br><span class="line">    <span class="keyword">this</span>(master, appName, <span class="literal">null</span>, <span class="type">Nil</span>, <span class="type">Map</span>())</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Alternative constructor that allows setting common Spark properties directly</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).</span></span><br><span class="line"><span class="comment">   * @param appName A name for your application, to display on the cluster web UI.</span></span><br><span class="line"><span class="comment">   * @param sparkHome Location where Spark is installed on cluster nodes.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(master: <span class="type">String</span>, appName: <span class="type">String</span>, sparkHome: <span class="type">String</span>) =</span><br><span class="line">    <span class="keyword">this</span>(master, appName, sparkHome, <span class="type">Nil</span>, <span class="type">Map</span>())</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Alternative constructor that allows setting common Spark properties directly</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).</span></span><br><span class="line"><span class="comment">   * @param appName A name for your application, to display on the cluster web UI.</span></span><br><span class="line"><span class="comment">   * @param sparkHome Location where Spark is installed on cluster nodes.</span></span><br><span class="line"><span class="comment">   * @param jars Collection of JARs to send to the cluster. These can be paths on the local file</span></span><br><span class="line"><span class="comment">   *             system or HDFS, HTTP, HTTPS, or FTP URLs.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(master: <span class="type">String</span>, appName: <span class="type">String</span>, sparkHome: <span class="type">String</span>, jars: <span class="type">Seq</span>[<span class="type">String</span>]) =</span><br><span class="line">    <span class="keyword">this</span>(master, appName, sparkHome, jars, <span class="type">Map</span>())</span><br><span class="line"></span><br><span class="line">  <span class="comment">// log out Spark Version in Spark driver log</span></span><br><span class="line">  logInfo(<span class="string">s&quot;Running Spark version <span class="subst">$SPARK_VERSION</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* ------------------------------------------------------------------------------------- *</span></span><br><span class="line"><span class="comment">   | Private variables. These variables keep the internal state of the context, and are    |</span></span><br><span class="line"><span class="comment">   | not accessible by the outside world. They&#x27;re mutable since we want to initialize all  |</span></span><br><span class="line"><span class="comment">   | of them to some neutral value ahead of time, so that calling &quot;stop()&quot; while the       |</span></span><br><span class="line"><span class="comment">   | constructor is still running is safe.                                                 |</span></span><br><span class="line"><span class="comment">   * ------------------------------------------------------------------------------------- */</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _conf: <span class="type">SparkConf</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _eventLogDir: <span class="type">Option</span>[<span class="type">URI</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _eventLogCodec: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _listenerBus: <span class="type">LiveListenerBus</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _env: <span class="type">SparkEnv</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _statusTracker: <span class="type">SparkStatusTracker</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _progressBar: <span class="type">Option</span>[<span class="type">ConsoleProgressBar</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _ui: <span class="type">Option</span>[<span class="type">SparkUI</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _hadoopConfiguration: <span class="type">Configuration</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _executorMemory: <span class="type">Int</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _schedulerBackend: <span class="type">SchedulerBackend</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _taskScheduler: <span class="type">TaskScheduler</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _heartbeatReceiver: <span class="type">RpcEndpointRef</span> = _</span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> _dagScheduler: <span class="type">DAGScheduler</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _applicationId: <span class="type">String</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _applicationAttemptId: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _eventLogger: <span class="type">Option</span>[<span class="type">EventLoggingListener</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _driverLogger: <span class="type">Option</span>[<span class="type">DriverLogger</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _executorAllocationManager: <span class="type">Option</span>[<span class="type">ExecutorAllocationManager</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _cleaner: <span class="type">Option</span>[<span class="type">ContextCleaner</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _listenerBusStarted: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _jars: <span class="type">Seq</span>[<span class="type">String</span>] = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _files: <span class="type">Seq</span>[<span class="type">String</span>] = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _archives: <span class="type">Seq</span>[<span class="type">String</span>] = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _shutdownHookRef: <span class="type">AnyRef</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _statusStore: <span class="type">AppStatusStore</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _heartbeater: <span class="type">Heartbeater</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _resources: immutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">ResourceInformation</span>] = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _shuffleDriverComponents: <span class="type">ShuffleDriverComponents</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _plugins: <span class="type">Option</span>[<span class="type">PluginContainer</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _resourceProfileManager: <span class="type">ResourceProfileManager</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="comment">/* ------------------------------------------------------------------------------------- *</span></span><br><span class="line"><span class="comment">   | Accessors and public fields. These provide access to the internal state of the        |</span></span><br><span class="line"><span class="comment">   | context.                                                                              |</span></span><br><span class="line"><span class="comment">   * ------------------------------------------------------------------------------------- */</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">conf</span></span>: <span class="type">SparkConf</span> = _conf</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return a copy of this SparkContext&#x27;s configuration. The configuration &#x27;&#x27;cannot&#x27;&#x27; be</span></span><br><span class="line"><span class="comment">   * changed at runtime.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getConf</span></span>: <span class="type">SparkConf</span> = conf.clone()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">resources</span></span>: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">ResourceInformation</span>] = _resources</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">jars</span></span>: <span class="type">Seq</span>[<span class="type">String</span>] = _jars</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">files</span></span>: <span class="type">Seq</span>[<span class="type">String</span>] = _files</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">archives</span></span>: <span class="type">Seq</span>[<span class="type">String</span>] = _archives</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">master</span></span>: <span class="type">String</span> = _conf.get(<span class="string">&quot;spark.master&quot;</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deployMode</span></span>: <span class="type">String</span> = _conf.get(<span class="type">SUBMIT_DEPLOY_MODE</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">appName</span></span>: <span class="type">String</span> = _conf.get(<span class="string">&quot;spark.app.name&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">isEventLogEnabled</span></span>: <span class="type">Boolean</span> = _conf.get(<span class="type">EVENT_LOG_ENABLED</span>)</span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">eventLogDir</span></span>: <span class="type">Option</span>[<span class="type">URI</span>] = _eventLogDir</span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">eventLogCodec</span></span>: <span class="type">Option</span>[<span class="type">String</span>] = _eventLogCodec</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isLocal</span></span>: <span class="type">Boolean</span> = <span class="type">Utils</span>.isLocalMaster(_conf)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * @return true if context is stopped or in the midst of stopping.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isStopped</span></span>: <span class="type">Boolean</span> = stopped.get()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">statusStore</span></span>: <span class="type">AppStatusStore</span> = _statusStore</span><br><span class="line"></span><br><span class="line">  <span class="comment">// An asynchronous listener bus for Spark events</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">listenerBus</span></span>: <span class="type">LiveListenerBus</span> = _listenerBus</span><br><span class="line"></span><br><span class="line">  <span class="comment">// This function allows components created by SparkEnv to be mocked in unit tests:</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">createSparkEnv</span></span>(</span><br><span class="line">      conf: <span class="type">SparkConf</span>,</span><br><span class="line">      isLocal: <span class="type">Boolean</span>,</span><br><span class="line">      listenerBus: <span class="type">LiveListenerBus</span>): <span class="type">SparkEnv</span> = &#123;</span><br><span class="line">    <span class="type">SparkEnv</span>.createDriverEnv(conf, isLocal, listenerBus, <span class="type">SparkContext</span>.numDriverCores(master, conf))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<ul>
<li><strong>SparkContext</strong>中的几个重要的字段属性<ul>
<li> <strong>SparkConf</strong>：配置对象</li>
<li>基础的环境配置</li>
<li><strong>SparkEnv</strong>：环境对象<ul>
<li>通信环境</li>
</ul>
</li>
<li><strong>SchedulerBackend</strong>：通信后台<ul>
<li>主要用于和Executor之间进行通信</li>
</ul>
</li>
<li><strong>TaskScheduler</strong>：任务调度器<ul>
<li>主要用于任务的调度</li>
</ul>
</li>
<li><strong>DAGScheduler</strong>：节点调度器<ul>
<li>主要用于阶段的划分以及任务的切分</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="1-RDD依赖"><a href="#1-RDD依赖" class="headerlink" title="1. RDD依赖"></a>1. RDD依赖</h4><p>源码显示，RDD的依赖父类是<code>Dependency</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">_rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">Dependency</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Get the parent partitions for a child partition.</span></span><br><span class="line"><span class="comment">   * @param partitionId a partition of the child RDD</span></span><br><span class="line"><span class="comment">   * @return the partitions of the parent RDD that the child partition depends upon</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">Seq</span>[<span class="type">Int</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">T</span>] = _rdd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其RDD的依赖关系可分为两种：<code>窄依赖：NarrowDependency</code>和<code>宽依赖：ShuffleDependency</code></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20211002093226904-16331383487631.png" alt="image-20211002093226904"></p>
<h5 id="窄依赖NarrowDependency"><a href="#窄依赖NarrowDependency" class="headerlink" title="窄依赖NarrowDependency"></a>窄依赖NarrowDependency</h5><blockquote>
<p>父RDD的一个或者多个分区只被子RDD的一个分区所使用，不涉及到shuffle</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 窄依赖是一个抽象类，其具体的实现子类包括两种</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">_rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">Dependency</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Get the parent partitions for a child partition.</span></span><br><span class="line"><span class="comment">   * @param partitionId a partition of the child RDD</span></span><br><span class="line"><span class="comment">   * @return the partitions of the parent RDD that the child partition depends upon</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">Seq</span>[<span class="type">Int</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">T</span>] = _rdd</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 一种是OneToOneDependency</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OneToOneDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">rdd</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(partitionId)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RangeDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>], inStart: <span class="type">Int</span>, outStart: <span class="type">Int</span>, length: <span class="type">Int</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">NarrowDependency</span>[<span class="type">T</span>](rdd) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123;</span><br><span class="line">      <span class="type">List</span>(partitionId - outStart + inStart)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">Nil</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>对应于两种窄依赖，如下图所示：<strong>OneToOneDependency</strong></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AA%84%E4%BE%9D%E8%B5%961.png" alt="窄依赖"></p>
<p><strong>RangeDependency</strong></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%AA%84%E4%BE%9D%E8%B5%962.png" alt="窄依赖2"></p>
<h5 id="宽依赖ShuffleDependency"><a href="#宽依赖ShuffleDependency" class="headerlink" title="宽依赖ShuffleDependency"></a>宽依赖ShuffleDependency</h5><blockquote>
<p>父RDD的一个分区被子RDD的多个分区使用，涉及到shuffle</p>
</blockquote>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/834652-20170505114231242-1674540562.png" alt="img"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment"> * Represents a dependency on the output of a shuffle stage. Note that in the case of shuffle,</span></span><br><span class="line"><span class="comment"> * the RDD is transient since we don&#x27;t need it on the executor side.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param _rdd the parent RDD</span></span><br><span class="line"><span class="comment"> * @param partitioner partitioner used to partition the shuffle output</span></span><br><span class="line"><span class="comment"> * @param serializer [[org.apache.spark.serializer.Serializer Serializer]] to use. If not set</span></span><br><span class="line"><span class="comment"> *                   explicitly then the default serializer, as specified by `spark.serializer`</span></span><br><span class="line"><span class="comment"> *                   config option, will be used.</span></span><br><span class="line"><span class="comment"> * @param keyOrdering key ordering for RDD&#x27;s shuffles</span></span><br><span class="line"><span class="comment"> * @param aggregator map/reduce-side aggregator for RDD&#x27;s shuffle</span></span><br><span class="line"><span class="comment"> * @param mapSideCombine whether to perform partial aggregation (also known as map-side combine)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffleDependency</span>[<span class="type">K</span>: <span class="type">ClassTag</span>, <span class="type">V</span>: <span class="type">ClassTag</span>, <span class="type">C</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    @transient private val _rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span></span></span><br><span class="line"><span class="params"><span class="class">    val partitioner: <span class="type">Partitioner</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val serializer: <span class="type">Serializer</span> = <span class="type">SparkEnv</span>.get.serializer,</span></span></span><br><span class="line"><span class="params"><span class="class">    val keyOrdering: <span class="type">Option</span>[<span class="type">Ordering</span>[<span class="type">K</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val aggregator: <span class="type">Option</span>[<span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val mapSideCombine: <span class="type">Boolean</span> = false</span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Dependency</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]] &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]] = _rdd.asInstanceOf[<span class="type">RDD</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> keyClassName: <span class="type">String</span> = reflect.classTag[<span class="type">K</span>].runtimeClass.getName</span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> valueClassName: <span class="type">String</span> = reflect.classTag[<span class="type">V</span>].runtimeClass.getName</span><br><span class="line">  <span class="comment">// Note: It&#x27;s possible that the combiner class tag is null, if the combineByKey</span></span><br><span class="line">  <span class="comment">// methods in PairRDDFunctions are used instead of combineByKeyWithClassTag.</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> combinerClassName: <span class="type">Option</span>[<span class="type">String</span>] =</span><br><span class="line">    <span class="type">Option</span>(reflect.classTag[<span class="type">C</span>]).map(_.runtimeClass.getName)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> shuffleId: <span class="type">Int</span> = _rdd.context.newShuffleId()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> shuffleHandle: <span class="type">ShuffleHandle</span> = _rdd.context.env.shuffleManager.registerShuffle(</span><br><span class="line">    shuffleId, _rdd.partitions.length, <span class="keyword">this</span>)</span><br><span class="line"></span><br><span class="line">  _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(<span class="keyword">this</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h5 id="小总结"><a href="#小总结" class="headerlink" title="小总结"></a>小总结</h5><p>在Spark中，使用<strong>转换算子</strong>对RDD进行操作时，会形成RDD的血缘依赖关系；这个依赖关系即为<code>Dependency</code>，分为<code>NarrowDependency</code>和<code>ShuffleDependency</code>两种。这个阶段并不会触发算子的实际执行。RDD中的阶段即根据RDD的血缘关系依赖类型（<code>ShuflleDependency</code>）进行Stage的划分的。</p>
<h4 id="2-阶段的划分"><a href="#2-阶段的划分" class="headerlink" title="2. 阶段的划分"></a>2. 阶段的划分</h4><p>入口是编写的Spark任务中的行动算子，此处是<code>collect()</code>算子</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> transformation.singleValue</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">mapRDD</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">		<span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;mapRDD&quot;</span>)</span><br><span class="line">		<span class="keyword">val</span> sc: <span class="type">SparkContext</span> = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">		<span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;0&quot;</span>)</span><br><span class="line">		<span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="number">4</span>)</span><br><span class="line">		<span class="comment">//TODO map</span></span><br><span class="line">		println(<span class="string">&quot;=====map算子======&quot;</span>)</span><br><span class="line">		rdd1.map(&#123;</span><br><span class="line">			<span class="keyword">case</span> i: <span class="type">Int</span> =&gt;</span><br><span class="line">				println(<span class="string">&quot;执行一次map操作&quot;</span>)</span><br><span class="line">				i -&gt; <span class="number">1</span></span><br><span class="line">		&#125;).collect()</span><br><span class="line">		<span class="comment">//TODO mapPartitions</span></span><br><span class="line">		println(<span class="string">&quot;=====mapPartitions算子======&quot;</span>)</span><br><span class="line">		rdd1.mapPartitions(x =&gt; &#123;</span><br><span class="line">			println(<span class="string">&quot;执行一次map操作&quot;</span>)</span><br><span class="line">			x.map(_ -&gt; <span class="number">1</span>)</span><br><span class="line">		&#125;).collect()</span><br><span class="line"></span><br><span class="line">		<span class="comment">//TODO</span></span><br><span class="line">		rdd1.mapPartitionsWithIndex((index, data) =&gt; &#123;</span><br><span class="line">			data.map((index, _))</span><br><span class="line">		&#125;).collect().foreach(print)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>collect()</code>算子会触发任务的执行</p>
<p><strong>RDD.scala</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//========================= RDD.scala ===============================</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return an array that contains all of the elements in this RDD.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @note This method should only be used if the resulting array is expected to be small, as</span></span><br><span class="line"><span class="comment">   * all the data is loaded into the driver&#x27;s memory.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> results = sc.runJob(<span class="keyword">this</span>, (iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; iter.toArray)<span class="comment">// runJob执行作业</span></span><br><span class="line">    <span class="type">Array</span>.concat(results: _*)</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//========================= SparkContext.scala ===============================</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Run a job on all partitions in an RDD and return the results in an array.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">    runJob(rdd, func, <span class="number">0</span> until rdd.partitions.length)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Run a job on a given set of partitions of an RDD, but take a function of type</span></span><br><span class="line"><span class="comment">   * `Iterator[T] =&gt; U` instead of `(TaskContext, Iterator[T]) =&gt; U`.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>,</span><br><span class="line">      partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanedFunc = clean(func)</span><br><span class="line">    runJob(rdd, (ctx: <span class="type">TaskContext</span>, it: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedFunc(it), partitions)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Run a function on a given set of partitions in an RDD and return the results as an array.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">      partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> results = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">U</span>](partitions.size)</span><br><span class="line">    runJob[<span class="type">T</span>, <span class="type">U</span>](rdd, func, partitions, (index, res) =&gt; results(index) = res)</span><br><span class="line">    results</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Run a function on a given set of partitions in an RDD and pass the results to the given</span></span><br><span class="line"><span class="comment">   * handler function. This is the main entry point for all actions in Spark.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">&quot;SparkContext has been shutdown&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> callSite = getCallSite</span><br><span class="line">    <span class="keyword">val</span> cleanedFunc = clean(func)</span><br><span class="line">    logInfo(<span class="string">&quot;Starting job: &quot;</span> + callSite.shortForm)</span><br><span class="line">    <span class="keyword">if</span> (conf.getBoolean(<span class="string">&quot;spark.logLineage&quot;</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">      logInfo(<span class="string">&quot;RDD&#x27;s recursive dependencies:\n&quot;</span> + rdd.toDebugString)</span><br><span class="line">    &#125;</span><br><span class="line">    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) <span class="comment">// 最终通过DagScheduler执行Job</span></span><br><span class="line">    progressBar.foreach(_.finishAll())</span><br><span class="line">    rdd.doCheckpoint()</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//========================= DAGScheduler.scala ===============================</span></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="class"><span class="keyword">class</span> <span class="title">DAGSchedulerEventProcessLoop</span>(<span class="params">dagScheduler: <span class="type">DAGScheduler</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">EventLoop</span>[<span class="type">DAGSchedulerEvent</span>](<span class="string">&quot;dag-scheduler-event-loop&quot;</span>) <span class="keyword">with</span> <span class="type">Logging</span>&#123;...&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="keyword">val</span> eventProcessLoop = <span class="keyword">new</span> <span class="type">DAGSchedulerEventProcessLoop</span>(<span class="keyword">this</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Run an action job on the given RDD and pass all the results to the resultHandler function as</span></span><br><span class="line"><span class="comment">   * they arrive.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param rdd target RDD to run tasks on</span></span><br><span class="line"><span class="comment">   * @param func a function to run on each partition of the RDD</span></span><br><span class="line"><span class="comment">   * @param partitions set of partitions to run on; some jobs may not want to compute on all</span></span><br><span class="line"><span class="comment">   *   partitions of the target RDD, e.g. for operations like first()</span></span><br><span class="line"><span class="comment">   * @param callSite where in the user program this job was called</span></span><br><span class="line"><span class="comment">   * @param resultHandler callback to pass each result to</span></span><br><span class="line"><span class="comment">   * @param properties scheduler properties to attach to this job, e.g. fair scheduler pool name</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @throws Exception when the job fails</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">      callSite: <span class="type">CallSite</span>,</span><br><span class="line">      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">      properties: <span class="type">Properties</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> start = <span class="type">System</span>.nanoTime</span><br><span class="line">    <span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)  <span class="comment">// 提交任务</span></span><br><span class="line">    <span class="comment">// Note: Do not call Await.ready(future) because that calls `scala.concurrent.blocking`,</span></span><br><span class="line">    <span class="comment">// which causes concurrent SQL executions to fail if a fork-join pool is used. Note that</span></span><br><span class="line">    <span class="comment">// due to idiosyncrasies in Scala, `awaitPermission` is not actually used anywhere so it&#x27;s</span></span><br><span class="line">    <span class="comment">// safe to pass in null here. For more detail, see SPARK-13747.</span></span><br><span class="line">    <span class="keyword">val</span> awaitPermission = <span class="literal">null</span>.asInstanceOf[scala.concurrent.<span class="type">CanAwait</span>]</span><br><span class="line">    waiter.completionFuture.ready(<span class="type">Duration</span>.<span class="type">Inf</span>)(awaitPermission)</span><br><span class="line">    waiter.completionFuture.value.get <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> scala.util.<span class="type">Success</span>(_) =&gt;</span><br><span class="line">        logInfo(<span class="string">&quot;Job %d finished: %s, took %f s&quot;</span>.format</span><br><span class="line">          (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))</span><br><span class="line">      <span class="keyword">case</span> scala.util.<span class="type">Failure</span>(exception) =&gt;</span><br><span class="line">        logInfo(<span class="string">&quot;Job %d failed: %s, took %f s&quot;</span>.format</span><br><span class="line">          (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))</span><br><span class="line">        <span class="comment">// SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler.</span></span><br><span class="line">        <span class="keyword">val</span> callerStackTrace = <span class="type">Thread</span>.currentThread().getStackTrace.tail</span><br><span class="line">        exception.setStackTrace(exception.getStackTrace ++ callerStackTrace)</span><br><span class="line">        <span class="keyword">throw</span> exception</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Submit an action job to the scheduler.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param rdd target RDD to run tasks on</span></span><br><span class="line"><span class="comment">   * @param func a function to run on each partition of the RDD</span></span><br><span class="line"><span class="comment">   * @param partitions set of partitions to run on; some jobs may not want to compute on all</span></span><br><span class="line"><span class="comment">   *   partitions of the target RDD, e.g. for operations like first()</span></span><br><span class="line"><span class="comment">   * @param callSite where in the user program this job was called</span></span><br><span class="line"><span class="comment">   * @param resultHandler callback to pass each result to</span></span><br><span class="line"><span class="comment">   * @param properties scheduler properties to attach to this job, e.g. fair scheduler pool name</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @return a JobWaiter object that can be used to block until the job finishes executing</span></span><br><span class="line"><span class="comment">   *         or can be used to cancel the job.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @throws IllegalArgumentException when partitions ids are illegal</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span><br><span class="line">      rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">      partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">      callSite: <span class="type">CallSite</span>,</span><br><span class="line">      resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">      properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">    <span class="comment">// Check to make sure we are not launching a task on a partition that does not exist.</span></span><br><span class="line">    <span class="keyword">val</span> maxPartitions = rdd.partitions.length</span><br><span class="line">    partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; <span class="number">0</span>).foreach &#123; p =&gt;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(</span><br><span class="line">        <span class="string">&quot;Attempting to access a non-existent partition: &quot;</span> + p + <span class="string">&quot;. &quot;</span> +</span><br><span class="line">          <span class="string">&quot;Total number of partitions: &quot;</span> + maxPartitions)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> jobId = nextJobId.getAndIncrement()</span><br><span class="line">    <span class="keyword">if</span> (partitions.size == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// Return immediately if the job is running 0 tasks</span></span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, <span class="number">0</span>, resultHandler)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    assert(partitions.size &gt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> func2 = func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]</span><br><span class="line">    <span class="keyword">val</span> waiter = <span class="keyword">new</span> <span class="type">JobWaiter</span>(<span class="keyword">this</span>, jobId, partitions.size, resultHandler)</span><br><span class="line">      <span class="comment">// 核心代码，JobSubmitted会创建一个DAGSchedulerEvent事件，</span></span><br><span class="line">      <span class="comment">// DAGSchedulerEventProcessLoop会通过post方法将上述创建的DAGSchedulerEvent添加到EventQueue事件队列中，等待后续事件的执行</span></span><br><span class="line">    eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span><br><span class="line">      jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">      <span class="type">SerializationUtils</span>.clone(properties)))</span><br><span class="line">    waiter</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>DAGSchedulerEvent.scala</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** A result-yielding job was submitted on a target RDD */</span></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JobSubmitted</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    jobId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    finalRDD: <span class="type">RDD</span>[_],</span></span></span><br><span class="line"><span class="params"><span class="class">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]</span>) <span class="title">=&gt;</span> <span class="title">_</span>,</span></span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    listener: <span class="type">JobListener</span>,</span><br><span class="line">    properties: <span class="type">Properties</span> = <span class="literal">null</span>)</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">DAGSchedulerEvent</span></span><br></pre></td></tr></table></figure>

<p><strong>EventLoop.scala</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// 事件队列器其实就是一个由链表结构组成的双向阻塞队列</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> eventQueue: <span class="type">BlockingQueue</span>[<span class="type">E</span>] = <span class="keyword">new</span> <span class="type">LinkedBlockingDeque</span>[<span class="type">E</span>]()</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">一个事件循环，用于从调用者接收事件并处理事件线程中的所有事件。 它将启动一个独占事件线程来处理所有事件</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Put the event into the event queue. The event thread will process it later.</span></span><br><span class="line"><span class="comment">   将事件添加到事件队列中，event线程会之后处理它</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">post</span></span>(event: <span class="type">E</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    eventQueue.put(event)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 如何取事件？通过一个子线程eventThread</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> stopped = <span class="keyword">new</span> <span class="type">AtomicBoolean</span>(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> eventThread = <span class="keyword">new</span> <span class="type">Thread</span>(name) &#123;</span><br><span class="line">    setDaemon(<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (!stopped.get) &#123;</span><br><span class="line">          <span class="keyword">val</span> event = eventQueue.take() <span class="comment">// 通过take方法取出事件</span></span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            onReceive(event) <span class="comment">// 核心代码</span></span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                onError(e)</span><br><span class="line">              &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(<span class="string">&quot;Unexpected error in &quot;</span> + name, e)</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="comment">// exit even if eventQueue is not empty</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(<span class="string">&quot;Unexpected error in &quot;</span> + name, e)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><strong>DAGScheduler.scala</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> timerContext = timer.time()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      doOnReceive(event)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      timerContext.stop()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123; <span class="comment">// 模式匹配，匹配出相应类型的Event</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class="line">      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) <span class="comment">// 通过dagScheduler.handleJobSubmitted方法处理提交的事件</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">MapStageSubmitted</span>(jobId, dependency, callSite, listener, properties) =&gt;</span><br><span class="line">      dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">StageCancelled</span>(stageId) =&gt;</span><br><span class="line">      dagScheduler.handleStageCancellation(stageId)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobCancelled</span>(jobId) =&gt;</span><br><span class="line">      dagScheduler.handleJobCancellation(jobId)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobGroupCancelled</span>(groupId) =&gt;</span><br><span class="line">      dagScheduler.handleJobGroupCancelled(groupId)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">AllJobsCancelled</span> =&gt;</span><br><span class="line">      dagScheduler.doCancelAllJobs()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">ExecutorAdded</span>(execId, host) =&gt;</span><br><span class="line">      dagScheduler.handleExecutorAdded(execId, host)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">ExecutorLost</span>(execId, reason) =&gt;</span><br><span class="line">      <span class="keyword">val</span> filesLost = reason <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">SlaveLost</span>(_, <span class="literal">true</span>) =&gt; <span class="literal">true</span></span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="literal">false</span></span><br><span class="line">      &#125;</span><br><span class="line">      dagScheduler.handleExecutorLost(execId, filesLost)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">BeginEvent</span>(task, taskInfo) =&gt;</span><br><span class="line">      dagScheduler.handleBeginEvent(task, taskInfo)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">GettingResultEvent</span>(taskInfo) =&gt;</span><br><span class="line">      dagScheduler.handleGetTaskResult(taskInfo)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> completion: <span class="type">CompletionEvent</span> =&gt;</span><br><span class="line">      dagScheduler.handleTaskCompletion(completion)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">TaskSetFailed</span>(taskSet, reason, exception) =&gt;</span><br><span class="line">      dagScheduler.handleTaskSetFailed(taskSet, reason, exception)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">ResubmitFailedStages</span> =&gt;</span><br><span class="line">      dagScheduler.resubmitFailedStages()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*================================== Stage的划分 =================================== */</span></span><br><span class="line"> <span class="comment">// 此方法会涉及到阶段Stage的划分</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</span><br><span class="line">      finalRDD: <span class="type">RDD</span>[_],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">      partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">      callSite: <span class="type">CallSite</span>,</span><br><span class="line">      listener: <span class="type">JobListener</span>,</span><br><span class="line">      properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">    <span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span>  <span class="comment">// FinalStage，也叫做ResultStage</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span></span><br><span class="line">      <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span></span><br><span class="line">      finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite) <span class="comment">// 创建ResultStage</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">        logWarning(<span class="string">&quot;Creating new stage failed due to exception - job: &quot;</span> + jobId, e)</span><br><span class="line">        listener.jobFailed(e)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, callSite, listener, properties)</span><br><span class="line">    clearCacheLocs()</span><br><span class="line">    logInfo(<span class="string">&quot;Got job %s (%s) with %d output partitions&quot;</span>.format(</span><br><span class="line">      job.jobId, callSite.shortForm, partitions.length))</span><br><span class="line">    logInfo(<span class="string">&quot;Final stage: &quot;</span> + finalStage + <span class="string">&quot; (&quot;</span> + finalStage.name + <span class="string">&quot;)&quot;</span>)</span><br><span class="line">    logInfo(<span class="string">&quot;Parents of final stage: &quot;</span> + finalStage.parents)</span><br><span class="line">    logInfo(<span class="string">&quot;Missing parents: &quot;</span> + getMissingParentStages(finalStage))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> jobSubmissionTime = clock.getTimeMillis()</span><br><span class="line">    jobIdToActiveJob(jobId) = job</span><br><span class="line">    activeJobs += job</span><br><span class="line">    finalStage.setActiveJob(job)</span><br><span class="line">    <span class="keyword">val</span> stageIds = jobIdToStageIds(jobId).toArray</span><br><span class="line">    <span class="keyword">val</span> stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))</span><br><span class="line">    listenerBus.post(</span><br><span class="line">      <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))</span><br><span class="line">    submitStage(finalStage)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Create a ResultStage associated with the provided jobId.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createResultStage</span></span>(</span><br><span class="line">      rdd: <span class="type">RDD</span>[_],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">      partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">      jobId: <span class="type">Int</span>,</span><br><span class="line">      callSite: <span class="type">CallSite</span>): <span class="type">ResultStage</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> parents = getOrCreateParentStages(rdd, jobId) <span class="comment">// 获取或创建上一阶段，说白了就是父阶段，在此阶段创建完成之后要保证父阶段处理完成</span></span><br><span class="line">    <span class="keyword">val</span> id = nextStageId.getAndIncrement()</span><br><span class="line">    <span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ResultStage</span>(id, rdd, func, partitions, parents, jobId, callSite)  <span class="comment">// 真正创建ResultStage的位置</span></span><br><span class="line">    stageIdToStage(id) = stage</span><br><span class="line">    updateJobIdStageIdMaps(jobId, stage)</span><br><span class="line">    stage</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Get or create the list of parent stages for a given RDD.  The new Stages will be created with</span></span><br><span class="line"><span class="comment">   * the provided firstJobId.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getOrCreateParentStages</span></span>(rdd: <span class="type">RDD</span>[_], firstJobId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span><br><span class="line">    getShuffleDependencies(rdd).map &#123; shuffleDep =&gt;</span><br><span class="line">      getOrCreateShuffleMapStage(shuffleDep, firstJobId)</span><br><span class="line">    &#125;.toList</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Returns shuffle dependencies that are immediate parents of the given RDD.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * This function will not return more distant ancestors.  For example, if C has a shuffle</span></span><br><span class="line"><span class="comment">   * dependency on B which has a shuffle dependency on A:</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * A &lt;-- B &lt;-- C</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * calling this function with rdd C will only return the B &lt;-- C dependency.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * This function is scheduler-visible for the purpose of unit testing.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">getShuffleDependencies</span></span>(</span><br><span class="line">      rdd: <span class="type">RDD</span>[_]): <span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> parents = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]]</span><br><span class="line">    <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">    <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">Stack</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">    waitingForVisit.push(rdd)</span><br><span class="line">    <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;</span><br><span class="line">      <span class="keyword">val</span> toVisit = waitingForVisit.pop()</span><br><span class="line">      <span class="keyword">if</span> (!visited(toVisit)) &#123;</span><br><span class="line">        visited += toVisit</span><br><span class="line">        toVisit.dependencies.foreach &#123;</span><br><span class="line">          <span class="keyword">case</span> shuffleDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</span><br><span class="line">            parents += shuffleDep</span><br><span class="line">          <span class="keyword">case</span> dependency =&gt;</span><br><span class="line">            waitingForVisit.push(dependency.rdd)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    parents</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Gets a shuffle map stage if one exists in shuffleIdToMapStage. Otherwise, if the</span></span><br><span class="line"><span class="comment">   * shuffle map stage doesn&#x27;t already exist, this method will create the shuffle map stage in</span></span><br><span class="line"><span class="comment">   * addition to any missing ancestor shuffle map stages.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getOrCreateShuffleMapStage</span></span>(</span><br><span class="line">      shuffleDep: <span class="type">ShuffleDependency</span>[_, _, _],</span><br><span class="line">      firstJobId: <span class="type">Int</span>): <span class="type">ShuffleMapStage</span> = &#123;</span><br><span class="line">    shuffleIdToMapStage.get(shuffleDep.shuffleId) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(stage) =&gt;</span><br><span class="line">        stage</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        <span class="comment">// Create stages for all missing ancestor shuffle dependencies.</span></span><br><span class="line">        getMissingAncestorShuffleDependencies(shuffleDep.rdd).foreach &#123; dep =&gt;</span><br><span class="line">          <span class="comment">// Even though getMissingAncestorShuffleDependencies only returns shuffle dependencies</span></span><br><span class="line">          <span class="comment">// that were not already in shuffleIdToMapStage, it&#x27;s possible that by the time we</span></span><br><span class="line">          <span class="comment">// get to a particular dependency in the foreach loop, it&#x27;s been added to</span></span><br><span class="line">          <span class="comment">// shuffleIdToMapStage by the stage creation process for an earlier dependency. See</span></span><br><span class="line">          <span class="comment">// SPARK-13902 for more information.</span></span><br><span class="line">          <span class="keyword">if</span> (!shuffleIdToMapStage.contains(dep.shuffleId)) &#123;</span><br><span class="line">            createShuffleMapStage(dep, firstJobId)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Finally, create a stage for the given shuffle dependency.</span></span><br><span class="line">        createShuffleMapStage(shuffleDep, firstJobId)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Creates a ShuffleMapStage that generates the given shuffle dependency&#x27;s partitions. If a</span></span><br><span class="line"><span class="comment">   * previously run stage generated the same shuffle data, this function will copy the output</span></span><br><span class="line"><span class="comment">   * locations that are still available from the previous shuffle to avoid unnecessarily</span></span><br><span class="line"><span class="comment">   * regenerating data.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createShuffleMapStage</span></span>(shuffleDep: <span class="type">ShuffleDependency</span>[_, _, _], jobId: <span class="type">Int</span>): <span class="type">ShuffleMapStage</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> rdd = shuffleDep.rdd</span><br><span class="line">    <span class="keyword">val</span> numTasks = rdd.partitions.length</span><br><span class="line">    <span class="keyword">val</span> parents = getOrCreateParentStages(rdd, jobId) <span class="comment">// 判断此依赖的RDD是否也存在父阶段，如果有就创建ShuffleMapStage</span></span><br><span class="line">    <span class="keyword">val</span> id = nextStageId.getAndIncrement()</span><br><span class="line">    <span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ShuffleMapStage</span>(id, rdd, numTasks, parents, jobId, rdd.creationSite, shuffleDep)</span><br><span class="line"></span><br><span class="line">    stageIdToStage(id) = stage</span><br><span class="line">    shuffleIdToMapStage(shuffleDep.shuffleId) = stage</span><br><span class="line">    updateJobIdStageIdMaps(jobId, stage)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (mapOutputTracker.containsShuffle(shuffleDep.shuffleId)) &#123;</span><br><span class="line">      <span class="comment">// A previously run stage generated partitions for this shuffle, so for each output</span></span><br><span class="line">      <span class="comment">// that&#x27;s still available, copy information about that output location to the new stage</span></span><br><span class="line">      <span class="comment">// (so we don&#x27;t unnecessarily re-compute that data).</span></span><br><span class="line">      <span class="keyword">val</span> serLocs = mapOutputTracker.getSerializedMapOutputStatuses(shuffleDep.shuffleId)</span><br><span class="line">      <span class="keyword">val</span> locs = <span class="type">MapOutputTracker</span>.deserializeMapStatuses(serLocs)</span><br><span class="line">      (<span class="number">0</span> until locs.length).foreach &#123; i =&gt;</span><br><span class="line">        <span class="keyword">if</span> (locs(i) ne <span class="literal">null</span>) &#123;</span><br><span class="line">          <span class="comment">// locs(i) will be null if missing</span></span><br><span class="line">          stage.addOutputLoc(i, locs(i))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Kind of ugly: need to register RDDs with the cache and map output tracker here</span></span><br><span class="line">      <span class="comment">// since we can&#x27;t do it in the RDD constructor because # of partitions is unknown</span></span><br><span class="line">      logInfo(<span class="string">&quot;Registering RDD &quot;</span> + rdd.id + <span class="string">&quot; (&quot;</span> + rdd.getCreationSite + <span class="string">&quot;)&quot;</span>)</span><br><span class="line">      mapOutputTracker.registerShuffle(shuffleDep.shuffleId, rdd.partitions.length)</span><br><span class="line">    &#125;</span><br><span class="line">    stage</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><strong><font color=red>Spark中阶段的划分数（Stage数量）== ShuffleDependency（Shuffle依赖）的数量+1</font></strong></p>
<h5 id="小总结-1"><a href="#小总结-1" class="headerlink" title="小总结"></a>小总结</h5><ol>
<li><strong>DAGScheduler</strong>会将接收到的<strong>Job</strong>封装成<strong>JobSubmitted</strong>事件（实际是DAGSchedulerEvent），加入到eventQueue（事件阻塞队列中）</li>
<li>eventThread事件线程的<code>run()</code>方法会从阻塞队列中取出相应的event事件</li>
<li>事件取出之后，会经过<font color=green>handleJobSubmitted()</font>方法进行<strong>阶段的划分</strong>，阶段可分为两种类型：<ul>
<li><strong>ResultStage（也叫做FinalStage）</strong></li>
<li><strong>ShuffleMapStage</strong></li>
</ul>
</li>
<li>在每一个<strong>Stage创建之前</strong>，会通过<code>getOrCreateParentStages</code>方法获取或创建该阶段的<strong>父阶段</strong></li>
<li>**getOrCreateParentStages()**方法会根据传入的RDD的依赖类型是否是<code>ShuffleDependency</code>进行阶段的划分</li>
<li>阶段的划分是从后向前进行划分的，真正的Job执行，是从前往后进行执行的</li>
<li><strong>Spark的Job中的Stage阶段数=ShuffleDependency的数量+1</strong></li>
</ol>
<h4 id="3-任务的切分"><a href="#3-任务的切分" class="headerlink" title="3. 任务的切分"></a>3. 任务的切分</h4><p><strong>DAGScheduler.scala</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> <span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</span><br><span class="line">     finalRDD: <span class="type">RDD</span>[_],</span><br><span class="line">     func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">     partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">     callSite: <span class="type">CallSite</span>,</span><br><span class="line">     listener: <span class="type">JobListener</span>,</span><br><span class="line">     properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">   <span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line">     <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span></span><br><span class="line">     <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span></span><br><span class="line">     finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)  <span class="comment">// 此处会划分阶段Stage</span></span><br><span class="line">   &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">     <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">       logWarning(<span class="string">&quot;Creating new stage failed due to exception - job: &quot;</span> + jobId, e)</span><br><span class="line">       listener.jobFailed(e)</span><br><span class="line">       <span class="keyword">return</span></span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">// 阶段划分完成之后，会创建一个活跃的Job作业。</span></span><br><span class="line">   <span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, callSite, listener, properties)</span><br><span class="line">   clearCacheLocs()</span><br><span class="line">   logInfo(<span class="string">&quot;Got job %s (%s) with %d output partitions&quot;</span>.format(</span><br><span class="line">     job.jobId, callSite.shortForm, partitions.length))</span><br><span class="line">   logInfo(<span class="string">&quot;Final stage: &quot;</span> + finalStage + <span class="string">&quot; (&quot;</span> + finalStage.name + <span class="string">&quot;)&quot;</span>)</span><br><span class="line">   logInfo(<span class="string">&quot;Parents of final stage: &quot;</span> + finalStage.parents)</span><br><span class="line">   logInfo(<span class="string">&quot;Missing parents: &quot;</span> + getMissingParentStages(finalStage))</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> jobSubmissionTime = clock.getTimeMillis()</span><br><span class="line">   jobIdToActiveJob(jobId) = job</span><br><span class="line">   activeJobs += job</span><br><span class="line">   finalStage.setActiveJob(job)</span><br><span class="line">   <span class="keyword">val</span> stageIds = jobIdToStageIds(jobId).toArray</span><br><span class="line">   <span class="keyword">val</span> stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))</span><br><span class="line">   listenerBus.post(</span><br><span class="line">     <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))</span><br><span class="line">     <span class="comment">// 提交阶段</span></span><br><span class="line">   submitStage(finalStage)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*======================  提交阶段   ======================*/</span></span><br><span class="line"> <span class="comment">/** Submits stage, but first recursively submits any missing parents. */</span></span><br><span class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>) &#123;</span><br><span class="line">   <span class="keyword">val</span> jobId = activeJobForStage(stage)</span><br><span class="line">   <span class="keyword">if</span> (jobId.isDefined) &#123;</span><br><span class="line">     logDebug(<span class="string">&quot;submitStage(&quot;</span> + stage + <span class="string">&quot;)&quot;</span>)</span><br><span class="line">       <span class="comment">// 传过来的是ResultStage</span></span><br><span class="line">     <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">         <span class="comment">// getMissingParentStages：获取丢失的父阶段，或者是未提交的阶段</span></span><br><span class="line">       <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">       logDebug(<span class="string">&quot;missing: &quot;</span> + missing)</span><br><span class="line">         <span class="comment">// 如果missing为空，就代表次阶段没有父阶段了</span></span><br><span class="line">       <span class="keyword">if</span> (missing.isEmpty) &#123;</span><br><span class="line">         logInfo(<span class="string">&quot;Submitting &quot;</span> + stage + <span class="string">&quot; (&quot;</span> + stage.rdd + <span class="string">&quot;), which has no missing parents&quot;</span>)</span><br><span class="line">           <span class="comment">// 提交任务  submitMissingTasks</span></span><br><span class="line">         submitMissingTasks(stage, jobId.get)  <span class="comment">// 提交任务的核心方法</span></span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="comment">// 如果有未提交，或者父阶段，就递归的提交父阶段</span></span><br><span class="line">         <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">           submitStage(parent)  <span class="comment">// 此处会有一个递归</span></span><br><span class="line">         &#125;</span><br><span class="line">         waitingStages += stage</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     abortStage(stage, <span class="string">&quot;No active job for stage &quot;</span> + stage.id, <span class="type">None</span>)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p><strong>小总结</strong></p>
<p>上述源码可以看出，阶段划分完成之后，会通过<strong>submitStage()<strong>方法提交最后一个</strong>ResultStage</strong>阶段。</p>
<p>在<strong>submitStage()<strong>方法内部首先会通过</strong>getMissingParentStages</strong>方法找到传过来的Stage是否存在Parent<strong>父阶段</strong>；</p>
<ul>
<li><p>如果不存在父阶段，就通过<strong>submitMissingTask</strong>方法提交本阶段的任务；</p>
</li>
<li><p>如果存在父阶段，就递归的提交父阶段；</p>
</li>
</ul>
<p><strong>submitMissingTasks()</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"><span class="comment">/** Called when stage&#x27;s parents are available and we can now do its task. */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitMissingTasks</span></span>(stage: <span class="type">Stage</span>, jobId: <span class="type">Int</span>) &#123;</span><br><span class="line">    logDebug(<span class="string">&quot;submitMissingTasks(&quot;</span> + stage + <span class="string">&quot;)&quot;</span>)</span><br><span class="line">    <span class="comment">// Get our pending tasks and remember them in our pendingTasks entry</span></span><br><span class="line">    stage.pendingPartitions.clear()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// First figure out the indexes of partition ids to compute.</span></span><br><span class="line">    <span class="keyword">val</span> partitionsToCompute: <span class="type">Seq</span>[<span class="type">Int</span>] = stage.findMissingPartitions()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Use the scheduling pool, job group, description, etc. from an ActiveJob associated</span></span><br><span class="line">    <span class="comment">// with this Stage</span></span><br><span class="line">    <span class="keyword">val</span> properties = jobIdToActiveJob(jobId).properties</span><br><span class="line"></span><br><span class="line">    runningStages += stage</span><br><span class="line">    <span class="comment">// SparkListenerStageSubmitted should be posted before testing whether tasks are</span></span><br><span class="line">    <span class="comment">// serializable. If tasks are not serializable, a SparkListenerStageCompleted event</span></span><br><span class="line">    <span class="comment">// will be posted, which should always come after a corresponding SparkListenerStageSubmitted</span></span><br><span class="line">    <span class="comment">// event.</span></span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> s: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - <span class="number">1</span>)</span><br><span class="line">      <span class="keyword">case</span> s: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        outputCommitCoordinator.stageStart(</span><br><span class="line">          stage = s.id, maxPartitionId = s.rdd.partitions.length - <span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> taskIdToLocations: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">TaskLocation</span>]] = <span class="keyword">try</span> &#123;</span><br><span class="line">      stage <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> s: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt; (id, getPreferredLocs(stage.rdd, id))&#125;.toMap</span><br><span class="line">        <span class="keyword">case</span> s: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">            <span class="keyword">val</span> p = s.partitions(id)</span><br><span class="line">            (id, getPreferredLocs(stage.rdd, p))</span><br><span class="line">          &#125;.toMap</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">        stage.makeNewStageAttempt(partitionsToCompute.size)</span><br><span class="line">        listenerBus.post(<span class="type">SparkListenerStageSubmitted</span>(stage.latestInfo, properties))</span><br><span class="line">        abortStage(stage, <span class="string">s&quot;Task creation failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>&quot;</span>, <span class="type">Some</span>(e))</span><br><span class="line">        runningStages -= stage</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)</span><br><span class="line">    listenerBus.post(<span class="type">SparkListenerStageSubmitted</span>(stage.latestInfo, properties))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> Maybe we can keep the taskBinary in Stage to avoid serializing it multiple times.</span></span><br><span class="line">    <span class="comment">// Broadcasted binary for the task, used to dispatch tasks to executors. Note that we broadcast</span></span><br><span class="line">    <span class="comment">// the serialized copy of the RDD and for each task we will deserialize it, which means each</span></span><br><span class="line">    <span class="comment">// task gets a different copy of the RDD. This provides stronger isolation between tasks that</span></span><br><span class="line">    <span class="comment">// might modify state of objects referenced in their closures. This is necessary in Hadoop</span></span><br><span class="line">    <span class="comment">// where the JobConf/Configuration object is not thread-safe.</span></span><br><span class="line">    <span class="keyword">var</span> taskBinary: <span class="type">Broadcast</span>[<span class="type">Array</span>[<span class="type">Byte</span>]] = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep).</span></span><br><span class="line">      <span class="comment">// For ResultTask, serialize and broadcast (rdd, func).</span></span><br><span class="line">      <span class="keyword">val</span> taskBinaryBytes: <span class="type">Array</span>[<span class="type">Byte</span>] = stage <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">          <span class="type">JavaUtils</span>.bufferToArray(</span><br><span class="line">            closureSerializer.serialize((stage.rdd, stage.shuffleDep): <span class="type">AnyRef</span>))</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">          <span class="type">JavaUtils</span>.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): <span class="type">AnyRef</span>))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      taskBinary = sc.broadcast(taskBinaryBytes)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="comment">// In the case of a failure during serialization, abort the stage.</span></span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">NotSerializableException</span> =&gt;</span><br><span class="line">        abortStage(stage, <span class="string">&quot;Task not serializable: &quot;</span> + e.toString, <span class="type">Some</span>(e))</span><br><span class="line">        runningStages -= stage</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Abort execution</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">        abortStage(stage, <span class="string">s&quot;Task serialization failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>&quot;</span>, <span class="type">Some</span>(e))</span><br><span class="line">        runningStages -= stage</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">/*================== 创建任务Task  ==================*/</span></span><br><span class="line">    <span class="keyword">val</span> tasks: <span class="type">Seq</span>[<span class="type">Task</span>[_]] = <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// 根据Stage的类型：ShuffleMapStage类型；还是ResultStage类型创建不同类型的任务Task</span></span><br><span class="line">      stage <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">          <span class="comment">// 分区计算，返回相应的分区id，默认是0 until partitions</span></span><br><span class="line">          partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">            <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">            <span class="keyword">val</span> part = stage.rdd.partitions(id)</span><br><span class="line">              <span class="comment">// 创建ShuffleMapTask类型的Task</span></span><br><span class="line">            <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, stage.latestInfo.attemptId,</span><br><span class="line">              taskBinary, part, locs, stage.latestInfo.taskMetrics, properties, <span class="type">Option</span>(jobId),</span><br><span class="line">              <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">            <span class="keyword">val</span> p: <span class="type">Int</span> = stage.partitions(id)</span><br><span class="line">            <span class="keyword">val</span> part = stage.rdd.partitions(p)</span><br><span class="line">            <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">              <span class="comment">// 创建ResultTask类型的Task</span></span><br><span class="line">            <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptId,</span><br><span class="line">              taskBinary, part, locs, id, properties, stage.latestInfo.taskMetrics,</span><br><span class="line">              <span class="type">Option</span>(jobId), <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">        abortStage(stage, <span class="string">s&quot;Task creation failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>&quot;</span>, <span class="type">Some</span>(e))</span><br><span class="line">        runningStages -= stage</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">// 每个Stage中对应的任务Task创建好之后，会发送给TaskScheduler进行Task任务的调度</span></span><br><span class="line">    <span class="keyword">if</span> (tasks.size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      logInfo(<span class="string">&quot;Submitting &quot;</span> + tasks.size + <span class="string">&quot; missing tasks from &quot;</span> + stage + <span class="string">&quot; (&quot;</span> + stage.rdd + <span class="string">&quot;)&quot;</span>)</span><br><span class="line">      stage.pendingPartitions ++= tasks.map(_.partitionId)</span><br><span class="line">      logDebug(<span class="string">&quot;New pending partitions: &quot;</span> + stage.pendingPartitions)</span><br><span class="line">        <span class="comment">// 每一个阶段创建好的任务Task会被封装成TaskSet对象，通过TaskScheduler.submitTasks将Stage中的任务Tasks传递给TaskScheduler进行任务的调度</span></span><br><span class="line">      taskScheduler.submitTasks(<span class="keyword">new</span> <span class="type">TaskSet</span>(</span><br><span class="line">        tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))</span><br><span class="line">      stage.latestInfo.submissionTime = <span class="type">Some</span>(clock.getTimeMillis())</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Because we posted SparkListenerStageSubmitted earlier, we should mark</span></span><br><span class="line">      <span class="comment">// the stage as completed here in case there are no tasks to run</span></span><br><span class="line">      markStageAsFinished(stage, <span class="type">None</span>)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> debugString = stage <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">          <span class="string">s&quot;Stage <span class="subst">$&#123;stage&#125;</span> is actually done; &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;(available: <span class="subst">$&#123;stage.isAvailable&#125;</span>,&quot;</span> +</span><br><span class="line">            <span class="string">s&quot;available outputs: <span class="subst">$&#123;stage.numAvailableOutputs&#125;</span>,&quot;</span> +</span><br><span class="line">            <span class="string">s&quot;partitions: <span class="subst">$&#123;stage.numPartitions&#125;</span>)&quot;</span></span><br><span class="line">        <span class="keyword">case</span> stage : <span class="type">ResultStage</span> =&gt;</span><br><span class="line">          <span class="string">s&quot;Stage <span class="subst">$&#123;stage&#125;</span> is actually done; (partitions: <span class="subst">$&#123;stage.numPartitions&#125;</span>)&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">      logDebug(debugString)</span><br><span class="line"></span><br><span class="line">      submitWaitingChildStages(stage)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h5 id="小总结-2"><a href="#小总结-2" class="headerlink" title="小总结"></a>小总结</h5><ol>
<li><p>阶段划分完成之后，<strong>ResultStage</strong>类型的<strong>finalStage</strong>即可创建完成，之后通过**submitStage()**方法进行阶段的提交</p>
</li>
<li><p><strong>submitStage()<strong>接收到</strong>ResultStage</strong>后，首先会尝试获取传入阶段Stage的父阶段：<strong>getMissingParentStages()</strong></p>
</li>
<li><p>如果parent阶段为空，即传入的阶段<strong>没有父阶段</strong>，就通过**submitMissingTask()**方法来提交Stage中的任务</p>
</li>
<li><p>如果parent阶段不为空，即传入的阶段<strong>还有父阶段没有提交</strong>，就递归的调用**submitStage()**进行父阶段的提交</p>
</li>
<li><p>**submitMissingTask()**接收到传入的Stage，会根据传入的Stage的类型创建相应类型的Task</p>
<ul>
<li><table>
<thead>
<tr>
<th align="center">Stage类型</th>
<th align="center">Task类型</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>ShuffleMapStage</strong></td>
<td align="center"><strong>ShuffleMapTask</strong></td>
</tr>
<tr>
<td align="center"><strong>ResultStage</strong></td>
<td align="center"><strong>ResultTask</strong></td>
</tr>
</tbody></table>
</li>
</ul>
</li>
<li><p>任务<strong>Task</strong>创建好之后，会被封装成<strong>TaskSet</strong>对象，通过<strong>TaskScheduler</strong>的<strong>submitTasks()<strong>方法传递给</strong>TaskScheduler</strong>进行任务的调度</p>
</li>
</ol>
<h4 id="4-任务的调度"><a href="#4-任务的调度" class="headerlink" title="4. 任务的调度"></a>4. 任务的调度</h4><p><strong>TaskScheduler</strong>是一个Trait特质，其子类<strong>TaskSchedulerImpl</strong>是具体的实现类</p>
<p><strong>TaskSchedulerImpl.scala</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*================================     TaskSchedulerImpl     ================================*/</span>  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">submitTasks</span></span>(taskSet: <span class="type">TaskSet</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> tasks = taskSet.tasks</span><br><span class="line">    logInfo(<span class="string">&quot;Adding task set &quot;</span> + taskSet.id + <span class="string">&quot; with &quot;</span> + tasks.length + <span class="string">&quot; tasks&quot;</span>)</span><br><span class="line">    <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">        <span class="comment">// 将接收到的TaskSet进一步封装成TaskSetManager对象</span></span><br><span class="line">      <span class="keyword">val</span> manager = createTaskSetManager(taskSet, maxTaskFailures)</span><br><span class="line">      <span class="keyword">val</span> stage = taskSet.stageId</span><br><span class="line">      <span class="keyword">val</span> stageTaskSets =</span><br><span class="line">        taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">TaskSetManager</span>])</span><br><span class="line">      stageTaskSets(taskSet.stageAttemptId) = manager</span><br><span class="line">      <span class="keyword">val</span> conflictingTaskSet = stageTaskSets.exists &#123; <span class="keyword">case</span> (_, ts) =&gt;</span><br><span class="line">        ts.taskSet != taskSet &amp;&amp; !ts.isZombie</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (conflictingTaskSet) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s&quot;more than one active taskSet for stage <span class="subst">$stage</span>:&quot;</span> +</span><br><span class="line">          <span class="string">s&quot; <span class="subst">$&#123;stageTaskSets.toSeq.map&#123;_._2.taskSet.id&#125;</span>.mkString(&quot;</span>,<span class="string">&quot;)&#125;&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">        <span class="comment">// 将上述构建好的TaskSetManager对象添加到资源调度池中rootPool中（有放就有取，是在backend.reviveOffers()）</span></span><br><span class="line">        <span class="comment">// schedulableBuilder调度构建器，涉及到两种：FIFO和Fair，调度模式默认为FIFO</span></span><br><span class="line">      schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (!isLocal &amp;&amp; !hasReceivedTask) &#123;</span><br><span class="line">        starvationTimer.scheduleAtFixedRate(<span class="keyword">new</span> <span class="type">TimerTask</span>() &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">            <span class="keyword">if</span> (!hasLaunchedTask) &#123;</span><br><span class="line">              logWarning(<span class="string">&quot;Initial job has not accepted any resources; &quot;</span> +</span><br><span class="line">                <span class="string">&quot;check your cluster UI to ensure that workers are registered &quot;</span> +</span><br><span class="line">                <span class="string">&quot;and have sufficient resources&quot;</span>)</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="keyword">this</span>.cancel()</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;, <span class="type">STARVATION_TIMEOUT_MS</span>, <span class="type">STARVATION_TIMEOUT_MS</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      hasReceivedTask = <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 此处的调用从资源调度池中取相应的任务（SchedulerBackend 复活，唤醒 Offers）</span></span><br><span class="line">    backend.reviveOffers()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><strong>SchedulerBackend.scala</strong>是一个特质，其子类实现分为集群模式：<strong>CoarseGrainedSchedulerBackend</strong>和本地模式：<strong>LocalSchedulerBackend</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/*特质*/</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A backend interface for scheduling systems that allows plugging in different ones under</span></span><br><span class="line"><span class="comment"> * TaskSchedulerImpl. We assume a Mesos-like model where the application gets resource offers as</span></span><br><span class="line"><span class="comment"> * machines become available and can launch tasks on them.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">SchedulerBackend</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> appId = <span class="string">&quot;spark-application-&quot;</span> + <span class="type">System</span>.currentTimeMillis</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>(): <span class="type">Unit</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stop</span></span>(): <span class="type">Unit</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reviveOffers</span></span>(): <span class="type">Unit</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">defaultParallelism</span></span>(): <span class="type">Int</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">killTask</span></span>(taskId: <span class="type">Long</span>, executorId: <span class="type">String</span>, interruptThread: <span class="type">Boolean</span>): <span class="type">Unit</span> =</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isReady</span></span>(): <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Get an application ID associated with the job.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @return An application ID</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">applicationId</span></span>(): <span class="type">String</span> = appId</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Get the attempt ID for this run, if the cluster manager supports multiple</span></span><br><span class="line"><span class="comment">   * attempts. Applications run in client mode will not have attempt IDs.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @return The application attempt id, if available.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">applicationAttemptId</span></span>(): <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Get the URLs for the driver logs. These URLs are used to display the links in the UI</span></span><br><span class="line"><span class="comment">   * Executors tab for the driver.</span></span><br><span class="line"><span class="comment">   * @return Map containing the log names and their respective URLs</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getDriverLogUrls</span></span>: <span class="type">Option</span>[<span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>CoarseGrainedSchedulerBackend.scala</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reviveOffers</span></span>() &#123;</span><br><span class="line">    <span class="comment">// SchedulerBackend给自己发送消息，SchedulerBackend在receive方法中接收到消息</span></span><br><span class="line">   driverEndpoint.send(<span class="type">ReviveOffers</span>)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 接收消息</span></span><br><span class="line">   <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">     <span class="keyword">case</span> <span class="type">StatusUpdate</span>(executorId, taskId, state, data) =&gt;</span><br><span class="line">       scheduler.statusUpdate(taskId, state, data.value)</span><br><span class="line">       <span class="keyword">if</span> (<span class="type">TaskState</span>.isFinished(state)) &#123;</span><br><span class="line">         executorDataMap.get(executorId) <span class="keyword">match</span> &#123;</span><br><span class="line">           <span class="keyword">case</span> <span class="type">Some</span>(executorInfo) =&gt;</span><br><span class="line">             executorInfo.freeCores += scheduler.<span class="type">CPUS_PER_TASK</span></span><br><span class="line">             makeOffers(executorId)</span><br><span class="line">           <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">             <span class="comment">// Ignoring the update since we don&#x27;t know about the executor.</span></span><br><span class="line">             logWarning(<span class="string">s&quot;Ignored task status update (<span class="subst">$taskId</span> state <span class="subst">$state</span>) &quot;</span> +</span><br><span class="line">               <span class="string">s&quot;from unknown executor with ID <span class="subst">$executorId</span>&quot;</span>)</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;</span><br><span class="line">	<span class="comment">// 模式匹配，匹配到ReviveOffers类型，最终调用makeOffers方法</span></span><br><span class="line">     <span class="keyword">case</span> <span class="type">ReviveOffers</span> =&gt;</span><br><span class="line">       makeOffers()</span><br><span class="line"></span><br><span class="line">     <span class="keyword">case</span> <span class="type">KillTask</span>(taskId, executorId, interruptThread) =&gt;</span><br><span class="line">       executorDataMap.get(executorId) <span class="keyword">match</span> &#123;</span><br><span class="line">         <span class="keyword">case</span> <span class="type">Some</span>(executorInfo) =&gt;</span><br><span class="line">           executorInfo.executorEndpoint.send(<span class="type">KillTask</span>(taskId, executorId, interruptThread))</span><br><span class="line">         <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">           <span class="comment">// Ignoring the task kill since the executor is not registered.</span></span><br><span class="line">           logWarning(<span class="string">s&quot;Attempted to kill task <span class="subst">$taskId</span> for unknown executor <span class="subst">$executorId</span>.&quot;</span>)</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Make fake resource offers on all executors：翻译：在所有的Executor上创建假的resource offers</span></span><br><span class="line">   <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeOffers</span></span>() &#123;</span><br><span class="line">     <span class="comment">// Filter out executors under killing</span></span><br><span class="line">     <span class="keyword">val</span> activeExecutors = executorDataMap.filterKeys(executorIsAlive)   <span class="comment">// 过滤出目前还存活的Executors</span></span><br><span class="line">       <span class="comment">// 会把存活的Executor封装成workOffers样例类对象，其中包括：Executor的id，Executor的远程Host地址，和Executor的空闲核心数</span></span><br><span class="line">     <span class="keyword">val</span> workOffers = activeExecutors.map &#123; <span class="keyword">case</span> (id, executorData) =&gt;</span><br><span class="line">       <span class="keyword">new</span> <span class="type">WorkerOffer</span>(id, executorData.executorHost, executorData.freeCores)</span><br><span class="line">     &#125;.toIndexedSeq</span><br><span class="line">       <span class="comment">// 最终通过launchTasks方法启动具体的任务</span></span><br><span class="line">     launchTasks(scheduler.resourceOffers(workOffers))</span><br><span class="line">       </span><br><span class="line">       <span class="comment">/*</span></span><br><span class="line"><span class="comment">       	具体从调度池中取任务：scheduler.resourceOffers(workOffers)</span></span><br><span class="line"><span class="comment">       	这里的scheduler就是TaskScheduler任务调度器</span></span><br><span class="line"><span class="comment">       	任务取回之后，会伴随着LaunchTasks消息发送给相应的Executor</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="comment">// Launch tasks returned by a set of resource offers</span></span><br><span class="line">   <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchTasks</span></span>(tasks: <span class="type">Seq</span>[<span class="type">Seq</span>[<span class="type">TaskDescription</span>]]) &#123;</span><br><span class="line">     <span class="keyword">for</span> (task &lt;- tasks.flatten) &#123;</span><br><span class="line">         <span class="comment">// ，每一个task会先序列化</span></span><br><span class="line">       <span class="keyword">val</span> serializedTask = ser.serialize(task)</span><br><span class="line">       <span class="keyword">if</span> (serializedTask.limit &gt;= maxRpcMessageSize) &#123;</span><br><span class="line">         scheduler.taskIdToTaskSetManager.get(task.taskId).foreach &#123; taskSetMgr =&gt;</span><br><span class="line">           <span class="keyword">try</span> &#123;</span><br><span class="line">             <span class="keyword">var</span> msg = <span class="string">&quot;Serialized task %s:%d was %d bytes, which exceeds max allowed: &quot;</span> +</span><br><span class="line">               <span class="string">&quot;spark.rpc.message.maxSize (%d bytes). Consider increasing &quot;</span> +</span><br><span class="line">               <span class="string">&quot;spark.rpc.message.maxSize or using broadcast variables for large values.&quot;</span></span><br><span class="line">             msg = msg.format(task.taskId, task.index, serializedTask.limit, maxRpcMessageSize)</span><br><span class="line">             taskSetMgr.abort(msg)</span><br><span class="line">           &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">             <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logError(<span class="string">&quot;Exception in error callback&quot;</span>, e)</span><br><span class="line">           &#125;</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">else</span> &#123;</span><br><span class="line">         <span class="keyword">val</span> executorData = executorDataMap(task.executorId)</span><br><span class="line">         executorData.freeCores -= scheduler.<span class="type">CPUS_PER_TASK</span></span><br><span class="line"></span><br><span class="line">         logDebug(<span class="string">s&quot;Launching task <span class="subst">$&#123;task.taskId&#125;</span> on executor id: <span class="subst">$&#123;task.executorId&#125;</span> hostname: &quot;</span> +</span><br><span class="line">           <span class="string">s&quot;<span class="subst">$&#123;executorData.executorHost&#125;</span>.&quot;</span>)</span><br><span class="line">		<span class="comment">// 向ExecutorEndPoint发送相应的LaunchTask消息（序列化后的task会被封装成LaunchTask消息）</span></span><br><span class="line">         executorData.executorEndpoint.send(<span class="type">LaunchTask</span>(<span class="keyword">new</span> <span class="type">SerializableBuffer</span>(serializedTask)))</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<p><strong>TaskSchedulerImpl.scala</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从资源调度池中取出相应的任务Tasks </span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Called by cluster manager to offer resources on slaves. We respond by asking our active task</span></span><br><span class="line"><span class="comment">   * sets for tasks in order of priority. We fill each node with tasks in a round-robin manner so</span></span><br><span class="line"><span class="comment">   * that tasks are balanced across the cluster.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">resourceOffers</span></span>(offers: <span class="type">IndexedSeq</span>[<span class="type">WorkerOffer</span>]): <span class="type">Seq</span>[<span class="type">Seq</span>[<span class="type">TaskDescription</span>]] = synchronized &#123;</span><br><span class="line">    <span class="comment">// Mark each slave as alive and remember its hostname</span></span><br><span class="line">    <span class="comment">// Also track if new executor is added</span></span><br><span class="line">    <span class="keyword">var</span> newExecAvail = <span class="literal">false</span></span><br><span class="line">    <span class="keyword">for</span> (o &lt;- offers) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!hostToExecutors.contains(o.host)) &#123;</span><br><span class="line">        hostToExecutors(o.host) = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">String</span>]()</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (!executorIdToRunningTaskIds.contains(o.executorId)) &#123;</span><br><span class="line">        hostToExecutors(o.host) += o.executorId</span><br><span class="line">        executorAdded(o.executorId, o.host)</span><br><span class="line">        executorIdToHost(o.executorId) = o.host</span><br><span class="line">        executorIdToRunningTaskIds(o.executorId) = <span class="type">HashSet</span>[<span class="type">Long</span>]()</span><br><span class="line">        newExecAvail = <span class="literal">true</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">for</span> (rack &lt;- getRackForHost(o.host)) &#123;</span><br><span class="line">        hostsByRack.getOrElseUpdate(rack, <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">String</span>]()) += o.host</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Randomly shuffle offers to avoid always placing tasks on the same set of workers.</span></span><br><span class="line">    <span class="keyword">val</span> shuffledOffers = <span class="type">Random</span>.shuffle(offers)</span><br><span class="line">    <span class="comment">// Build a list of tasks to assign to each worker.</span></span><br><span class="line">      <span class="comment">// 每个Task和相应的workOffer会被分装成TaskDescription对象，这些TaskDescription对象就是最终要返回的Tasks</span></span><br><span class="line">    <span class="keyword">val</span> tasks = shuffledOffers.map(o =&gt; <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">TaskDescription</span>](o.cores))</span><br><span class="line">    <span class="keyword">val</span> availableCpus = shuffledOffers.map(o =&gt; o.cores).toArray</span><br><span class="line">      <span class="comment">// 调度池中的任务会根据调度算法进行任务的优先级排序，得到排序好的任务集</span></span><br><span class="line">    <span class="keyword">val</span> sortedTaskSets = rootPool.getSortedTaskSetQueue</span><br><span class="line">    <span class="keyword">for</span> (taskSet &lt;- sortedTaskSets) &#123;</span><br><span class="line">      logDebug(<span class="string">&quot;parentName: %s, name: %s, runningTasks: %s&quot;</span>.format(</span><br><span class="line">        taskSet.parent.name, taskSet.name, taskSet.runningTasks))</span><br><span class="line">      <span class="keyword">if</span> (newExecAvail) &#123;</span><br><span class="line">        taskSet.executorAdded()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Take each TaskSet in our scheduling order, and then offer it each node in increasing order</span></span><br><span class="line">    <span class="comment">// of locality levels so that it gets a chance to launch local tasks on all of them.</span></span><br><span class="line">    <span class="comment">// <span class="doctag">NOTE:</span> the preferredLocality order: PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY</span></span><br><span class="line">      <span class="comment">// 任务的本地化调度级别：进程本地化，节点本地化，机架本地化，任意，无偏向</span></span><br><span class="line">    <span class="keyword">for</span> (taskSet &lt;- sortedTaskSets) &#123;</span><br><span class="line">      <span class="keyword">var</span> launchedAnyTask = <span class="literal">false</span></span><br><span class="line">      <span class="keyword">var</span> launchedTaskAtCurrentMaxLocality = <span class="literal">false</span></span><br><span class="line">        <span class="comment">// 任务的本地化级别调度，移动数据不如移动计算</span></span><br><span class="line">      <span class="keyword">for</span> (currentMaxLocality &lt;- taskSet.myLocalityLevels) &#123;</span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">          launchedTaskAtCurrentMaxLocality = resourceOfferSingleTaskSet(</span><br><span class="line">            taskSet, currentMaxLocality, shuffledOffers, availableCpus, tasks)</span><br><span class="line">          launchedAnyTask |= launchedTaskAtCurrentMaxLocality</span><br><span class="line">        &#125; <span class="keyword">while</span> (launchedTaskAtCurrentMaxLocality)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (!launchedAnyTask) &#123;</span><br><span class="line">        taskSet.abortIfCompletelyBlacklisted(hostToExecutors)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (tasks.size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      hasLaunchedTask = <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">// 返回任务</span></span><br><span class="line">    <span class="keyword">return</span> tasks</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h5 id="小总结-3"><a href="#小总结-3" class="headerlink" title="小总结"></a>小总结</h5><ol>
<li><strong>TaskScheduler</strong>的<strong>submitTask()<strong>方法接收到</strong>DAGScheduler</strong>传递过来的<strong>TaskSet</strong>，将<strong>TaskSet</strong>进一步封装成<strong>TaskSetManager</strong>，来监控或者失败重试相应的<strong>Task</strong></li>
<li><strong>TaskSetManager</strong>构建好之后，<strong>TaskScheduler</strong>会将<strong>TaskSetManager</strong>加入到资源调度池<strong>rootPool</strong>中</li>
<li>资源调度池中有放就有取</li>
<li>实际触发取出资源调度池中的任务的方式是<strong>backend.reviveOffers()</strong></li>
<li><strong>SchedulerBackend</strong>(调度后端)的<strong>reviveOffers()<strong>方法会通过</strong>DriverEndPoint.send(ReviveOffers)<strong>给自己发送消息；</strong>CoarseGrainedSchedulerBackend</strong>接收到消息后根据消息的类型匹配到<strong>ReviveOffer</strong>类型，然后执行<strong>makeOffers()<strong>方法创建</strong>WorkOffer</strong></li>
<li><strong>makeOffers()<strong>方法中，首先会过滤出目前存活的</strong>Executor</strong>，然后将<strong>Executor</strong>封装成<strong>WorkOffer</strong>对象<strong>workOffers</strong></li>
<li><strong>workOffers</strong>创建完成后，<strong>TaskScheduler</strong>会按照<strong>FIFO</strong>（默认）或者<strong>Fair</strong>调度算法从资源调度池中取出相应的<strong>TaskSetManager</strong>，进一步取出<strong>TaskSet</strong>中<strong>Task</strong>，结合<strong>workOffers</strong>，按照任务的本地化调度级别，将<strong>Task</strong>和相应的<strong>workOffer</strong>封装成<strong>TaskDescription</strong>对象</li>
<li>接下来通过<strong>launchTask()<strong>方法将</strong>TaskDescription</strong>任务信息发送给远程的<strong>Executor</strong>机器上</li>
<li>在<strong>launchTask()<strong>方法内部，首先会对</strong>TaskDescription</strong>进行序列化，序列化完成后，会将序列化后的数据封装成<strong>LaunchTask</strong>对象消息</li>
<li>最终<strong>LaunchTask</strong>消息会被发送到远程的<strong>ExecutorEndPoint</strong>上，进行任务的执行</li>
</ol>
<h4 id="5-任务的执行"><a href="#5-任务的执行" class="headerlink" title="5. 任务的执行"></a>5. 任务的执行</h4><p><strong>CoarseGrainedExecutorBackend</strong>接收到<strong>CoarseGrainedSchedulerBackend</strong>发送来的<strong>LaunchTask</strong>消息。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*======================================  CoarseGrainedExecutorBackend.scala  =======================================*/</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">RegisteredExecutor</span> =&gt;</span><br><span class="line">      logInfo(<span class="string">&quot;Successfully registered with driver&quot;</span>)</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        executor = <span class="keyword">new</span> <span class="type">Executor</span>(executorId, hostname, env, userClassPath, isLocal = <span class="literal">false</span>)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">          exitExecutor(<span class="number">1</span>, <span class="string">&quot;Unable to create executor due to &quot;</span> + e.getMessage, e)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">RegisterExecutorFailed</span>(message) =&gt;</span><br><span class="line">      exitExecutor(<span class="number">1</span>, <span class="string">&quot;Slave registration failed: &quot;</span> + message)</span><br><span class="line">	<span class="comment">// 接收到消息，并匹配到LaunchTask类型的消息</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">LaunchTask</span>(data) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (executor == <span class="literal">null</span>) &#123;</span><br><span class="line">        exitExecutor(<span class="number">1</span>, <span class="string">&quot;Received LaunchTask command but executor was null&quot;</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 将LaunchTask中的消息进行反序列化，得到相应的TaskDescription</span></span><br><span class="line">        <span class="keyword">val</span> taskDesc = ser.deserialize[<span class="type">TaskDescription</span>](data.value)</span><br><span class="line">        logInfo(<span class="string">&quot;Got assigned task &quot;</span> + taskDesc.taskId)</span><br><span class="line">          <span class="comment">// executor通过launchTask()方法在Executor上发布相应的Task</span></span><br><span class="line">        executor.launchTask(<span class="keyword">this</span>, taskId = taskDesc.taskId, attemptNumber = taskDesc.attemptNumber,</span><br><span class="line">          taskDesc.name, taskDesc.serializedTask)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">KillTask</span>(taskId, _, interruptThread) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (executor == <span class="literal">null</span>) &#123;</span><br><span class="line">        exitExecutor(<span class="number">1</span>, <span class="string">&quot;Received KillTask command but executor was null&quot;</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        executor.killTask(taskId, interruptThread)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">StopExecutor</span> =&gt;</span><br><span class="line">      stopping.set(<span class="literal">true</span>)</span><br><span class="line">      logInfo(<span class="string">&quot;Driver commanded a shutdown&quot;</span>)</span><br><span class="line">      <span class="comment">// Cannot shutdown here because an ack may need to be sent back to the caller. So send</span></span><br><span class="line">      <span class="comment">// a message to self to actually do the shutdown.</span></span><br><span class="line">      self.send(<span class="type">Shutdown</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Shutdown</span> =&gt;</span><br><span class="line">      stopping.set(<span class="literal">true</span>)</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">&quot;CoarseGrainedExecutorBackend-stop-executor&quot;</span>) &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="comment">// executor.stop() will call `SparkEnv.stop()` which waits until RpcEnv stops totally.</span></span><br><span class="line">          <span class="comment">// However, if `executor.stop()` runs in some thread of RpcEnv, RpcEnv won&#x27;t be able to</span></span><br><span class="line">          <span class="comment">// stop until `executor.stop()` returns, which becomes a dead-lock (See SPARK-14180).</span></span><br><span class="line">          <span class="comment">// Therefore, we put this line in a new thread.</span></span><br><span class="line">          executor.stop()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.start()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>Executor.scala</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">// Maintains the list of running tasks.</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> runningTasks = <span class="keyword">new</span> <span class="type">ConcurrentHashMap</span>[<span class="type">Long</span>, <span class="type">TaskRunner</span>]  <span class="comment">// 维护正在运行的task列表</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">launchTask</span></span>(</span><br><span class="line">      context: <span class="type">ExecutorBackend</span>,</span><br><span class="line">      taskId: <span class="type">Long</span>,</span><br><span class="line">      attemptNumber: <span class="type">Int</span>,</span><br><span class="line">      taskName: <span class="type">String</span>,</span><br><span class="line">      serializedTask: <span class="type">ByteBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// 此处的tr实际上是创建一个实现了Runnable接口的线程对象TaskRunner</span></span><br><span class="line">    <span class="keyword">val</span> tr = <span class="keyword">new</span> <span class="type">TaskRunner</span>(context, taskId = taskId, attemptNumber = attemptNumber, taskName,</span><br><span class="line">      serializedTask)</span><br><span class="line">      <span class="comment">// 记录正在运行的Task</span></span><br><span class="line">    runningTasks.put(taskId, tr)</span><br><span class="line">      <span class="comment">// 通过线程池来最终执行相应的的Task任务</span></span><br><span class="line">    threadPool.execute(tr)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*================================   TaskRunner类继承了Runnable接口    ================================*/</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">TaskRunner</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">      execBackend: <span class="type">ExecutorBackend</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">      val taskId: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">      val attemptNumber: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">      taskName: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">      serializedTask: <span class="type">ByteBuffer</span></span>)</span></span><br><span class="line">    <span class="keyword">extends</span> <span class="type">Runnable</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> threadName = <span class="string">s&quot;Executor task launch worker for task <span class="subst">$taskId</span>&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Whether this task has been killed. */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> killed = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> threadId: <span class="type">Long</span> = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getThreadId</span></span>: <span class="type">Long</span> = threadId</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Whether this task has been finished. */</span></span><br><span class="line">    <span class="meta">@GuardedBy</span>(<span class="string">&quot;TaskRunner.this&quot;</span>)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> finished = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isFinished</span></span>: <span class="type">Boolean</span> = synchronized &#123; finished &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** How much the JVM process has spent in GC when the task starts to run. */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">var</span> startGCTime: <span class="type">Long</span> = _</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * The task to run. This will be set in run() by deserializing the task binary coming</span></span><br><span class="line"><span class="comment">     * from the driver. Once it is set, it will never be changed.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@volatile</span> <span class="keyword">var</span> task: <span class="type">Task</span>[<span class="type">Any</span>] = _</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">kill</span></span>(interruptThread: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      logInfo(<span class="string">s&quot;Executor is trying to kill <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>)&quot;</span>)</span><br><span class="line">      killed = <span class="literal">true</span></span><br><span class="line">      <span class="keyword">if</span> (task != <span class="literal">null</span>) &#123;</span><br><span class="line">        synchronized &#123;</span><br><span class="line">          <span class="keyword">if</span> (!finished) &#123;</span><br><span class="line">            task.kill(interruptThread)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Set the finished flag to true and clear the current thread&#x27;s interrupt status</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">setTaskFinishedAndClearInterruptStatus</span></span>(): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">      <span class="keyword">this</span>.finished = <span class="literal">true</span></span><br><span class="line">      <span class="comment">// SPARK-14234 - Reset the interrupted status of the thread to avoid the</span></span><br><span class="line">      <span class="comment">// ClosedByInterruptException during execBackend.statusUpdate which causes</span></span><br><span class="line">      <span class="comment">// Executor to crash</span></span><br><span class="line">      <span class="type">Thread</span>.interrupted()</span><br><span class="line">      <span class="comment">// Notify any waiting TaskReapers. Generally there will only be one reaper per task but there</span></span><br><span class="line">      <span class="comment">// is a rare corner-case where one task can have two reapers in case cancel(interrupt=False)</span></span><br><span class="line">      <span class="comment">// is followed by cancel(interrupt=True). Thus we use notifyAll() to avoid a lost wakeup:</span></span><br><span class="line">      notifyAll()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      threadId = <span class="type">Thread</span>.currentThread.getId</span><br><span class="line">      <span class="type">Thread</span>.currentThread.setName(threadName)</span><br><span class="line">      <span class="keyword">val</span> threadMXBean = <span class="type">ManagementFactory</span>.getThreadMXBean</span><br><span class="line">      <span class="keyword">val</span> taskMemoryManager = <span class="keyword">new</span> <span class="type">TaskMemoryManager</span>(env.memoryManager, taskId)</span><br><span class="line">      <span class="keyword">val</span> deserializeStartTime = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">      <span class="keyword">val</span> deserializeStartCpuTime = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class="line">        threadMXBean.getCurrentThreadCpuTime</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line">      <span class="type">Thread</span>.currentThread.setContextClassLoader(replClassLoader)</span><br><span class="line">      <span class="keyword">val</span> ser = env.closureSerializer.newInstance()</span><br><span class="line">      logInfo(<span class="string">s&quot;Running <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>)&quot;</span>)</span><br><span class="line">      execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">RUNNING</span>, <span class="type">EMPTY_BYTE_BUFFER</span>)</span><br><span class="line">      <span class="keyword">var</span> taskStart: <span class="type">Long</span> = <span class="number">0</span></span><br><span class="line">      <span class="keyword">var</span> taskStartCpu: <span class="type">Long</span> = <span class="number">0</span></span><br><span class="line">      startGCTime = computeTotalGcTime()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> (taskFiles, taskJars, taskProps, taskBytes) =</span><br><span class="line">          <span class="type">Task</span>.deserializeWithDependencies(serializedTask)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Must be set before updateDependencies() is called, in case fetching dependencies</span></span><br><span class="line">        <span class="comment">// requires access to properties contained within (e.g. for access control).</span></span><br><span class="line">        <span class="type">Executor</span>.taskDeserializationProps.set(taskProps)</span><br><span class="line"></span><br><span class="line">        updateDependencies(taskFiles, taskJars)</span><br><span class="line">        task = ser.deserialize[<span class="type">Task</span>[<span class="type">Any</span>]](taskBytes, <span class="type">Thread</span>.currentThread.getContextClassLoader)</span><br><span class="line">        task.localProperties = taskProps</span><br><span class="line">        task.setTaskMemoryManager(taskMemoryManager)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// If this task has been killed before we deserialized it, let&#x27;s quit now. Otherwise,</span></span><br><span class="line">        <span class="comment">// continue executing the task.</span></span><br><span class="line">        <span class="keyword">if</span> (killed) &#123;</span><br><span class="line">          <span class="comment">// Throw an exception rather than returning, because returning within a try&#123;&#125; block</span></span><br><span class="line">          <span class="comment">// causes a NonLocalReturnControl exception to be thrown. The NonLocalReturnControl</span></span><br><span class="line">          <span class="comment">// exception will be caught by the catch block, leading to an incorrect ExceptionFailure</span></span><br><span class="line">          <span class="comment">// for the task.</span></span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">TaskKilledException</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        logDebug(<span class="string">&quot;Task &quot;</span> + taskId + <span class="string">&quot;&#x27;s epoch is &quot;</span> + task.epoch)</span><br><span class="line">        env.mapOutputTracker.updateEpoch(task.epoch)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Run the actual task and measure its runtime.</span></span><br><span class="line">        taskStart = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">        taskStartCpu = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class="line">          threadMXBean.getCurrentThreadCpuTime</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line">        <span class="keyword">var</span> threwException = <span class="literal">true</span></span><br><span class="line">        <span class="keyword">val</span> value = <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 任务真正的运行</span></span><br><span class="line">          <span class="keyword">val</span> res = task.run(</span><br><span class="line">            taskAttemptId = taskId,</span><br><span class="line">            attemptNumber = attemptNumber,</span><br><span class="line">            metricsSystem = env.metricsSystem)</span><br><span class="line">          threwException = <span class="literal">false</span></span><br><span class="line">          res</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          <span class="keyword">val</span> releasedLocks = env.blockManager.releaseAllLocksForTask(taskId)</span><br><span class="line">          <span class="keyword">val</span> freedMemory = taskMemoryManager.cleanUpAllAllocatedMemory()</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (freedMemory &gt; <span class="number">0</span> &amp;&amp; !threwException) &#123;</span><br><span class="line">            <span class="keyword">val</span> errMsg = <span class="string">s&quot;Managed memory leak detected; size = <span class="subst">$freedMemory</span> bytes, TID = <span class="subst">$taskId</span>&quot;</span></span><br><span class="line">            <span class="keyword">if</span> (conf.getBoolean(<span class="string">&quot;spark.unsafe.exceptionOnMemoryLeak&quot;</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(errMsg)</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              logWarning(errMsg)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (releasedLocks.nonEmpty &amp;&amp; !threwException) &#123;</span><br><span class="line">            <span class="keyword">val</span> errMsg =</span><br><span class="line">              <span class="string">s&quot;<span class="subst">$&#123;releasedLocks.size&#125;</span> block locks were not released by TID = <span class="subst">$taskId</span>:\n&quot;</span> +</span><br><span class="line">                releasedLocks.mkString(<span class="string">&quot;[&quot;</span>, <span class="string">&quot;, &quot;</span>, <span class="string">&quot;]&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> (conf.getBoolean(<span class="string">&quot;spark.storage.exceptionOnPinLeak&quot;</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(errMsg)</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              logWarning(errMsg)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> taskFinish = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">        <span class="keyword">val</span> taskFinishCpu = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class="line">          threadMXBean.getCurrentThreadCpuTime</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">        <span class="comment">// If the task has been killed, let&#x27;s fail it.</span></span><br><span class="line">        <span class="keyword">if</span> (task.killed) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">TaskKilledException</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> resultSer = env.serializer.newInstance()</span><br><span class="line">        <span class="keyword">val</span> beforeSerialization = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">        <span class="keyword">val</span> valueBytes = resultSer.serialize(value)</span><br><span class="line">        <span class="keyword">val</span> afterSerialization = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Deserialization happens in two parts: first, we deserialize a Task object, which</span></span><br><span class="line">        <span class="comment">// includes the Partition. Second, Task.run() deserializes the RDD and function to be run.</span></span><br><span class="line">        task.metrics.setExecutorDeserializeTime(</span><br><span class="line">          (taskStart - deserializeStartTime) + task.executorDeserializeTime)</span><br><span class="line">        task.metrics.setExecutorDeserializeCpuTime(</span><br><span class="line">          (taskStartCpu - deserializeStartCpuTime) + task.executorDeserializeCpuTime)</span><br><span class="line">        <span class="comment">// We need to subtract Task.run()&#x27;s deserialization time to avoid double-counting</span></span><br><span class="line">        task.metrics.setExecutorRunTime((taskFinish - taskStart) - task.executorDeserializeTime)</span><br><span class="line">        task.metrics.setExecutorCpuTime(</span><br><span class="line">          (taskFinishCpu - taskStartCpu) - task.executorDeserializeCpuTime)</span><br><span class="line">        task.metrics.setJvmGCTime(computeTotalGcTime() - startGCTime)</span><br><span class="line">        task.metrics.setResultSerializationTime(afterSerialization - beforeSerialization)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Note: accumulator updates must be collected after TaskMetrics is updated</span></span><br><span class="line">        <span class="keyword">val</span> accumUpdates = task.collectAccumulatorUpdates()</span><br><span class="line">        <span class="comment">// <span class="doctag">TODO:</span> do not serialize value twice</span></span><br><span class="line">        <span class="keyword">val</span> directResult = <span class="keyword">new</span> <span class="type">DirectTaskResult</span>(valueBytes, accumUpdates)</span><br><span class="line">        <span class="keyword">val</span> serializedDirectResult = ser.serialize(directResult)</span><br><span class="line">        <span class="keyword">val</span> resultSize = serializedDirectResult.limit</span><br><span class="line"></span><br><span class="line">        <span class="comment">// directSend = sending directly back to the driver</span></span><br><span class="line">        <span class="keyword">val</span> serializedResult: <span class="type">ByteBuffer</span> = &#123;</span><br><span class="line">          <span class="keyword">if</span> (maxResultSize &gt; <span class="number">0</span> &amp;&amp; resultSize &gt; maxResultSize) &#123;</span><br><span class="line">            logWarning(<span class="string">s&quot;Finished <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>). Result is larger than maxResultSize &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;(<span class="subst">$&#123;Utils.bytesToString(resultSize)&#125;</span> &gt; <span class="subst">$&#123;Utils.bytesToString(maxResultSize)&#125;</span>), &quot;</span> +</span><br><span class="line">              <span class="string">s&quot;dropping it.&quot;</span>)</span><br><span class="line">            ser.serialize(<span class="keyword">new</span> <span class="type">IndirectTaskResult</span>[<span class="type">Any</span>](<span class="type">TaskResultBlockId</span>(taskId), resultSize))</span><br><span class="line">          &#125; <span class="keyword">else</span> <span class="keyword">if</span> (resultSize &gt; maxDirectResultSize) &#123;</span><br><span class="line">            <span class="keyword">val</span> blockId = <span class="type">TaskResultBlockId</span>(taskId)</span><br><span class="line">            env.blockManager.putBytes(</span><br><span class="line">              blockId,</span><br><span class="line">              <span class="keyword">new</span> <span class="type">ChunkedByteBuffer</span>(serializedDirectResult.duplicate()),</span><br><span class="line">              <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">            logInfo(</span><br><span class="line">              <span class="string">s&quot;Finished <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>). <span class="subst">$resultSize</span> bytes result sent via BlockManager)&quot;</span>)</span><br><span class="line">            ser.serialize(<span class="keyword">new</span> <span class="type">IndirectTaskResult</span>[<span class="type">Any</span>](blockId, resultSize))</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            logInfo(<span class="string">s&quot;Finished <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>). <span class="subst">$resultSize</span> bytes result sent to driver&quot;</span>)</span><br><span class="line">            serializedDirectResult</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FINISHED</span>, serializedResult)</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> ffe: <span class="type">FetchFailedException</span> =&gt;</span><br><span class="line">          <span class="keyword">val</span> reason = ffe.toTaskFailedReason</span><br><span class="line">          setTaskFinishedAndClearInterruptStatus()</span><br><span class="line">          execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FAILED</span>, ser.serialize(reason))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> _: <span class="type">TaskKilledException</span> =&gt;</span><br><span class="line">          logInfo(<span class="string">s&quot;Executor killed <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>)&quot;</span>)</span><br><span class="line">          setTaskFinishedAndClearInterruptStatus()</span><br><span class="line">          execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">KILLED</span>, ser.serialize(<span class="type">TaskKilled</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> _: <span class="type">InterruptedException</span> <span class="keyword">if</span> task.killed =&gt;</span><br><span class="line">          logInfo(<span class="string">s&quot;Executor interrupted and killed <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>)&quot;</span>)</span><br><span class="line">          setTaskFinishedAndClearInterruptStatus()</span><br><span class="line">          execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">KILLED</span>, ser.serialize(<span class="type">TaskKilled</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> <span class="type">CausedBy</span>(cDE: <span class="type">CommitDeniedException</span>) =&gt;</span><br><span class="line">          <span class="keyword">val</span> reason = cDE.toTaskFailedReason</span><br><span class="line">          setTaskFinishedAndClearInterruptStatus()</span><br><span class="line">          execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FAILED</span>, ser.serialize(reason))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">          <span class="comment">// Attempt to exit cleanly by informing the driver of our failure.</span></span><br><span class="line">          <span class="comment">// If anything goes wrong (or this was a fatal exception), we will delegate to</span></span><br><span class="line">          <span class="comment">// the default uncaught exception handler, which will terminate the Executor.</span></span><br><span class="line">          logError(<span class="string">s&quot;Exception in <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>)&quot;</span>, t)</span><br><span class="line"></span><br><span class="line">          <span class="comment">// Collect latest accumulator values to report back to the driver</span></span><br><span class="line">          <span class="keyword">val</span> accums: <span class="type">Seq</span>[<span class="type">AccumulatorV2</span>[_, _]] =</span><br><span class="line">            <span class="keyword">if</span> (task != <span class="literal">null</span>) &#123;</span><br><span class="line">              task.metrics.setExecutorRunTime(<span class="type">System</span>.currentTimeMillis() - taskStart)</span><br><span class="line">              task.metrics.setJvmGCTime(computeTotalGcTime() - startGCTime)</span><br><span class="line">              task.collectAccumulatorUpdates(taskFailed = <span class="literal">true</span>)</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="type">Seq</span>.empty</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> accUpdates = accums.map(acc =&gt; acc.toInfo(<span class="type">Some</span>(acc.value), <span class="type">None</span>))</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> serializedTaskEndReason = &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">              ser.serialize(<span class="keyword">new</span> <span class="type">ExceptionFailure</span>(t, accUpdates).withAccums(accums))</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> _: <span class="type">NotSerializableException</span> =&gt;</span><br><span class="line">                <span class="comment">// t is not serializable so just send the stacktrace</span></span><br><span class="line">                ser.serialize(<span class="keyword">new</span> <span class="type">ExceptionFailure</span>(t, accUpdates, <span class="literal">false</span>).withAccums(accums))</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          setTaskFinishedAndClearInterruptStatus()</span><br><span class="line">          execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FAILED</span>, serializedTaskEndReason)</span><br><span class="line"></span><br><span class="line">          <span class="comment">// Don&#x27;t forcibly exit unless the exception was inherently fatal, to avoid</span></span><br><span class="line">          <span class="comment">// stopping other tasks unnecessarily.</span></span><br><span class="line">          <span class="keyword">if</span> (<span class="type">Utils</span>.isFatalError(t)) &#123;</span><br><span class="line">            <span class="type">SparkUncaughtExceptionHandler</span>.uncaughtException(t)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        runningTasks.remove(taskId)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<h5 id="小总结-4"><a href="#小总结-4" class="headerlink" title="小总结"></a>小总结</h5><ol>
<li><strong>CoarseGrainedExecutorBackend</strong>接收到<strong>CoarseGrainedSchedulerBackend</strong>发送过来的<strong>LaunchTask</strong>消息</li>
<li>在<strong>CoarseGrainedExecutorBackend</strong>内部，首先会对<strong>LaunchTask</strong>消息进行反序列化，重新得到序列化之前的<strong>TaskDescription</strong>对象</li>
<li>反序列化出来的<strong>TaskDescription</strong>会被<strong>Executor</strong>通过**launchTask()**方法发布任务Task</li>
<li>在<strong>Executor</strong>的<strong>launchTask</strong>方法内部，首先会根据<strong>TaskDescription</strong>创建一个实现了<strong>Runnable</strong>接口的线程<strong>TaskRuner</strong>对象<strong>tr</strong></li>
<li>然后记录正在运行的任务</li>
<li>最终通过线程池<strong>threadPool</strong>来执行<strong>TaskRunner</strong>对象<strong>tr</strong></li>
</ol>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><img src="source/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E4%BB%B6-16334225169802.svg" alt="未命名文件"></p>
<ol>
<li>编写的<strong>Spark</strong>程序，每遇到一个<strong>Action</strong>算子就会启动一个<strong>Job</strong></li>
<li>首先会进入<strong>DAGScheduler</strong>的<strong>runJob</strong>方法中，进一步触发<strong>DAGScheduler</strong>中的**submitJob()**方法</li>
<li><strong>submitJob()<strong>方法内部会将</strong>rdd</strong>封装成实现了<strong>DAGSchedulerEvent</strong>类的<strong>JobSubmitted</strong>事件，添加到<strong>DAGSchedulerEventProcessLoop</strong>中的<strong>eventQueue</strong>中</li>
<li><strong>DAGSchedulerEventProcessLoop</strong>中会有一个事件线程<strong>EventThread</strong>，<strong>run</strong>方法就会从阻塞队列<code>EventQueue</code>中取出相应的事件进行运行。</li>
<li>取出来的事件会通过<strong>handleJobSubmitted()<strong>方法进行</strong>阶段的划分</strong>和<strong>阶段的提交</strong>处理</li>
<li>在<strong>handleJobSubmitted</strong>方法内部，首先会定义一个<strong>ResultStage</strong>类型的变量<strong>finalStage</strong>，在真正赋值之前会先创建其父阶段（如果父阶段存在），每个<strong>stage</strong>对象都会记录其<strong>父阶段</strong>的信息<strong>Parent</strong></li>
<li><strong>finalStage</strong>创建完成之后，通过<strong>submitStage()<strong>方法提交</strong>ResultStage</strong>阶段</li>
<li>submitStage()方法内部，首先会通过<strong>getMissingParentStages()<strong>方法获取传递过来的阶段的父阶段，如果父阶段存在，则递归的提交父阶段</strong>submitStage(parentStage)</strong>,如果不存在父阶段，就通过<strong>submitMissingTasks(…)<strong>进行</strong>阶段中任务的切分与提交</strong></li>
<li>在<strong>submitMissingTask()<strong>方法中，会根据传递过来的</strong>stage</strong>的类型创建相应类型的<strong>Task</strong><ul>
<li>如果<strong>Stage</strong>的类型是<strong>ShuffleMapStage</strong>，就创建<strong>ShuffleMapTask</strong>类型的<strong>Task</strong></li>
<li>如果<strong>Stage</strong>的类型是<strong>ResultStage</strong>，就创建<strong>ResultTask</strong>类型的<strong>Task</strong></li>
</ul>
</li>
<li>一个阶段中的<strong>Tasks</strong>创建完成后，会被封装成<strong>TaskSet</strong>对象</li>
<li>通过<strong>taskScheduler.submitTask()<strong>方法传递给</strong>TaskScheduler</strong>进行任务级别的调度</li>
<li><strong>TaskScheduler</strong>的<strong>submitTask()<strong>方法接收到</strong>TaskSet</strong>后，会把<strong>TaskSet</strong>进一步封装成<strong>TaskSetManager</strong>对象（负责监控任务的执行状态，失败重试等），然后根据<strong>FIFO（默认）</strong>或者<strong>Fair策略</strong>加入到资源调度池<strong>rooPool</strong>中</li>
<li>每调用一次<strong>submitTask</strong>方法，就会相应的执行一次**backend.reviveOffers()**方法</li>
<li>集群下对应的<strong>SchedulerBackend</strong>为<strong>CoarseGrainedSchedulerBackend</strong>，其中<strong>reviveOffers()<strong>方法内部会通过</strong>driverEndPoint.send(ReviveOffers)<strong>给自己发送</strong>ReviveOffers</strong>消息</li>
<li><strong>CoarseGrainedSchedulerBackend</strong>收到传来的<strong>ReviveOffers</strong>类型的消息后，触发**makeOffers()**方法</li>
<li><strong>makeOffers()<strong>方法首先会过滤出目前还存活的</strong>Executor</strong>，并将存活的<strong>Executor</strong>封装成<strong>WorkOffers</strong>对象<strong>workOffers</strong>，之后调用<code>taskScheduler.resourceOffers(workOffers)</code></li>
<li>随后会通过<strong>TaskScheduler</strong>中的<strong>resourceOffers(workOffers)<strong>，将资源调度池rootPool中的TaskSetManager按照特定的</strong>调度算法FIFO(默认)或者Fair</strong>取出，将其中的<strong>Task</strong>和<strong>WorkOffers</strong>对象结合任务的<strong>本地化调度级别</strong>，封装成<strong>TaskDescription</strong>对象，并将其<strong>TaskDescription</strong>进行返回到上述<strong>SchedulerBackend</strong>的**makeOffers()**中，继续向下执行</li>
<li>接下来会调用<strong>launchTask()<strong>方法进行</strong>Task的发布</strong></li>
<li>在<strong>launchTask()<strong>方法内部会将相应的</strong>TaskDescription</strong>进行<strong>序列化</strong>，然后将<strong>序列化后的TaskDescription封装成LaunchTask</strong>消息</li>
<li>封装好的<strong>LaunchTask消息</strong>会通过<strong>executorEndPoint.send()方法</strong>发送给远程机器相应的<strong>Executor</strong>上</li>
<li><strong>CoarseGrainedExecutorBackend</strong>中的<strong>receive()<strong>方法负责接收远程的调度后端</strong>CoarseGrainedSchedulerBackend</strong>发送来的消息，其中首先会创建Executor对象，用于执行相应的任务</li>
<li><strong>CoarseGrainedExecutorBackend</strong>接收到<strong>CoarseGrainedSchedulerBackend</strong>发送过来的<strong>LaunchTask消息</strong>，会对该消息进行<strong>反序列化</strong>，得到相应的<strong>TaskDescription</strong></li>
<li>然后通过<strong>executor.launchTask(…)<strong>方法发布相应序列化后的任务</strong>TaskDescription</strong>到对应的<strong>Executor对象</strong>中</li>
<li>在具体的<strong>Executor对象</strong>中，会根据<strong>launchTask()<strong>方法接收到的</strong>Task</strong>创建一个实现了<strong>Runnable接口</strong>的<strong>TaskRunner</strong>对象<strong>rn</strong>，并将该运行中的任务记录到runningTasks中做记录</li>
<li>最终通过<strong>线程池threadPool</strong>的方式，运行上述<strong>rn</strong>对象，其具体的实现细节定义在了<strong>rn</strong>的**run()**方法中</li>
</ol>
<h3 id="4-Shuffle"><a href="#4-Shuffle" class="headerlink" title="4. Shuffle"></a>4. Shuffle</h3><h4 id="1-Shuffle的原理和执行过程"><a href="#1-Shuffle的原理和执行过程" class="headerlink" title="1. Shuffle的原理和执行过程"></a>1. Shuffle的原理和执行过程</h4><h4 id="2-Shuffle写磁盘"><a href="#2-Shuffle写磁盘" class="headerlink" title="2. Shuffle写磁盘"></a>2. Shuffle写磁盘</h4><h4 id="3-Shuffle读取磁盘"><a href="#3-Shuffle读取磁盘" class="headerlink" title="3. Shuffle读取磁盘"></a>3. Shuffle读取磁盘</h4><h3 id="5-内存的管理"><a href="#5-内存的管理" class="headerlink" title="5. 内存的管理"></a>5. 内存的管理</h3><h4 id="1-内存的分类"><a href="#1-内存的分类" class="headerlink" title="1. 内存的分类"></a>1. 内存的分类</h4><h4 id="2-内存的配置"><a href="#2-内存的配置" class="headerlink" title="2. 内存的配置"></a>2. 内存的配置</h4><h2 id="11-面试中遇到的问题"><a href="#11-面试中遇到的问题" class="headerlink" title="11. 面试中遇到的问题"></a>11. 面试中遇到的问题</h2><h3 id="1-如何减少任务的网络IO"><a href="#1-如何减少任务的网络IO" class="headerlink" title="1. 如何减少任务的网络IO"></a>1. 如何减少任务的网络IO</h3><p>当数据量比较大的时候，如何减少数据网络传输的网络IO？</p>
<blockquote>
<p>使用轻量级序列化</p>
<p>采用压缩</p>
</blockquote>
<h1 id="Hive-复习"><a href="#Hive-复习" class="headerlink" title="Hive 复习"></a>Hive 复习</h1><h3 id="1-外部表与内部表"><a href="#1-外部表与内部表" class="headerlink" title="1. 外部表与内部表"></a>1. 外部表与内部表</h3><p>内部表又叫做管理表，创建表时不做任何指定，默认创建的就是内部表。想要创建外部表，则需要使用External进行修饰。</p>
<table>
<thead>
<tr>
<th></th>
<th>内部表</th>
<th>外部表</th>
</tr>
</thead>
<tbody><tr>
<td>数据存储位置</td>
<td>内部表数据存储的位置由<code>hive.metastore.warehouse.dir</code>参数指定，默认情况下表的数据存储在HDFS的<code>/user/hive/warehouse/数据库名.db/表名/</code>目录下</td>
<td>外部表数据的存储位置创建表时由<code>Location</code>参数指定；</td>
</tr>
<tr>
<td>导入数据</td>
<td>在导入数据到内部表，内部表将数据移动到自己的数据仓库目录下，数据的声明周期有Hive来进行管理</td>
<td>外部表不会将数据移动到自己的数据仓库目录下，只是在元数据中存储了数据的位置。</td>
</tr>
<tr>
<td>删除表</td>
<td>删除元数据（metadata）和文件</td>
<td>只删除元数据（metadata）</td>
</tr>
</tbody></table>
<h3 id="2-分区与分桶"><a href="#2-分区与分桶" class="headerlink" title="2. 分区与分桶"></a>2. 分区与分桶</h3><ul>
<li><p>分区是将数据按照目录进行划分，将Hive的表的数据进行划分，可以在查询的时候只查询一部分的数据，从而避免全量查询。</p>
</li>
<li><p>分桶是在分区的基础上更细粒度的将数据进行划分。</p>
<p>将整个数据内容按照某列取hash值，对桶的个数取模的方式决定该条记录存放在哪个桶中；具有相同hash值的数据进入到同一个桶，形成同一个文件</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">4</span>; # 分成<span class="number">4</span>个桶</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> user_buckets(id <span class="type">int</span>, name string)</span><br><span class="line">clustered <span class="keyword">by</span>(id) <span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="3-Hive-抽样"><a href="#3-Hive-抽样" class="headerlink" title="3. Hive 抽样"></a>3. Hive 抽样</h3><ol>
<li><p>数据块抽样</p>
<ol>
<li><p>tablesample(n percent)</p>
<blockquote>
<p>根据hive表数据的大小按比例抽取数据，并保存到新的hive表中。</p>
</blockquote>
</li>
<li><p>tablesample(n M)</p>
<blockquote>
<p>指定抽样数据的大小，单位为M</p>
</blockquote>
</li>
<li><p>tablesample(n rows)</p>
<blockquote>
<p>指定抽样数据的行数，其中n代表每个map任务均取n行数据，map数量可通过hive表的简单查询语句确定</p>
</blockquote>
</li>
</ol>
</li>
<li><p>分桶抽样</p>
<p>hive中分桶其实就是根据某一个字段Hash取模，放入指定数据的桶中，比如将表table_1按照ID分成100个桶，其算法是hash(id) % 100，这样，hash(id) % 100 = 0的数据被放到第一个桶中，hash(id) % 100 = 1的记录被放到第二个桶中。创建分桶表的关键语句为：CLUSTER BY语句。</p>
<blockquote>
<p>分桶抽样语法：</p>
<p><code>tablesample(bucket x out of y [on colname])</code></p>
<p>其中x是要抽样的桶编号，编号从1开始，colname表示抽样的列，y表示桶的数量。</p>
<p>例如：将表随机分成10组，抽取其中的第一个桶的数据</p>
<p><code>select * from table1 tablesample(bucket 1 out of 10 on rand())</code></p>
<p>y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。</p>
<p>x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。</p>
<p>注意：x的值必须小于等于y的值，否则</p>
<p>FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table stu_buck</p>
</blockquote>
</li>
<li><p>随机抽样</p>
<ol>
<li><p>使用<code>rand()函数</code>进行随机抽样，limit关键字限制抽样返回的数据，其中rand函数前的distribute 和sort 关键字可以保证数据在mapper和reducer阶段是随机分布的。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> table_n <span class="keyword">where</span> col<span class="operator">=</span>xxx distribute <span class="keyword">by</span> rand() sort <span class="keyword">by</span> rand() limit num;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 还可以使用order by</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> table_n <span class="keyword">where</span> col<span class="operator">=</span>xxx <span class="keyword">order</span> <span class="keyword">by</span> rand() limit num;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ol>
<h3 id="4-order-by-、-distribute-by-、sort-by-和-cluster-by-四个by的区别"><a href="#4-order-by-、-distribute-by-、sort-by-和-cluster-by-四个by的区别" class="headerlink" title="4. order by 、 distribute by 、sort by 和 cluster by 四个by的区别"></a>4. order by 、 distribute by 、sort by 和 cluster by 四个by的区别</h3><h4 id="1-order-by"><a href="#1-order-by" class="headerlink" title="1. order by"></a>1. order by</h4><p>全局排序，只有一个reduce；</p>
<p>缺点：当数据量非常大的时候，耗时太长，效率低下，适用于数据量较小的场景。</p>
<p>有点：数据全局排序</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>

<p>当使用order by 时，默认只走一个reduce，和设置多少个reduce个数无关。</p>
<h4 id="2-sort-by"><a href="#2-sort-by" class="headerlink" title="2. sort by"></a>2. sort by</h4><p>对每一reduce内部的数据进行排序，全局结果集来说不是排序的，即只能保证每一个reduce输出的文件中的数据是按照规定的字段进行排序的；适用于数据量较大，但对排序要求不严格的场合，可以大幅度提升执行效率。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">3</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp sort <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>

<p>需要预先设置reduce个数，结果各个reduce文件内部有序，全局无序</p>
<h4 id="3-distribute-by"><a href="#3-distribute-by" class="headerlink" title="3. distribute by"></a>3. distribute by</h4><p>控制特定的key到指定的reducer，方便后续的聚集操作。类似于MR中partiton，一般会结合sort by使用；这边需要设置reduces的数量为分区的数量，否则不会启动相应的reducer去进行任务的执行，这最终会导致不能完全分区。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.job.reduces=<span class="number">3</span>;</span><br><span class="line">select * from emp distribute by deptno sort by empno desc;</span><br></pre></td></tr></table></figure>

<p>distribute by的分区规则是根据分区字段的hash码与reduce的个数进行取模后，余数相同的分到一个分区。</p>
<p>hive要求distribute by语句要写在sort by语句之前。</p>
<h4 id="4-Cluster-By"><a href="#4-Cluster-By" class="headerlink" title="4. Cluster By"></a>4. Cluster By</h4><p>当distribute by和sort by字段相同时，可以使用cluster by方式。</p>
<p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。<br>1）以下两种写法等价</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp distribute <span class="keyword">by</span> deptno sort <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure>

<h3 id="5-函数"><a href="#5-函数" class="headerlink" title="5. 函数"></a>5. 函数</h3><h4 id="5-1-系统内置函数"><a href="#5-1-系统内置函数" class="headerlink" title="5.1 系统内置函数"></a>5.1 系统内置函数</h4><ol>
<li><p><strong>nvl</strong>：NVL：给值为 NULL 的数据赋值，它的格式是 NVL( value，default_value)。它的功能是如 果 value 为 NULL，则 NVL 函数返回 default_value 的值，否则返回 value 的值，如果两个参数 都为 NULL  ，则返回 NULL。</p>
</li>
<li><p>```sql<br>case     when 条件 then</p>
<pre><code>    when 条件    then
    else    
</code></pre>
<p>end    as 字段名</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">3. ```sql</span><br><span class="line">   concat_ws(&#x27;分隔符&#x27;,字符串1，字符串2,或者 collect_set())</span><br></pre></td></tr></table></figure></li>
<li><p><strong>炸裂函数</strong>：explode</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">		id</span><br><span class="line">		,catagory</span><br><span class="line"><span class="keyword">from</span> 	<span class="keyword">table</span></span><br><span class="line"><span class="keyword">where</span>	...</span><br><span class="line"><span class="keyword">and</span>		...</span><br><span class="line"><span class="keyword">lateral</span> <span class="keyword">view</span></span><br><span class="line">		explode(集合) temp_view <span class="keyword">as</span> catagory</span><br><span class="line">;</span><br></pre></td></tr></table></figure></li>
<li><p>split()</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">split(category,<span class="string">&#x27;,&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="5-2-窗口函数over"><a href="#5-2-窗口函数over" class="headerlink" title="5.2 窗口函数over()"></a>5.2 窗口函数over()</h4><h5 id="LAG-col-n-default-val"><a href="#LAG-col-n-default-val" class="headerlink" title="LAG(col, n, default_val)"></a>LAG(col, n, default_val)</h5><p>当前行的前第n行数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 查询顾客上次购买时间</span></span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    |select</span></span><br><span class="line"><span class="string">    | name, orderdate, cost,</span></span><br><span class="line"><span class="string">    | lag(orderdate, 1, &#x27;1997-03-15&#x27;) over(partition by name order by orderdate)</span></span><br><span class="line"><span class="string">    |from business</span></span><br><span class="line"><span class="string">    |&quot;&quot;&quot;</span>.stripMargin</span><br><span class="line">).show(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">+----+----------+----+----------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">|name|orderdate |cost|lag(orderdate, <span class="number">1</span>, <span class="number">1997</span><span class="number">-03</span><span class="number">-15</span>) <span class="type">OVER</span> (<span class="type">PARTITION</span> <span class="type">BY</span> name <span class="type">ORDER</span> <span class="type">BY</span> orderdate <span class="type">ASC</span> <span class="type">NULLS</span> <span class="type">FIRST</span> <span class="type">ROWS</span> <span class="type">BETWEEN</span> <span class="number">1</span> <span class="type">PRECEDING</span> <span class="type">AND</span> <span class="number">1</span> <span class="type">PRECEDING</span>)|</span><br><span class="line">+----+----------+----+----------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">|mart|<span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>|<span class="number">62</span>  |<span class="number">1997</span><span class="number">-03</span><span class="number">-15</span>                                                                                                                        |</span><br><span class="line">|mart|<span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>|<span class="number">68</span>  |<span class="number">2017</span><span class="number">-04</span><span class="number">-08</span>                                                                                                                        |</span><br><span class="line">|mart|<span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>|<span class="number">75</span>  |<span class="number">2017</span><span class="number">-04</span><span class="number">-09</span>                                                                                                                        |</span><br><span class="line">|mart|<span class="number">2017</span><span class="number">-04</span><span class="number">-13</span>|<span class="number">94</span>  |<span class="number">2017</span><span class="number">-04</span><span class="number">-11</span>                                                                                                                        |</span><br><span class="line">|jack|<span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>|<span class="number">10</span>  |<span class="number">1997</span><span class="number">-03</span><span class="number">-15</span>                                                                                                                        |</span><br><span class="line">|jack|<span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>|<span class="number">46</span>  |<span class="number">2017</span><span class="number">-01</span><span class="number">-01</span>                                                                                                                        |</span><br><span class="line">|jack|<span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>|<span class="number">55</span>  |<span class="number">2017</span><span class="number">-01</span><span class="number">-05</span>                                                                                                                        |</span><br><span class="line">|jack|<span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>|<span class="number">23</span>  |<span class="number">2017</span><span class="number">-01</span><span class="number">-08</span>                                                                                                                        |</span><br><span class="line">|jack|<span class="number">2017</span><span class="number">-04</span><span class="number">-06</span>|<span class="number">42</span>  |<span class="number">2017</span><span class="number">-02</span><span class="number">-03</span>                                                                                                                        |</span><br><span class="line">|tony|<span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>|<span class="number">15</span>  |<span class="number">1997</span><span class="number">-03</span><span class="number">-15</span>                                                                                                                        |</span><br><span class="line">|tony|<span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>|<span class="number">29</span>  |<span class="number">2017</span><span class="number">-01</span><span class="number">-02</span>                                                                                                                        |</span><br><span class="line">|tony|<span class="number">2017</span><span class="number">-01</span><span class="number">-07</span>|<span class="number">50</span>  |<span class="number">2017</span><span class="number">-01</span><span class="number">-04</span>                                                                                                                        |</span><br><span class="line">|neil|<span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>|<span class="number">12</span>  |<span class="number">1997</span><span class="number">-03</span><span class="number">-15</span>                                                                                                                        |</span><br><span class="line">|neil|<span class="number">2017</span><span class="number">-06</span><span class="number">-12</span>|<span class="number">80</span>  |<span class="number">2017</span><span class="number">-05</span><span class="number">-10</span>                                                                                                                        |</span><br><span class="line">+----+----------+----+----------------------------------------------------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<h5 id="LEAD-col-n-default-val"><a href="#LEAD-col-n-default-val" class="headerlink" title="LEAD(col, n, default_val)"></a>LEAD(col, n, default_val)</h5><p>当前行的后第n行数据</p>
<h5 id="ntile-n"><a href="#ntile-n" class="headerlink" title="ntile(n)"></a>ntile(n)</h5><p>把有序窗口的行分发到指定数据的组中，各个组有编号，编号从 1 开始，对 于每一行，NTILE 返回此行所属的组的编号。</p>
<p>可用于返回前%多少的数据，例如分5组，取前20%，那么取组号为1的即可。</p>
<h5 id="rank排名函数"><a href="#rank排名函数" class="headerlink" title="rank排名函数"></a>rank排名函数</h5><p>三种常用的排名函数</p>
<ol>
<li>rank()<ul>
<li>排序相同时会重复，总数不会变</li>
<li>例如：1,2,2,4</li>
</ul>
</li>
<li>row_number()<ul>
<li>根据顺序计算</li>
<li>例如：1,2,3,4</li>
</ul>
</li>
<li>dense_rank()<ul>
<li>排序相同时会重复，总数会减少</li>
<li>例如：1,2,2,3</li>
</ul>
</li>
</ol>
<h3 id="6-Hive调优"><a href="#6-Hive调优" class="headerlink" title="6. Hive调优"></a>6. Hive调优</h3><h4 id="6-1-开启map端预聚合"><a href="#6-1-开启map端预聚合" class="headerlink" title="6.1 开启map端预聚合"></a>6.1 开启map端预聚合</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 是否在map端及你行聚合，默认为True</span></span><br><span class="line"><span class="keyword">set</span> hive.map.aggr<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 在Map端进行预聚合操作的条目数</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.mapaggr.checkinterval <span class="operator">=</span> <span class="number">100000</span>;</span><br><span class="line"><span class="comment">-- 有数据倾斜时进行负载均衡（默认为false）</span></span><br><span class="line"><span class="keyword">set</span> hive.groupby.skewindata <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<p>默认情况下，使用<strong>group by</strong>，Map阶段同一key的数据会分发给一个reduce，当一个key数据过大时，就会产生数据倾斜。</p>
<p>这是可以开启map端预聚合，可以在map端先做一部分的聚合，这样就在一定程度上可以较少进入同一个reducer中的数据了。</p>
<h4 id="6-2-count（distinct-）-优化"><a href="#6-2-count（distinct-）-优化" class="headerlink" title="6.2 count（distinct ） 优化"></a>6.2 count（distinct ） 优化</h4><p>由于<code>count(distinct)</code>操作需要用一个reduce task来完成，这一个reduce需要处理的数据量太大，就会导致整个Job很难完成。</p>
<p>一般<code>count distinct</code>可使用<code>先group by， 再count</code>的方式替换。</p>
<p>因为<code>group by</code>可以通过增加reduces的数量，加快执行速度</p>
<p><code>set mapreduce.job.reduces=5;</code></p>
<h4 id="6-3-避免笛卡尔积"><a href="#6-3-避免笛卡尔积" class="headerlink" title="6.3 避免笛卡尔积"></a>6.3 避免笛卡尔积</h4><h4 id="6-4-行列过滤"><a href="#6-4-行列过滤" class="headerlink" title="6.4 行列过滤"></a>6.4 行列过滤</h4><p>在进行<code>join</code>的时候, 尽量先使用<code>where</code>条件过滤掉一些不合符业务场景的数据，从而减少join前后的两张表的数据量。</p>
<p>如果，先join后过滤，那么就会先全表关联，之后再过滤</p>
<p>查询列的时候，避免使用*;（实际的SQL规范中，也是禁止使用**）</p>
<h4 id="6-5-合理设置map和reduce的数量"><a href="#6-5-合理设置map和reduce的数量" class="headerlink" title="6.5 合理设置map和reduce的数量"></a>6.5 合理设置map和reduce的数量</h4><ol>
<li>通常情况下，作业会通过input的目录产生一个或者多个map任务，主要决定因素取决于input的文件总个数，input的文件大小，集群设置的文件块大小。（切片数）</li>
<li>是不是map数越多越好呢？<ul>
<li>答案hi否定</li>
<li>如果一个任务有很多小文件（远远小于块大小 128m），则每个小文件也会被当做一个块，用一个 map 任务来完成，而一个 map 任务启动和初始化的时间远远大 于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 map 数是受限的。</li>
</ul>
</li>
<li>是不是保证每个 map 处理接近 128m 的文件块，就高枕无忧了？<ul>
<li>答案也是不一定。比如有一个 127m 的文件，正常会用一个 map 去完成，但这个文件只 有一个或者两个小字段，却有几千万的记录，如果 map 处理的逻辑比较复杂，用一个 map 任务去做，肯定也比较耗时。</li>
</ul>
</li>
</ol>
<h5 id="复杂文件增加map数"><a href="#复杂文件增加map数" class="headerlink" title="复杂文件增加map数"></a>复杂文件增加map数</h5><p>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</p>
<h5 id="小文件行合并"><a href="#小文件行合并" class="headerlink" title="小文件行合并"></a>小文件行合并</h5><p>在map执行前，对于小文件进行合并，减少不必要的map数（比如一个文件只有一条数据，但是不合并的情况就会启动一个map来处理这个一条数据，但是map</p>
<p>的启动和加载时间远远大于处理时间，反而耗时）</p>
<h4 id="6-6-并行执行"><a href="#6-6-并行执行" class="headerlink" title="6.6 并行执行"></a>6.6 并行执行</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel<span class="operator">=</span><span class="literal">true</span>; <span class="comment">-- 打开任务并行执行</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number<span class="operator">=</span><span class="number">16</span>; <span class="comment">-- 同一个SQL允许最大并行度，默认为8</span></span><br></pre></td></tr></table></figure>



<h3 id="7-Hive源码学习"><a href="#7-Hive源码学习" class="headerlink" title="7. Hive源码学习"></a>7. Hive源码学习</h3><img src="source/2021年8月份秋招复习笔记/src=http%3A%2F%2Fwww.pianshen.com%2Fimages%2F312%2F2c1818a3d8f7bee6d472827c0fc3c908.JPEG&refer=http%3A%2F%2Fwww.pianshen.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg" alt="img" style="zoom:50%;" />



<p><strong>SQL Parser 解析器</strong></p>
<p>将sql字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在，字段是否存在，sql语义是否有误。</p>
<p><strong>Physical Plan 编译器</strong></p>
<p>将AST编译生成逻辑执行计划。</p>
<p><strong>Query Optimizer 优化器</strong></p>
<p>对逻辑执行计划进行优化</p>
<p><strong>Execution 执行器</strong></p>
<p>把逻辑执行计划转换成可以运行的物理执行计划。对于Hive来说，就是MR、Spark。</p>
<h4 id="7-1-HOL转换成MR任务流程说明"><a href="#7-1-HOL转换成MR任务流程说明" class="headerlink" title="7.1 HOL转换成MR任务流程说明"></a>7.1 HOL转换成MR任务流程说明</h4><ol>
<li>进入程序，利用<code>Antlr</code>框架定义的HQL语法规则，对HQL完成<code>词法分析</code>、<code>语法分析</code>，将HQL转换成抽象语法树：<code>AST</code></li>
<li>遍历AST，抽象出查询的基本组成单元<code>QueryBlock(查询快)</code>，可以理解为最小的查询执行单元</li>
<li>遍历<code>QueryBlock</code>,将其转换为<code>OperatorTree</code>（操作树，也就是逻辑执行计划），可以理解为不可拆分的一个逻辑执行单元</li>
<li>利用逻辑优化器对<code>OperatorTree（操作树）</code>进行逻辑优化。例如合并不必要的<code>ReduceSinkOperator</code>，减少Shuffle数据量；</li>
<li>遍历<code>OperatorTree</code>，转换成TaskTree。也就是翻译成MR任务的流程，将逻辑执行计划转换为物理执行计划。</li>
<li>使用物理优化器对TaskTree进行物理优化；</li>
<li>生成最终的执行计划，提交任务到Hadoop集群运行。</li>
</ol>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210814152542812-16289259443932.png" alt="image-20210814152542812"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">runInternal</span><span class="params">(String command, <span class="type">boolean</span> alreadyCompiled)</span></span><br><span class="line"><span class="keyword">throws</span> CommandProcessorResponse &#123;</span><br><span class="line"> errorMessage = <span class="literal">null</span>;</span><br><span class="line"> SQLState = <span class="literal">null</span>;</span><br><span class="line"> downstreamError = <span class="literal">null</span>;</span><br><span class="line"> LockedDriverState.setLockedDriverState(lDrvState);</span><br><span class="line"> lDrvState.stateLock.lock();</span><br><span class="line"></span><br><span class="line"> ... ...</span><br><span class="line"> <span class="type">PerfLogger</span> <span class="variable">perfLogger</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"> <span class="keyword">if</span> (!alreadyCompiled) &#123;</span><br><span class="line"> <span class="comment">// compile internal will automatically reset the perf logger</span></span><br><span class="line"> <span class="comment">//1.编译 HQL 语句</span></span><br><span class="line"> compileInternal(command, <span class="literal">true</span>);   <span class="comment">// 这个方法里面会包含 解析器，编译器和优化器这个三个组件</span></span><br><span class="line"> <span class="comment">// then we continue to use this perf logger</span></span><br><span class="line"> perfLogger = SessionState.getPerfLogger();</span><br><span class="line"> &#125;</span><br><span class="line"> ... ...</span><br><span class="line"></span><br><span class="line"> <span class="keyword">try</span> &#123;</span><br><span class="line"> <span class="comment">//2.执行</span></span><br><span class="line"> execute();		<span class="comment">// 这里会涉及到执行器</span></span><br><span class="line"> &#125; <span class="keyword">catch</span> (CommandProcessorResponse cpr) &#123;</span><br><span class="line"> rollback(cpr);</span><br><span class="line"> <span class="keyword">throw</span> cpr;</span><br><span class="line"> &#125;</span><br><span class="line"> isFinishedWithError = <span class="literal">false</span>;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ASTNode</span> <span class="variable">tree</span> <span class="operator">=</span> ParseUtils.parse(command, ctx); <span class="comment">// 解析器解析HQL</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 在ParseUtils.parse() 方法中</span></span><br><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment">1. 首先会对sql进行词法分析，得到词法单元token</span></span><br><span class="line"><span class="comment">2. 对token进行语法分析，最终将解析结果赋值给r</span></span><br><span class="line"><span class="comment">3. 通过r.getTree(); 方法获取最终的抽象语法树AST</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line">sem.analyze(tree,ctx); <span class="comment">// 编译器与优化器</span></span><br><span class="line"><span class="comment">// Hive中的优化器（逻辑优化器）都好多好多种，其基本的抽象类为Transform类，其子类实现了各种的优化策略：例如（MapJoinProcessor策略，group Optimizer；分区过滤优化：PartitionConditionRemover；SimplePredicatePushDown）、</span></span><br><span class="line"><span class="comment">// 物理优化器提供的三种：Tez，MR，Spark </span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><font color=red>小总结：</font></p>
<ol>
<li><strong>CliDriver处理传过来的SQL，按照<code>;</code>进行HQL语句的切分，每个HQL单独进行处理</strong></li>
<li><strong>将<code>HQL</code>传递给Hive的<code>Driver组件</code></strong></li>
<li><strong>在Hive的<code>Driver</code>内部：</strong><ol>
<li><strong>解析器将<code>HQL</code>解析成抽象语法树<code>AST(具体使用的是</code>ParseUtils.parse()`方法)</strong><ul>
<li><strong>先对HQL进行词法分析，生成<font color=green>词法单元:Token</font></strong></li>
<li><strong>然后对词法单元Token进行语法分析，生成抽象语法树ASt</strong></li>
<li><strong>这个过程使用的是第三方框架<code>Antlr</code></strong></li>
</ul>
</li>
<li><strong>编译器将AST进行编译，抽象出<code>QueryBlock</code>查询子块，进一步编译成<code>OperatorTree操作树</code>逻辑执行计划</strong></li>
<li><strong>逻辑优化器对<code>OperatorTree</code>进行逻辑优化（具体的逻辑优化器抽象类为<code>Transform</code>类）,编译器将优化后的逻辑执行计划转换成<code>TaskTree物理执行计划</code></strong></li>
<li><strong>物理优化器将<code>TaskTree进行物理优化</code>（物理优化器共有三种 ：<code>TezOptimizer</code>,<code>MapReduceOptimizer</code>,<code>SparkOptimizer</code>）</strong><ul>
<li><strong>其中MR的物理优化器在执行物理优化器方法之前就已经进行了相应的优化。</strong></li>
</ul>
</li>
<li><strong><code>execut()方法</code>,将物理执行计划转化成MR任务，封装成<code>Job</code>,提交至集群运行任务。</strong></li>
</ol>
</li>
</ol>
<h3 id="8-HQL练习"><a href="#8-HQL练习" class="headerlink" title="8. HQL练习"></a>8. HQL练习</h3><p>作者：Pure201911132215663<br>链接：<a target="_blank" rel="noopener" href="https://www.nowcoder.com/discuss/737802?source_id=discuss_experience_nctrack&amp;channel=-1">https://www.nowcoder.com/discuss/737802?source_id=discuss_experience_nctrack&amp;channel=-1</a><br>来源：牛客网</p>
<p>表sal_sum，每次统计前三个月的<a href="">销售</a>额<br> month    sal<br> 1       5<br> 2       3<br> 3       5<br> 4       7    </p>
<p>​     …….<br>​    </p>
<p> 12      6    </p>
<p> 1+2+3，2+3+4，3+4+5 。。。    </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">		<span class="keyword">month</span></span><br><span class="line">		<span class="built_in">sum</span>(sal) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">month</span> <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">2</span> preceding <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span>)	<span class="keyword">as</span> sal_3_sum</span><br><span class="line"><span class="keyword">from</span>	sal_sum</span><br></pre></td></tr></table></figure>











<h1 id="Zookeeper复习"><a href="#Zookeeper复习" class="headerlink" title="Zookeeper复习"></a>Zookeeper复习</h1><h2 id="1-Zookeeper内部原理"><a href="#1-Zookeeper内部原理" class="headerlink" title="1. Zookeeper内部原理"></a>1. Zookeeper内部原理</h2><h3 id="1-1-选举机制"><a href="#1-1-选举机制" class="headerlink" title="1.1 选举机制"></a>1.1 选举机制</h3><ol>
<li><font color=red>半数机制：集群中半数以上机器存活，集群可用。所以Zookeeper适合安装奇数台服务器。</font></li>
<li>Zookeeper虽然在配置文件中并没有指定<strong>Master</strong>和<strong>Slave</strong>。但是，Zookeeper工作时，是有一个节点为Leader，其他则为Follower，Leader是通过内部的选举机制临时产生的。</li>
</ol>
<p><strong>三个核心的选举原则</strong></p>
<ol>
<li>Zookeeper集群中只有超过半数以上的服务器启动，集群才能正常工作；</li>
<li>在集群正常工作之前，<code>myid</code>小的服务器给<code>myid</code>大的服务器投票，知道集群正常工作，选出<code>leader</code>；</li>
<li>选出<code>Leader</code>之后，之前的服务器状态由<code>Looking</code>改变为<code>Following</code>，以后的服务器都是<code>Follower</code>。</li>
</ol>
<p>以一个简单的例子来说明整个选举的过程：</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/webp-20210810162423999" alt="img"></p>
<p>假设有五台服务器组成的Zookeeper集群，他们的id从1~5，同时他们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动。</p>
<ol>
<li><p>服务器1启动，发起一次选举</p>
<blockquote>
<p>服务器1投自己一票，此时集群不够半数以上（3票），选举无法完成；</p>
<p>服务器1状态保持为<code>LOOKING</code></p>
</blockquote>
</li>
<li><p>服务器2启动，再发起一次选举</p>
<blockquote>
<p>服务器2投自己一票，然后服务器1与服务器2交换投票信息，此时服务器1发现服务器2的id比自己的大，更改选票投给服务器2.</p>
<p>此时服务器1票数0票，服务器2票数2票，不够半数以上（3票），选举无法完成；（服务器1的票数在投票结束后，清零）</p>
<p>服务器1，2保持<code>LOOKING</code></p>
</blockquote>
</li>
<li><p>服务器3启动，发起一次选举</p>
<blockquote>
<p>与上面过程一样，服务器1和服务器2先投自己一票，服务器3也投自己一票。</p>
<p>在交换投票信息的时候，发现此时还是LOOKING状态（还没有选取出leader），且服务器3的id最大，那么服务器3就会得到3票，此时服务器3的票数已经超过半数了（3票）</p>
<p>服务器1，2更新状态为FOLLOWING，服务器3更改状态为LEADING；</p>
</blockquote>
</li>
<li><p>服务器4启动，发起一次选举</p>
<blockquote>
<p>此时服务器1，2，3已经不是<code>LOOKING</code>状态，不会改变选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。</p>
<p>此时服务器4服从多数，更改选票信息为服务器3；服务器3此时有4票</p>
<p>服务器4并更改状态为<code>FOLLOWING</code>;</p>
</blockquote>
</li>
<li><p>服务器5启动，同4一样投票给3，此时服务器3一共5票，服务器5为0票；</p>
<blockquote>
<p>服务器5更改状态为<code>FOLLOWING</code></p>
</blockquote>
</li>
</ol>
<h3 id="2-节点类型"><a href="#2-节点类型" class="headerlink" title="2. 节点类型"></a>2. 节点类型</h3><ul>
<li>持久（Persistent）：客户端和服务器端断开连接后，创建的节点不删除</li>
<li>短暂（Ephemeral）：客户端与服务器端断开连接后，创建的节点自己会自动删除</li>
</ul>
<p>说明：创建znode时，设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护。</p>
<p><font color=red>注意：分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序。</font></p>
<h3 id="3-Stat结构体"><a href="#3-Stat结构体" class="headerlink" title="3. Stat结构体"></a>3. Stat结构体</h3><h3 id="4-监听器原理"><a href="#4-监听器原理" class="headerlink" title="4. 监听器原理"></a>4. 监听器原理</h3><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/src=http%253A%252F%252Fimage.bubuko.com%252Finfo%252F202002%252F20200209175734687167.png&refer=http%253A%252F%252Fimage.bubuko.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg" alt="img"></p>
<ol>
<li>首先要有一个main()线程</li>
<li>在main线程中创建Zookeeper客户端，这时就会创建两个线程：一个负责网络连接通信（connect），一个负责监听（Listener）。</li>
<li>通过connect线程，将注册的监听事件发送给Zookeeper。</li>
<li>在Zookeeper的注册监听器列表中将注册的监听事件添加到列表中</li>
<li>Zookeeper监听到数据或路径变化，就会将这个消息发送给listener线程</li>
<li>listener线程内部调用了process()方法。</li>
</ol>
<p>常见的监听：</p>
<ol>
<li><p>监听节点数据的变化</p>
<p><code>get path[watch]</code></p>
</li>
<li><p>监听子节点增减的变化</p>
<p><code>ls path[watch]</code></p>
</li>
</ol>
<h3 id="5-Zookeeper写数据流程"><a href="#5-Zookeeper写数据流程" class="headerlink" title="5. Zookeeper写数据流程"></a>5. Zookeeper写数据流程</h3><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/src=http%253A%252F%252Fimg2018.cnblogs.com%252Fi-beta%252F1201165%252F202002%252F1201165-20200226224404735-1637370414.png&refer=http%253A%252F%252Fimg2018.cnblogs.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg" alt="img"></p>
<ol>
<li>client向zookeeper的Server1上写数据，发送一个写请求。</li>
<li>如果server1不是leader，那么server1会把接收到的请求进一步转发给Leader，因为每个Zookeeper的Server里面有一个Leader。这个Leader会将写请求广播给各个Server，比如Server1，Server2，Server4，Server5；各个Server写成功后就会通知Leader。</li>
<li>当Leader收到<strong>大多数Server数据写成功了</strong>（内部协议：<strong>ZAB</strong>），那么就说明数据写成功了。写成功之后，Leader会告诉Server1数据写成功了。</li>
<li>Server1会进一步通知Client数据写成功了，这时救人位整个操作成功了。</li>
</ol>
<h3 id="6-Zookeeper的常用命令"><a href="#6-Zookeeper的常用命令" class="headerlink" title="6. Zookeeper的常用命令"></a>6. Zookeeper的常用命令</h3><table>
<thead>
<tr>
<th>命令基本语法</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td>ls path[watch]</td>
<td>使用ls命令来查看当前znode中所包含的内容</td>
</tr>
<tr>
<td>ls2 path[watch]</td>
<td>查看当前节点数据并能看到更新次数等数据</td>
</tr>
<tr>
<td>create</td>
<td>普通创建<br />-s含有序列<br />-e 临时（重启或超时消失）</td>
</tr>
<tr>
<td>get path[watch]</td>
<td>获得节点的值</td>
</tr>
<tr>
<td>set</td>
<td>设置节点的具体值</td>
</tr>
<tr>
<td>stat</td>
<td>查看节点状态</td>
</tr>
<tr>
<td>delete</td>
<td>删除节点</td>
</tr>
<tr>
<td>rmr</td>
<td>递归删除节点</td>
</tr>
</tbody></table>
<h1 id="Kafka复习"><a href="#Kafka复习" class="headerlink" title="Kafka复习"></a>Kafka复习</h1><p>kafka是一个分布式的，基于消息订阅-发布模式的消息队列。Kafka对消息保存时根据Topic进行归类。每个消费者组可以订阅不同的Topic；且同一个消费者组的一个消费者只能消费一个分区中的数据。</p>
<p>无论是Kafka集群，还是consumer都依赖于<strong>zookeeper</strong>集群保存一些meta信息，来保证系统可用性。</p>
<p><strong>点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）</strong></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70.png" alt="在这里插入图片描述"></p>
<p>消息生产者生产消息发送到Queue，然后消息消费者从Queue中取到并且消费消息。消息被消费后，queue中不再有存储，所以消息消费者不可能消费已经被消费的消息。Queue支持存在多个消费者，但是对一个消费者而言，只会有一个消费者可以消费。</p>
<p><strong>发布/订阅模式（一对多，消费者消费数据之后不会清除消息）</strong></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70-20210811102106660.png" alt="在这里插入图片描述"></p>
<p>消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。</p>
<p>而发布/订阅模式的消息队列还可以分为两种：</p>
<ol>
<li>队列推送消息给消费者</li>
<li>消费者主动拉取消息</li>
</ol>
<p>如果采用第一种 队列推送消息给消费者，但是每个消费者的消费速度是不一定一致的，因此会发生消息的浪费情况。</p>
<p>在Kafka中则采用的第二种：消费者主动拉取消息。</p>
<h2 id="1-消息队列的作用（为什么使用消息队列？）"><a href="#1-消息队列的作用（为什么使用消息队列？）" class="headerlink" title="1. 消息队列的作用（为什么使用消息队列？）"></a>1. 消息队列的作用（为什么使用消息队列？）</h2><h3 id="削峰"><a href="#削峰" class="headerlink" title="削峰"></a>削峰</h3><p>举个例子：双十一用户，假设在某一时间段内用户的下单请求峰值达到了一亿多，如果不使用消息队列，服务器端就要同时处理这一亿多的请求，如果服务器性能不够，就会崩掉！<br>如果使用了消息队列，就可以实现削峰的作用，限制同时访问服务器的请求数量，从而降低服务器端的负担。</p>
<h3 id="解耦"><a href="#解耦" class="headerlink" title="解耦"></a>解耦</h3><p>通常生产者和消费者的业务逻辑是不同的，当消费者的消费业务逻辑发生变化时，如果不使用消息队列，就需要把整个生产-消费的逻辑改变。<br>如果使用了消息队列，生产者和消费者就实现了解耦，当需求发生变化时，只需要更改需要更改的一方，而不是修改整个 生产-消费的过程。</p>
<h3 id="异步"><a href="#异步" class="headerlink" title="异步"></a>异步</h3><p>很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户<br>把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要<br>的时候再去处理它们。</p>
<h2 id="2-Kafka的特点"><a href="#2-Kafka的特点" class="headerlink" title="2. Kafka的特点"></a>2. Kafka的特点</h2><ul>
<li>类似于消息队列和商业的消息系统，kafka提供对流式数据的发布和订阅</li>
<li>kafka提供一种持久的容错的方式存储流式数据</li>
<li>kafka拥有良好的性能，可以及时地处理流式数据</li>
</ul>
<h2 id="3-Kafka基础架构"><a href="#3-Kafka基础架构" class="headerlink" title="3. Kafka基础架构"></a>3. Kafka基础架构</h2><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70-20210811102926988.png" alt="在这里插入图片描述"></p>
<ol>
<li><p><strong>producer</strong>：消息生产者，就是向Kafka broker发消息的客户端；</p>
</li>
<li><p><strong>consumer</strong>：消息消费者，想kafka broker拉取消息的客户端；</p>
</li>
<li><p><strong>Topic</strong>：可以理解为一个队列；</p>
</li>
<li><p><strong>consumer group（CG）</strong>：这是Kafka用来实现一个topic消息的广播（发送给所有的Consumer）和单播（发给任意一个consumer）的手段</p>
<p>一个topic可以有多个CG。topic的消息复制（不是真正的复制，是逻辑上的复制）到所有的CG，<strong>但每个partition只会把消息发给CG中的一个consumer</strong></p>
<p>如果需要实现广播，那么只需将每个consumer单独设置一个CG就可以了</p>
<p>如果需要实现单播（一个消费者一个分区），只需将所有的consumer设置在同一个CG中即可。</p>
<p>用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic中；</p>
</li>
<li><p><strong>Broker</strong>：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic；</p>
</li>
<li><p><strong>partition</strong>：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器上）上。一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。</p>
</li>
<li><p><strong>Offset</strong>：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如想找位于2049的位置，只要找到2048.kafka的文件即可。淡然the first offset就是<code>00000000000.kafka</code>。</p>
</li>
<li><p><strong>Replication</strong>:副本，为了保证集群中的某个节点发生故障的时候，该节点上的partition数据不会丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个<code>leader</code>和若干个<code>follower</code></p>
</li>
<li><p><strong>leader</strong>: 每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader</p>
</li>
<li><p><strong>follower</strong>：每个分区多个副本中的“从“，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。</p>
</li>
</ol>
<h2 id="4-Kafka部署注意事项"><a href="#4-Kafka部署注意事项" class="headerlink" title="4. Kafka部署注意事项"></a>4. Kafka部署注意事项</h2><p>在配置Kafka上的server.properties时，要注意，每台broker的broker.id是需要设置不同的数字.<font  color=red>broker.id不能重复</font></p>
<p>Kafka运行需要依赖于zookeeper，需要在配置文件中配置zookeeper集群的地址。</p>
<p>在启动kafka集群之前要确保zookeeper集群已经成功启动了。kafka集群停止的时候，要先停kafka，后停zookeeper，要不然kafka会关不掉的。</p>
<h2 id="5-Kafka架构深入"><a href="#5-Kafka架构深入" class="headerlink" title="5. Kafka架构深入"></a>5. Kafka架构深入</h2><h3 id="5-1-Kafka工作流程以及文件存储机制"><a href="#5-1-Kafka工作流程以及文件存储机制" class="headerlink" title="5.1 Kafka工作流程以及文件存储机制"></a>5.1 Kafka工作流程以及文件存储机制</h3><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16286927154611" alt="在这里插入图片描述"></p>
<p>Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。<br>topic是逻辑上的概念，而partition是物理上的概念，每个partition对应一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了那个offset，以便出错恢复时，从上次的位置继续消费。</p>
<h4 id="文件存储机制"><a href="#文件存储机制" class="headerlink" title="文件存储机制"></a>文件存储机制</h4><img src="source/2021年8月份秋招复习笔记/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16286931799323" alt="在这里插入图片描述" style="zoom:50%;" />

<p>由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，kafka采取了<strong>切片</strong>和<strong>索引</strong>机制，将每个partition分为多个<strong>segment</strong>。每个segment对应两个文件：<code>.index</code>和<code>.log</code>。这些文件位于一个文件夹下，该文件命名规则为：<code>topic名称+分区序号</code>。</p>
<p>例如，first 这个 topic 有三个分区，则其对应的文件夹为：first0,first-1,first-2。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">00000000000000000000.index</span><br><span class="line">00000000000000000000.log</span><br><span class="line">00000000000000170410.index</span><br><span class="line">00000000000000170410.log</span><br><span class="line">00000000000000239430.index</span><br><span class="line">00000000000000239430.log</span><br></pre></td></tr></table></figure>

<p>index 和 log 文件以当前 segment 的第一条消息的 offset 命名。</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16286934145025" alt="**在这里插入图片描述**"></p>
<p>“index”文件存储着大量的索引信息，“log”文件存储大量的数据，索引文件中的元数据指向对应数据文件中message的物理偏移地址。</p>
<h3 id="5-2-生产者"><a href="#5-2-生产者" class="headerlink" title="5.2 生产者"></a>5.2 生产者</h3><p><strong>分区的原因：</strong></p>
<ol>
<li>在集群中扩展，每个partition可以通过调整以适应它所在的机器，而一个topic又可以有多个patirion，因此整个集群就可以使用任意大小的数据了。</li>
<li>可以提高并发，因为可以以partition为单位读写了。</li>
</ol>
<p><strong>分区原则</strong></p>
<p>生产者producer发送数据的时候，需要将数据封装成一个<code>ProducerRecord</code>对象。生产者发送<code>ProducerRecord</code>对象的时候，可以指定分区号;</p>
<ol>
<li>指定partition的情况下，直接将指定的值作为partition值</li>
<li>没有指定partition的值但是有key的情况下，将key的hash值与topic的partitions数进行取余得到partition值；</li>
<li>既没有partition值又没有key值的情况下，第一次调用时随机生成一个证书（后面每次调用的时候，在这个整数上自增），将这个值与topic可用的partition总数取余得到partition值，这就是常说的<code>round-robin</code>轮询算法。</li>
</ol>
<h4 id="数据可靠性"><a href="#数据可靠性" class="headerlink" title="数据可靠性"></a>数据可靠性</h4><p><strong>为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送<code>ack</code>（acknowledge确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据（一定时间内没有接收到ack的时候）。</strong></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16286953647487" alt="在这里插入图片描述"></p>
<p><strong>何时发送ack？</strong></p>
<p>确保有follower与leader同步完成，leader再发送ack，这样才能保证leader挂掉之后，能在follower中选举出新的leader。</p>
<p><strong>多少个follower同步完成之后发送ack？</strong></p>
<p>现有方案：</p>
<ul>
<li>半数以上的follower同步完成，即可发送ack</li>
<li>全部的follower同步完成，才可以发送ack</li>
</ul>
<table>
<thead>
<tr>
<th>方案</th>
<th>延迟</th>
<th>副本数量</th>
</tr>
</thead>
<tbody><tr>
<td>半数以上完成同步，就发送ack</td>
<td>低延迟</td>
<td>选举新的leader时，容忍n台节点的故障，需要2n+1个副本</td>
</tr>
<tr>
<td>全部完成同步，才发送ack</td>
<td>延迟高</td>
<td>选举新的leader时，容忍n台节点的故障，需要n+1个副本</td>
</tr>
</tbody></table>
<p>Kafka选择了第二种方案，原因如下：</p>
<ol>
<li>同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。</li>
<li>虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。</li>
</ol>
<h5 id="ISR同步副本机制"><a href="#ISR同步副本机制" class="headerlink" title="ISR同步副本机制"></a>ISR同步副本机制</h5><p>采用第二种方案之后，设想以下场景：leader收到数据，所有的follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那么leader就要一直等下去，直到它完成同步，才能发送ack，这个问题怎么解决呢？</p>
<p><font color=green>在kafka中，每个分区的Leader维护一个动态<code>in-sync-replica set</code>(ISR),意为和leader保持同步的follower的集合。当ISR中的follower完成数据的同步之后，leader就会给follower发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由<code>replica.lag.time.max.ms</code>参数设定。Leader发生故障之后，就会从ISR中选举新的Leader。</font></p>
<h5 id="ack应答机制"><a href="#ack应答机制" class="headerlink" title="ack应答机制"></a>ack应答机制</h5><p>对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没有必要等ISR中的follower全部接收成功。所以Kafka为用户提供了三种可靠性级别，用户根据可靠性和延迟的要求进行权衡，选择配置。</p>
<p><strong>ACK参数：</strong></p>
<blockquote>
<p>0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障的时候有可能<strong>丢数据</strong>。</p>
<p>1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会<strong>丢数据</strong>；</p>
<p>-1（all）：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成之后，broker发送ack之前，leader发生故障，那么会造成<strong>数据重复</strong>。</p>
</blockquote>
<h5 id="故障处理细节"><a href="#故障处理细节" class="headerlink" title="故障处理细节"></a>故障处理细节</h5><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70-20210812102618331.png" alt="在这里插入图片描述"></p>
<p>接下来介绍一下Kafka中一个topic的一个分区的broker挂掉之后，是如恢复的。</p>
<p><font color=green>两个重要的核心概念：</font></p>
<ul>
<li>LEO(Log End Offset)：日志末偏移量<ul>
<li>指的是每个副本最大的offset</li>
</ul>
</li>
<li>HW(Hign WaterMark)：高水位<ul>
<li>指的是消费者能够见到的最大的offset，ISR队列中最小的LEO（高水位）</li>
</ul>
</li>
</ul>
<h6 id="1-follower故障"><a href="#1-follower故障" class="headerlink" title="1. follower故障"></a>1. follower故障</h6><p>follower发生故障之后，会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。<strong>等待follower的LEO大于等于此时的该分区Partition的HW</strong>，即follower追上leader之后，就可以重新加入ISR了。</p>
<h6 id="2-leader故障"><a href="#2-leader故障" class="headerlink" title="2. leader故障"></a>2. leader故障</h6><p>leader发生故障之后，会从ISR中选取一个新的Leader，之后，为了保证多个副本之间的数据一致性，其余的follower会先将各自的log文件中<font color=red>高于HW的部分截取掉</font>，然后从新的leader同步数据。（新官上任三把火，其他人都要按照我的规矩办事，废除上任的制度）</p>
<p><font color=red>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</font></p>
<h5 id="Exactly-Once语义"><a href="#Exactly-Once语义" class="headerlink" title="Exactly Once语义"></a>Exactly Once语义</h5><p>将服务器的ack级别设置为-1，可以保证Producer到Server之间不会丢失数据，即 <strong>At Least Once语义（至少一次）</strong>。相对的，将服务器ack级别设置为0，可以保证生产者每条消息只会被发送一次，即<strong>At Most Once语义（至多一次）。</strong></p>
<p>AtLeaseOnce可以保证数据不丢失，但是不能保证数据不重复；相对的，AtMostOnce可以保证数据不重复，但是不能保证数据不丢失。但是对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据即不能重复也不能丢失，即Exactly Once语义。在 0.11 版本以前的 Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。<br>0.11 版本的 Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指 Producer 不论向 Server 发送多少次重复数据，Server 端都只会持久化一条。幂等性结合 At Least Once 语义，就构成了 Kafka 的 Exactly Once 语义。即：<br><strong>AtLeaseOnce + <em>幂等性</em> = Exactly Once</strong></p>
<p>要启动幂等性，只需将Producer的参数中<code>enable.idompotence</code>设置为<code>true</code>即可。</p>
<h3 id="5-3-分区分配策略"><a href="#5-3-分区分配策略" class="headerlink" title="5.3 分区分配策略"></a>5.3 分区分配策略</h3><h4 id="Range"><a href="#Range" class="headerlink" title="Range"></a>Range</h4><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70-20210812104838555.png" alt="在这里插入图片描述"></p>
<p>Range策略针对的是主题。</p>
<p>由于消费者1，2同时订阅了主题1，而消费者1，2属于同一消费者组。</p>
<p>range策略：主题中的消息数/消费者组中的消费者数   ——  3/2 = 1…..1.那么化为范围的时候，就有一个消费者中要消费2条消息（因为没有整除）</p>
<h4 id="RoundRobin"><a href="#RoundRobin" class="headerlink" title="RoundRobin"></a>RoundRobin</h4><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70-20210812105106318.png" alt="在这里插入图片描述"></p>
<p>RoundRobin策略针对的是消费者组，组内不同消费者订阅的不同主题topic，消费者组内的所有消费者都可以消费这些主题中的消息，消费方式采用轮询。<br><strong>这里需要注意：</strong></p>
<blockquote>
<p>如果采用了这种分区分配策略，就会导致没有订阅topic的消费者也消费到了消息，这样就有可能造成信息分配错了。所以默认是Range策略。同时在生产环境中，同一个消费者组内的消费者订阅的主题是要保持一致的。</p>
</blockquote>
<h3 id="5-4-kafka-消息发送流程"><a href="#5-4-kafka-消息发送流程" class="headerlink" title="5.4 kafka 消息发送流程"></a>5.4 kafka 消息发送流程</h3><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210812145139764-8751101.png" alt="image-20210812145139764" style="zoom:50%;" />



<p>Kafka的Producer发送消息采用的是异步发送的方式。在消息发送的过程中，涉及到了两个线程：**<code>main线程 和 Sender线程</code><strong>，以及一个线程共享变量</strong><code>RecordAccumulator</code>.**</p>
<p>main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到kafka broker上。</p>
<h3 id="5-5-Kafka中的选举机制"><a href="#5-5-Kafka中的选举机制" class="headerlink" title="5.5 Kafka中的选举机制"></a>5.5 Kafka中的选举机制</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/357042753">参考文章</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_27143551/article/details/103033641?utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control">Kafka| 你一定不能错过的Kafka控制器</a></p>
<h4 id="1-Kafka的控制器Controller是如何被选举出来的？"><a href="#1-Kafka的控制器Controller是如何被选举出来的？" class="headerlink" title="1. Kafka的控制器Controller是如何被选举出来的？"></a>1. Kafka的控制器Controller是如何被选举出来的？</h4><p>每台Broker都能充当控制器，那么，当集群启动后，Kafka怎么确认控制器位于哪台Broker呢？</p>
<p><font color=red>实际上，Broker在启动时，会尝试去Zookeeper中创建 <code>/controller节点</code>。Kafka当前选举控制器的规则是：第一个成功创建<code>/controller节点</code>的Broker会被指定为控制器。</font></p>
<h5 id="控制器的作用"><a href="#控制器的作用" class="headerlink" title="控制器的作用"></a>控制器的作用</h5><ol>
<li><p><strong>主题管理（创建、删除、增加分区）</strong></p>
<ul>
<li>控制器帮助我们完成对Kafka主题的创建、删除、以及分区增加的操作。换句话说，当我们执行 <em><strong>*kafka-topics*</strong></em> 脚本时，大部分的后台工作都是控制器来完成的。</li>
</ul>
</li>
<li><p><strong>分区重分配</strong></p>
<ul>
<li>分区重分配主要是指：<code>kafka-reassign-partition脚本</code>，提供的对已有主题分区进行细粒度的分配功能。这部分功能也是控制器实现的。</li>
</ul>
</li>
<li><p><strong>Preferred领导者选举</strong></p>
</li>
<li><p><strong>集群成员管理（新增Broker， Broker主动关闭、Broker宕机）</strong></p>
<ul>
<li><p>这是控制器提供的第 4 类功能，包括自动检测新增 Broker、Broker 主动关闭及被动宕机。这种自动检测是依赖于前面提到的 Watch 功能和 ZooKeeper 临时节点组合实现的。</p>
<p>比如，控制器组件会利用 ****Watch 机制****检查 ZooKeeper 的 /brokers/ids 节点下的子节点数量变更。目前，当有新 Broker 启动后，它会在 /brokers 下创建专属的 znode 节点。一旦创建完毕，ZooKeeper 会通过 Watch 机制将消息通知推送给控制器，这样，控制器就能自动地感知到这个变化，进而开启后续的新增 Broker 作业。</p>
<p>侦测 Broker 存活性则是依赖于刚刚提到的另一个机制：****临时节点****。每个 Broker 启动后，会在 /brokers/ids 下创建一个临时 znode。当 Broker 宕机或主动关闭后，该 Broker 与 ZooKeeper 的会话结束，这个 znode 会被自动删除。同理，ZooKeeper 的 Watch 机制将这一变更推送给控制器，这样控制器就能知道有 Broker 关闭或宕机了，从而进行“善后”。</p>
</li>
</ul>
</li>
<li><p><strong>数据服务</strong></p>
<ul>
<li>控制器的最后一大类工作，就是向其他Broker提供数据服务。控制器上保存了最全的集群元数据信息，其他所有Broker会定期接收控制器发来的元数据更新请求，从而更新其内存中的缓存数据。</li>
</ul>
</li>
</ol>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/640.jpeg" alt="640?wx_fmt=jpeg"></p>
<h5 id="控制器故障转移"><a href="#控制器故障转移" class="headerlink" title="控制器故障转移"></a>控制器故障转移</h5><p>在 Kafka 集群运行过程中，只能有一台 Broker 充当控制器的角色，那么这就存在****单点失效****（Single Point of Failure）的风险，Kafka 是如何应对单点失效的呢？答案就是，为控制器提供故障转移功能，也就是说所谓的 Failover。</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/640-20210812152030393.jpeg" alt="640?wx_fmt=jpeg"></p>
<p>最开始时，Broker0是控制器，当Broker0宕机后，Zookeeper通过Watch机制感知到并删除了<code>/controller临时节点</code>。之后，所有存活的Broker开始竞选新的控制器身份。Broker3最后抢占成功，成功在Zookeeper上创建了<code>controller节点</code>。之后，Broker3会从Zookeeper中读取集群元数据信息，并初始化到自己的缓存中。</p>
<h4 id="分区Leader的选取"><a href="#分区Leader的选取" class="headerlink" title="分区Leader的选取"></a>分区Leader的选取</h4><p><strong>分区leader副本的选举</strong>由<strong>控制器</strong>负责具实施。当创建分区、分区上线这些时候，都需要执行leader的选举动作。</p>
<p><strong>基本策略</strong></p>
<p>按照<strong>AR</strong>（Kafka topic中一个分区的所有副本）集合中副本的顺序查找第一个存活的副本，并且这个副本在ISR集合中。一个分区的AR集合在分配的时候就被指定，并且只要不发生重分配的情况，集合内部副本的顺序是保持不变的，而分区的ISR集合中副本的顺序可能会改变。</p>
<h2 id="常见面试题"><a href="#常见面试题" class="headerlink" title="常见面试题"></a>常见面试题</h2><ol>
<li>Kafka中的ISR，OSR，AR分别代表什么？<ul>
<li>ISR：与leader 保持同步的follower</li>
<li>OSR：与leader副本同步滞后过多的follower集合</li>
<li>AR：分区的所有副本</li>
</ul>
</li>
<li>Kafka中的HW、LEO等分别代表什么？<ul>
<li>LEO：是log文件末尾的偏移量</li>
<li>HW：一个分区中所有副本最小的offset</li>
</ul>
</li>
<li>Kafka中是怎么体现消息顺序性的。<ul>
<li>生产消息时，每次生产的数据都会追加到log文件的末尾</li>
<li>消费时，消费者会维护一个offset偏移量，根据偏移量就可以保证消息的顺序性</li>
</ul>
</li>
<li>Kafka中的分区器、序列化器、拦截器是否了解？他们之间的处理顺序是什么？<ul>
<li><strong>拦截器</strong></li>
<li><strong>序列化器</strong></li>
<li><strong>分区器</strong></li>
</ul>
</li>
<li>Kafka生产者客户端的整体结构是什么样子的？使用了几个线程来处理？分别是什么？<ul>
<li>包括两个线程：main线程和sender线程</li>
<li>组件包括：Procuder、拦截器。序列化器、分区器、共享变量RecordAccumulator和sender线程</li>
<li><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210812153815055-8753896.png" alt="image-20210812153815055" style="zoom:50%;" /></li>
</ul>
</li>
<li>Kafka哪些情形会造成重复消费？<ul>
<li>先消费信息，然后提交offset（如果当信息消费成功之后，offset提交之前，consumer宕机了）</li>
<li>在consumer恢复的时候，还是会从之前的那个偏移量读取数据，这时就会造成重复消费。</li>
</ul>
</li>
<li>kafka哪些情景会造成消息泄漏消费？<ul>
<li>先提交offset，后消费数据</li>
<li>在offset提交offset完成后，消费数据前，consumer宕机了，就有可能造成消息漏消费</li>
</ul>
</li>
</ol>
<h3 id="零拷贝"><a href="#零拷贝" class="headerlink" title="零拷贝"></a>零拷贝</h3><h3 id="如何提高生产者的吞吐量？"><a href="#如何提高生产者的吞吐量？" class="headerlink" title="如何提高生产者的吞吐量？"></a>如何提高生产者的吞吐量？</h3><ul>
<li>ack设置为0，不等待broker是否落盘成功，生产者继续发送数据。</li>
<li>生产者端配置<code>batch.size</code>参数，一次按照batch发送，默认16KB</li>
<li><code>linger.ms</code>,这个参数控制：当消息不够一个批次的时候，最长等待时间，如果超过这个时间就算每个达到<code>batch.size</code>,也会发送数据到<code>broker </code></li>
</ul>
<p><strong>待补充</strong></p>
<h1 id="Flink复习"><a href="#Flink复习" class="headerlink" title="Flink复习"></a>Flink复习</h1><h2 id="1-WaterMark"><a href="#1-WaterMark" class="headerlink" title="1. WaterMark"></a>1. WaterMark</h2><p>WaterMark主要是缓解数据乱序所带来的的影响。</p>
<h2 id="2-CheckPoint（一致性检查点）"><a href="#2-CheckPoint（一致性检查点）" class="headerlink" title="2. CheckPoint（一致性检查点）"></a>2. CheckPoint（一致性检查点）</h2><ul>
<li>flink故障恢复机制的核心</li>
<li>有状态流应用的一致性检查点，其实就是所有任务的<strong>状态</strong>，在某个时间点的一份快照，这个时间点，应该是所有任务都恰好处理完一个相同的输入数据的时候。</li>
</ul>
<h4 id="检查点保存（Checkpoint）"><a href="#检查点保存（Checkpoint）" class="headerlink" title="检查点保存（Checkpoint）"></a>检查点保存（Checkpoint）</h4><p>做checkpoint的时候，状态后端是按照一组一组进行保存的，当所有的任务都恰好处理完一个相同的输入数据的时候，就将所有任务的<font color=green>当前状态进行保存</font>（<strong>而不是保存正在处理的数据</strong>）。</p>
<p> <img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/src=http://oscimg.oschina.net/oscnet/c15e5cf0-5f5a-48ac-ba14-b3d370a196cb.png&refer=http://oscimg.oschina.net&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg" alt="img"></p>
<p>如上图：状态的保存时机是source，sum_even和sum_odd这三个任务都处理完5这个数据的时间点，将这些任务的状态进行保存。</p>
<p>如果不能保证所有任务都处理完相同的输入，可能会出现以下情况：</p>
<p>source处理完5，但是由于网络原因，4,3都在堵塞中，导致sum_event和sum_odd还没有处理出完3,4,5这三个数据。如果此时发生故障，在进行故障恢复的时候，就会丢失数据。</p>
<h4 id="从检查点恢复状态"><a href="#从检查点恢复状态" class="headerlink" title="从检查点恢复状态"></a>从检查点恢复状态</h4><p><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg2020.cnblogs.com%2Fblog%2F1824311%2F202009%2F1824311-20200908213604870-202910531.png&refer=http%3A%2F%2Fimg2020.cnblogs.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1630159603&t=9156f9309a498e83d6fe0a15fa6dc8e9" alt="img"></p>
<p>如上述图：当处理到7这个数据的时候，sum_odd任务所负责的分区挂了，此时就需要做检查点恢复。</p>
<ul>
<li>在执行流应用程序期间，Flink会定期保存状态的一致检查点。</li>
<li>如果发生故障，Flink将会使用最近的检查点来一致恢复应用程序的状态，并重新启动处理流程。</li>
</ul>
<p>这就需要数据源能够有<font color=red>数据重放的功能</font></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/url=http://dingyue.ws.126.net/2021/0727/1e1f12b7j00qwwn0g002ac000hs00umg.jpg&thumbnail=650x2147483647&quality=80&type=jpg" alt="img"></p>
<ol>
<li>遇到故障之后，第一步就是重启应用</li>
<li>第二步是从checkpoint中读取状态，将状态重置</li>
<li>从检查点重启应用程序后，其内部状态与检查点完成时的状态完全相同。</li>
<li>第三步：开始消费并处理检查点到发生故障之间的所有数据</li>
<li>这种检查点的保存和恢复机制可以为应用程序状态提供（Exactly-one）的一致性，因为所有算子都会保存检查点并恢复其所有状态，这样一来所有的输入流都会被重置到检查点完成时的位置。</li>
</ol>
<p>关于Flink实现Exactly-Once的几点要求：</p>
<ul>
<li>source端支持数据重放的功能，当发生故障的时候，source可以进行故障之前的数据重放</li>
<li>Flink内部通过CheckPoint保证发生故障时，状态的重置。</li>
<li>sink端需要有<strong>幂等性</strong>或者<strong>事务性</strong>（日志预写和两阶段提交2PC）的支持。</li>
</ul>
<h4 id="CheckPoint算法"><a href="#CheckPoint算法" class="headerlink" title="CheckPoint算法"></a>CheckPoint算法</h4><p>其核心就是<font color=red>barrier对齐</font></p>
<p>其本质是基于Chandy-Lamport算法的分布式快照算法的改进。</p>
<p>将检查点的保存和数据处理分开，不暂停整个应用</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/i=20210111152147546.png" alt="在这里插入图片描述"></p>
<ul>
<li>现在是一个两个输入流的应用程序，用并行的两个source任务来读取</li>
</ul>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/i=2021011115251435.png" alt="在这里插入图片描述"></p>
<ul>
<li>JobManager会向每个Source任务发送一条带有检查点ID的消息，通过这种方法来启动检查点</li>
<li>这个消息会传递给下游所有任务</li>
</ul>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/i=20210111153027619.png" alt="在这里插入图片描述"></p>
<ul>
<li>数据源将它们的状态写入检查点，并发出一个检查点barrier，广播发到下游任务。状态后端在状态存入检查点之后，会返回通知给source任务，source任务就会向jobmanager确定检查点完成。</li>
</ul>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/i=20210111153426447.png" alt="在这里插入图片描述"></p>
<ul>
<li><p>分界线对齐（barrier对齐）：barrier向下游传递，sum任务会等待所有输入分区的barrier到达。</p>
</li>
<li><p>对于barrier已经到达的分区，继续接收到的数据会被缓存起来；而barrier尚未到达的分区，数据会被正常处理。</p>
</li>
</ul>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/i=20210111153926877.png" alt="在这里插入图片描述"></p>
<ul>
<li>当接收到所有分区的barrier时，任务就将其状态保存到状态后端的检查点中，然后barrier继续向下游转发。</li>
</ul>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/i=2021011115420988-16300710261841.png" alt="在这里插入图片描述"></p>
<ul>
<li>向下游转发barrier后，任务继续正常的进行数据处理</li>
</ul>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/i=20210111154304922.png" alt="在这里插入图片描述"></p>
<ul>
<li>sink任务想JobManager确认状态保存到checkpoint完毕</li>
<li>当所有任务都确认已经成功将状态保存到检查点时，检查点就真正完成了。</li>
</ul>
<p>每个任务都有相应的状态的检查点，所有任务的状态checkpoint被状态后端state backend保存，当出现故障的时候哦，从此checkpoint就可以恢复任务。</p>
<h4 id="状态后端"><a href="#状态后端" class="headerlink" title="状态后端"></a>状态后端</h4><ul>
<li><p><strong>MemoryStateBackend</strong></p>
<p>内存级的状态后端，会将<strong>键控状态</strong>作为内存中的对象进行管理，将它们存储在TaskManager的JVM堆上 ；而将Checkpoint存储在JobManager的内存中。</p>
</li>
<li><p><strong>FsStateBackend</strong></p>
<p>将checkpoint存到远程的持久化文件系统（FileSystem）上。而对于本地状态，跟MemoryStateBackend一样，也会存在TaskManager的JVM堆上。</p>
</li>
<li><p><strong>RocksDBStateBackend</strong></p>
<p>将所有状态序列化后，存入本地的RocksDB中存储。</p>
</li>
</ul>
<h2 id="3-Flink中的状态一致性"><a href="#3-Flink中的状态一致性" class="headerlink" title="3.Flink中的状态一致性"></a>3.Flink中的状态一致性</h2><h3 id="一致性级别"><a href="#一致性级别" class="headerlink" title="一致性级别"></a>一致性级别</h3><p>在流处理中，一致性可以分为3个级别：</p>
<ul>
<li><strong>at-most-once</strong>: 这其实是没有正确性保证的委婉说法——故障发生之后，计数结果可能丢失，同样的还有UDP。（至多一次）</li>
<li><strong>at-least-once</strong>：这表示计数结果可能大于正确值，但绝不会小于正确值。也就是说，计数程序在发生故障后可能多算，但是绝不会少算。（至少一次）</li>
<li><strong>exactly-once</strong>：这指的是系统保证在发生故障后得到的计数结果与正确值一致。（精确一致）</li>
</ul>
<p>Flink的一个重大价值在于：<strong>它既保证了exactly-once，也具有低延迟和高吞吐的处理能力。</strong></p>
<h4 id="端到端（end-to-end）状态一致性"><a href="#端到端（end-to-end）状态一致性" class="headerlink" title="端到端（end to end）状态一致性"></a>端到端（end to end）状态一致性</h4><ul>
<li>flink内部保证——依赖checkpoint</li>
<li>source端——需要外部数据源可重放数据（可重置数据的读取位置）</li>
<li>sink端——需要保证从故障恢复时，数据不会重复写入外部系统。<ul>
<li>对于sink端，又有两种具体的实现方式：<strong>幂等性写入</strong>和<strong>事务性写入</strong></li>
<li><strong>幂等性写入</strong>：就是说一个操作，可以重复执行很多次，但只导致一次结果更改，也就是说，后面再重复执行就不起作用了。</li>
<li><strong>事务写入</strong>：需要构建事务来写入外部系统，构建的事务对应着checkpoint，等到checkpoint真正完成的时候，才把所有对应的结果写入sink系统中。<ul>
<li>对于事务写入，具体又有两种实现方式：预写日志（WAL）和两阶段提交（2PC）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="4-分布式事务"><a href="#4-分布式事务" class="headerlink" title="4.分布式事务"></a>4.分布式事务</h2><h3 id="1-本地事务"><a href="#1-本地事务" class="headerlink" title="1. 本地事务"></a>1. 本地事务</h3><p>在计算机系统中，更多的是通过关系型数据库来控制事务，这是利用数据库本身的事务特性来实现的，因此叫数据库事务。由于应用主要靠关系数据库来控制事务，而数据库通常和应用在同一个服务器，所以<strong>基于关系型数据库的事务又称为本地事务</strong></p>
<p><u>数据库事务在实现时会将一次事务设计的所有操作全部纳入到一个不可分割的执行单元，执行单元中的所有操作要么成功，要么都失败，只要其中任一操作执行失败，都将导致整个事务的回滚</u></p>
<h3 id="2-分布式事务"><a href="#2-分布式事务" class="headerlink" title="2.分布式事务"></a>2.分布式事务</h3><p>分布式系统会把一个应用系统拆分为可独立部署的多个服务，因此需要服务与服务之间远程协作才能完成事务操作，这种分布式系统环境下由不同的服务之间通过网络远程协作完成事务称之为<strong>分布式事务</strong>，例如用户注册送积分事务，创建订单减少库存事务，银行转账事务等都是分布式事务。</p>
<blockquote>
<p>举个例子</p>
<p>创建订单在一台服务器上；</p>
<p>减少库存在另一台服务器上；</p>
<p>两个服务要通过分布式事务来保证 <strong>一致性</strong></p>
</blockquote>
<ul>
<li>跨数据库示例，也需要分布式事务。</li>
<li>多服务访问同一个数据库实例<ul>
<li>跨JVM进程，也需要分布式事务</li>
</ul>
</li>
</ul>
<h3 id="3-CAP-理论"><a href="#3-CAP-理论" class="headerlink" title="3. CAP 理论"></a>3. CAP 理论</h3><ul>
<li>C一致性</li>
<li>A可用性</li>
<li>分区容错性</li>
</ul>
<h4 id="一致性C"><a href="#一致性C" class="headerlink" title="一致性C"></a>一致性C</h4><p>一致性是指写操作后的读操作可以读取到最新的数据状态，当数据分布在多个节点上，从任意节点读取到的数据都是最新的状态。</p>
<p><font color=green>说白了就是主从之间的数据一致性</font></p>
<p><strong>如何实现一致性？</strong></p>
<ul>
<li>写入主数据库后要将数据同步到从数据库</li>
<li>写入主数据库后，再向从数据库同步期间要将从数据库锁定，待同步完成后再释放锁，以免在新数据写入成功后，向从数据库查询到旧的数据。</li>
</ul>
<p><strong>分布式系统一致性的特点：</strong></p>
<ul>
<li>由于存在数据同步的过程，写操作的相应会有一定的延迟</li>
<li>为了保证数据一致性会对资源暂时锁定，待数据同步完成释放锁定资源</li>
<li>如果请求数据同步失败的节点则会返回错误信息，一定不会返回旧数据。</li>
</ul>
<h4 id="可用性A"><a href="#可用性A" class="headerlink" title="可用性A"></a>可用性A</h4><p>可用性是指任何事物操作都可以得到相应结果，且不会出现响应超时或响应错误。</p>
<p><strong>如何实现可用性？</strong></p>
<ol>
<li>写入主数据库后要将数据同步到从数据库</li>
<li>由于要保证从数据库的可用性，不可将从数据库中的资源进行锁定</li>
<li>即使数据还没有同步过来，从数据库也要返回要查询到的数据，哪怕是旧数据，如果旧数据也没有则可以按照约定返回一个默认信息，但不能返回错误或响应超时。</li>
</ol>
<h4 id="分期容忍性"><a href="#分期容忍性" class="headerlink" title="分期容忍性"></a>分期容忍性</h4><p>通常分布式系统的各个节点部署在不同的子网，这就是网络分区，不可避免的会出现由于网络问题而导致节点之间通信失败，此时仍可对外提供服务，这叫分区容忍性。</p>
<p><strong>如何实现分区容忍性？</strong></p>
<ol>
<li>尽量使用异步代取同步操作，例如使用异步方式将数据从主数据库同步到从数据库，这样节点之间能有效的实现松耦合。</li>
<li>添加从数据库节点，其中一个节点挂了其他从节点提供服务</li>
</ol>
<p><font color=red>分区容忍性是分布式系统的基本能力</font></p>
<h4 id="CAP-组合方式"><a href="#CAP-组合方式" class="headerlink" title="CAP 组合方式"></a>CAP 组合方式</h4><p><strong>在所有分布式事务场景中不会同时具备CAP三个特性，因为在具备P的前提下C和A是不能共存的</strong></p>
<h4 id="BASE理论"><a href="#BASE理论" class="headerlink" title="BASE理论"></a>BASE理论</h4><ol>
<li>强一致性和最终一致性</li>
</ol>
<ul>
<li><p>CP</p>
</li>
<li><p>AP</p>
</li>
</ul>
<p>BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的缩写。BASE理论是对CAP中AP的一个扩展，通过牺牲强一致性来获得可用性，当出现故障允许部分不可用但要保证核心功能可用，允许数据在一段时间内是不一致的，但最终达到一致状态。满足BASE理论的事务，称之为<strong>柔性事务</strong></p>
<h3 id="4-分布式事务解决方案☞2PC（两阶段提交）"><a href="#4-分布式事务解决方案☞2PC（两阶段提交）" class="headerlink" title="4. 分布式事务解决方案☞2PC（两阶段提交）"></a>4. 分布式事务解决方案☞2PC（两阶段提交）</h3><p>针对不同的分布式场景，业界常见的解决方案有2PC，TCC可靠消息最终一致性、最大努力通知这几种</p>
<h4 id="1-什么是2PC"><a href="#1-什么是2PC" class="headerlink" title="1. 什么是2PC"></a>1. 什么是2PC</h4><p>2PC即两阶段提交协议，是将整个事务流程分为两个阶段，<strong>准备阶段（Prepare phase）</strong>、<strong>提交阶段（commit phase）</strong>，2是指两个阶段，P是指准备阶段；C是指提交阶段。</p>
<blockquote>
<p>举例</p>
<p>张三和李四好久不见，老友约起聚餐，饭店老板要求先买单，才能出票。这时张三和李四分别抱怨近况不如意，囊中羞涩，都不愿意请客，这时只能AA。只有张三和李四都付款老板才能出票安排就餐。但由于张三和李四都是铁公鸡，形成了尴尬的一幕：</p>
<ul>
<li>准备阶段：老板要求张三付款，张三付款；老板要求李四付款，李四付款。</li>
<li>提交阶段：老板出票，两人纷纷落座就餐。</li>
</ul>
<p>例子中形成了一个事务，若张三和李四其中一人拒绝付款，或钱不够，店老板都不会给出票，并且会把已收款退回。</p>
</blockquote>
<p>整个事务过程由<font color=red>事务管理器</font>和<font color=red>参与者</font>组成</p>
<p>店老板就是事务管理器；张三和李四就是参与者；事务管理器负责决策整个分布式事务的提交和回滚，事务参与者负责自己本地事务的提交和回滚。</p>
<p>在计算机部分关系数据库如MySql支持两阶段提交协议：</p>
<ol>
<li><p>准备阶段：事务管理器给每个参与者发送Prepare消息，每个数据库参与者在本地执行事务，并写本地的Undo/Redo日志，此时事务没有提交</p>
<p>(Undo日志是记录修改前的数据，用于数据库的回滚；Redo日志是记录修改后的数据，用于提交事务后写入数据文件）</p>
</li>
<li><p>提交阶段：如果<strong>事务管理器</strong>收到了<strong>参与者的执行失败或者超时消息时</strong>，直接给每个参与者发送回滚（Rollback）消息；否则，发送提交（Commit）消息；参与者根据事务管理器的指令执行提交或者回滚操作，并释放事务处理过程中使用的锁资源。<font color=red>注意：必须在最后阶段释放锁资源。</font></p>
</li>
</ol>
<p><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fattach.dataguru.cn%2Fattachments%2Fforum%2F201509%2F20%2F084322huqeon7nkwtakmwq.png&refer=http%3A%2F%2Fattach.dataguru.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1633742797&t=84d69a96c3939ab09772d54e6a7613b6" alt="img"></p>
<p>如果任何一方参与者的本地事务执行失败了，那么事务管理器就会给其他参与者发送<strong>Rollback</strong>指令，是其他参与者进行<strong>回滚</strong>。</p>
<h4 id="2-解决方案XA"><a href="#2-解决方案XA" class="headerlink" title="2. 解决方案XA"></a>2. 解决方案XA</h4><p><strong>DTP模型定义角色</strong></p>
<ul>
<li>AP：应用程序，可以理解为使用DTP分布式事务的程序</li>
<li>RM：资源管理器，可以理解为事务的参与者</li>
<li>TM：事务管理器，负责协调和管理事务，事务管理器控制着全局事务，管理事务生命周期，并协调各个RM。<strong>全局事务</strong>是指分布式处理环境中，需要操作多个数据库共同完成一个工作，这个工作即是一个全局事务。</li>
<li>DTP模型定义TM和RM之间通讯的接口规范叫<strong>XA</strong>，简单理解为数据库提供的2PC接口协议，<strong>基于数据库的XA协议来实现2PC又称为XA方案。</strong></li>
</ul>
<ol>
<li>在<strong>准备阶段</strong>RM执行实际的业务操作，但不提交事务，资源锁定；</li>
<li>在<strong>提交阶段</strong>TM会接受RM在准备阶段的执行回复，只要有任一个RM执行失败，TM会通知所有RM执行回滚操作，否则，TM将会通知所有RM提交该事务。提交阶段结束资源锁释放。</li>
</ol>
<p><strong>XA方案的问题：</strong></p>
<ul>
<li>需要本地数据库支持XA协议</li>
<li>资源所需要等到两个阶段结束才释放，性能较差。</li>
</ul>
<h4 id="3-解决方案Seata方案"><a href="#3-解决方案Seata方案" class="headerlink" title="3.解决方案Seata方案"></a>3.解决方案Seata方案</h4><p>Seata是阿里中间件团队发起的开源项目Fescar，后更名为Seata，他是一个开源的分布式事务框架。</p>
<p>传统的2PC的问题在Seata中得到了解决，它通过对本地关系数据库的分支事务的协调来驱动完成全局事务，是工作在应用层的中间件。主要优点是性能较好，且不长时间占用连接资源，它以高效并且对业务0侵入的方式解决微服务场景下面临的分布式事务问题，他目前提供AT模式（即2PC）及TCC模式的分布式事务解决方案。</p>
<p><strong>Seata的设计思想如下：</strong></p>
<p>Seata的设计目标是其一是对业务无侵入，因此从业务侵入的2PC方案着手，在传统2PC的基础上演进，并讲解2PC方案面临的问题。</p>
<p>Seata把一个分布式事务理解成一个包含了若干<strong>分支事务</strong>的<strong>全局事务</strong>。全局事务的职责是协调其下管辖的分支事务达成一致，要么一起成功提交，要么一起失败回滚。此外，通常分支事务本身就是一个关系数据库的本地事务，下图全局事务与分支事务的关系图：</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210909102257596-16311541787475.png" alt="image-20210909102257596"></p>
<p>与传统2PC的模型类似，Seata定义了3个组件来协议分布式事务的处理过程</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210909102436921.png" alt="image-20210909102436921"></p>
<ul>
<li>Transaction Coordinate（TC）：<strong>事务协调器</strong>，它是独立的中间件，需要独立部署运行，它维护全局事务的运行状态，接收TM指令发起全局事务的提交与回滚，负责与RM通信协调各个分支事务的提交或回滚。</li>
<li>Transaction Manager（TM）：<strong>事务管理器</strong>，TM需要嵌入应用程序中工作，它负责开启一个全局事务，并最终向<strong>TC</strong>发起<strong>全局提交或全局回滚的指令</strong>。</li>
<li>Resource Manager（RM）：<strong>控制分支事务</strong>，负责分支注册、状态汇报，并且接收事务协调器TC的指令，驱动分支（本地）事务的提交和回滚。</li>
</ul>
<p>Seata方案的每个本地或者分支事务在执行完毕之后就执行提交操作，回滚的时候，删除数据的方式进行回滚。</p>
<p><strong>Seata实现2PC与传统的2PC的差别</strong></p>
<p>架构层次方面，传统2PC方案的RM实际上是在数据库层，RM本质上就是数据库自身，通过XA协议实现，而Seata的RM是以jar包的形式作为中间件层部署在应用程序这一侧的。</p>
<p>而两阶段提交方面，传统2PC无论第二阶段的决议是commit还是rollback，事务性资源的锁都要保持到Phase2完成才释放。而Seata的做法是在Phase1就将本地事务提交，这样就可省去Phase2持锁的时间，整体提高效率。</p>
<h1 id="数仓复习"><a href="#数仓复习" class="headerlink" title="数仓复习"></a>数仓复习</h1><h2 id="数仓理论"><a href="#数仓理论" class="headerlink" title="数仓理论"></a>数仓理论</h2><h3 id="待补充。。。"><a href="#待补充。。。" class="headerlink" title="待补充。。。"></a>待补充。。。</h3><h3 id="路径分析-与-桑基图"><a href="#路径分析-与-桑基图" class="headerlink" title="路径分析  与  桑基图"></a>路径分析  与  桑基图</h3><blockquote>
<p>路径分析:</p>
<p>​    是指用户在APP中或者网站中的<strong>访问路径</strong>。为了衡量网站<strong>优化的效果</strong>或者<strong>营销推广的效果</strong>，以及了解<strong>用户行为偏好</strong>，时常要对访问路径进行分析。</p>
</blockquote>
<blockquote>
<p>桑基图：</p>
<p>​    是用于描述一组值到另一组值的流向的图表。比如下图的人口流动情况</p>
<img src="source/大数据学习之路/v2-5d7aee3655ceb6be8625ddb58e1724bd_r.jpg" alt="img" style="zoom:100%;" />

<p>边的左侧节点表示起始点：<strong>Source</strong></p>
<p>边的右侧节点表示终止点：<strong>Target</strong></p>
<p>桑基图需要提供每种页面，节点的跳转次数。每个流向或者跳转由 source 与 target 表示。</p>
<ul>
<li>source：跳转起始页面（<strong>不允许为空</strong>）</li>
<li>target：跳转终到页面（<strong>可为空</strong>）</li>
</ul>
<p>❌<strong>桑基图所展示的流程中不允许存在环</strong></p>
</blockquote>
<p>😨<strong>source不允许为空的解决方案</strong></p>
<p>通常情况下我们以直观简单的思想去定义 source和target的时候，可能会存在source为null的业务场景，例如下述sql，将last_page_id作为source就存在null的情况：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	session_id</span><br><span class="line">    ,last_page_id 	source</span><br><span class="line">    ,page_id 		target</span><br><span class="line"><span class="keyword">from</span> dwd_page_log</span><br><span class="line"><span class="keyword">where</span> dt<span class="operator">=</span><span class="string">&#x27;2020-06-14&#x27;</span></span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>source</th>
<th>target</th>
</tr>
</thead>
<tbody><tr>
<td><font color=red>null</font></td>
<td>home</td>
</tr>
<tr>
<td>home</td>
<td>search</td>
</tr>
<tr>
<td>search</td>
<td>good_list</td>
</tr>
<tr>
<td>good_list</td>
<td>good_detail</td>
</tr>
<tr>
<td>good_detail</td>
<td>login</td>
</tr>
<tr>
<td>login</td>
<td>good_detail</td>
</tr>
<tr>
<td>good_detail</td>
<td>cart</td>
</tr>
<tr>
<td>cart</td>
<td>trade</td>
</tr>
<tr>
<td>trade</td>
<td>payment</td>
</tr>
</tbody></table>
<p>这种情况我们可以转变思路，可以将 <strong>page_id作为 source</strong>， <strong>当前行的下一行的page_id作为target</strong>，这样就避免了source为null的情况了。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	session_id</span><br><span class="line">	,page_id														source</span><br><span class="line">	,<span class="built_in">lead</span>(page,<span class="number">1</span>,<span class="keyword">null</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> session_id <span class="keyword">order</span> <span class="keyword">by</span> ts) 	target</span><br><span class="line"><span class="keyword">from</span> dwd_page_log</span><br><span class="line"><span class="keyword">where</span> dt<span class="operator">=</span><span class="string">&#x27;2020-06-14&#x27;</span></span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>source</th>
<th>target</th>
</tr>
</thead>
<tbody><tr>
<td>home</td>
<td>search</td>
</tr>
<tr>
<td>search</td>
<td>good_list</td>
</tr>
<tr>
<td>good_list</td>
<td>good_detail</td>
</tr>
<tr>
<td>good_detail</td>
<td>login</td>
</tr>
<tr>
<td>login</td>
<td>good_detail</td>
</tr>
<tr>
<td>good_detail</td>
<td>cart</td>
</tr>
<tr>
<td>cart</td>
<td>trade</td>
</tr>
<tr>
<td>trade</td>
<td>payment</td>
</tr>
<tr>
<td>payment</td>
<td><font color=red>null</font></td>
</tr>
</tbody></table>
<p>虽然解决了source不为null的问题，但是观察上述数据，<code>good_detail -&gt; login, login -&gt; good_detail</code> 存在环，此时可以为每次跳转增加跳转序号，根据跳转序号为每个页面增加页面序号。</p>
<p>预期结果：</p>
<table>
<thead>
<tr>
<th>source</th>
<th>target</th>
<th>step</th>
</tr>
</thead>
<tbody><tr>
<td>home1️⃣</td>
<td>search2️⃣</td>
<td>1</td>
</tr>
<tr>
<td>search2️⃣</td>
<td>good_list3️⃣</td>
<td>2</td>
</tr>
<tr>
<td>good_list3️⃣</td>
<td>good_detail4️⃣</td>
<td>3</td>
</tr>
<tr>
<td>good_detail4️⃣</td>
<td>login5️⃣</td>
<td>4</td>
</tr>
<tr>
<td>login5️⃣</td>
<td>good_detail6️⃣</td>
<td>5</td>
</tr>
<tr>
<td>good_detail6️⃣</td>
<td>cart7️⃣</td>
<td>6</td>
</tr>
<tr>
<td>cart7️⃣</td>
<td>trade8️⃣</td>
<td>7</td>
</tr>
<tr>
<td>trade8️⃣</td>
<td>payment9️⃣</td>
<td>8</td>
</tr>
<tr>
<td>payment9️⃣</td>
<td><font color=red>null</font></td>
<td>9</td>
</tr>
<tr>
<td>home1️⃣</td>
<td>mine2️⃣</td>
<td>1</td>
</tr>
<tr>
<td>mine2️⃣</td>
<td>orders_unpaid3️⃣</td>
<td>2</td>
</tr>
<tr>
<td>orders_unpaid3️⃣</td>
<td>good_detail4️⃣</td>
<td>3</td>
</tr>
<tr>
<td>good_detail4️⃣</td>
<td>good_spec5️⃣</td>
<td>4</td>
</tr>
<tr>
<td>good_spec5️⃣</td>
<td>comment6️⃣</td>
<td>5</td>
</tr>
<tr>
<td>comment6️⃣</td>
<td>trade7️⃣</td>
<td>6</td>
</tr>
<tr>
<td>trade7️⃣</td>
<td>payment8️⃣</td>
<td>7</td>
</tr>
<tr>
<td>payment8️⃣</td>
<td><font color=red>null</font></td>
<td>8</td>
</tr>
</tbody></table>
<p>这样 环<code>good_detail -&gt; login, login -&gt; good_detail</code> 就变成了<code>good_detail4 -&gt; login5, login5 -&gt; good_detail6</code>, 环就不复存在啦！😊</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 解决环的问题</span></span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        concat(source,<span class="string">&#x27;：&#x27;</span>,step) 	source <span class="comment">-- 其实页面添加id</span></span><br><span class="line">        ,concat(target,<span class="string">&#x27;：&#x27;</span>,step<span class="operator">+</span><span class="number">1</span>) 	target <span class="comment">-- 终止页面添加id</span></span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">    (</span><br><span class="line">        <span class="comment">-- 当前查询可将每个会话的跳转明细查询出来，包括页面的跳转次序</span></span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">            session_id</span><br><span class="line">            ,page_id source</span><br><span class="line">            ,<span class="built_in">lead</span>(page_id,<span class="number">1</span>,<span class="keyword">null</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> session_id <span class="keyword">order</span> <span class="keyword">by</span> ts) target  <span class="comment">-- 当前行的下一行page_id 作为target，没有下一行默认值为null</span></span><br><span class="line">            ,<span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> session_id <span class="keyword">order</span> <span class="keyword">by</span> ts) step <span class="comment">-- 每次会话的跳转序号</span></span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">        (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">                last_page_id</span><br><span class="line">                ,page_id</span><br><span class="line">                ,concat(mid_id,<span class="string">&#x27;-&#x27;</span>,<span class="built_in">last_value</span>(if(last_page_id <span class="keyword">is</span> <span class="keyword">null</span>,ts,<span class="keyword">null</span>),<span class="literal">true</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> mid_id <span class="keyword">order</span> <span class="keyword">by</span> ts)) session_id</span><br><span class="line">                ,ts</span><br><span class="line">            <span class="keyword">from</span> dwd_page_log</span><br><span class="line">            <span class="keyword">where</span> dt<span class="operator">=</span><span class="string">&#x27;2020-06-14&#x27;</span></span><br><span class="line">        ) t1</span><br><span class="line">    ) t2</span><br></pre></td></tr></table></figure>







<h2 id="开发小技巧"><a href="#开发小技巧" class="headerlink" title="开发小技巧"></a>开发小技巧</h2><h3 id="区分-维度-与-指标"><a href="#区分-维度-与-指标" class="headerlink" title="区分 维度 与 指标"></a>区分 维度 与 指标</h3><p>在进行指标的开发时，总会涉及到很多个维度字段，在开发时，首先就要区分这些<strong>维度字段</strong> 与 <strong>指标字段</strong>，然后先不考虑维度问题<code>换句话说就是：所有维度的汇总值</code>，<font color=red>先把所有维度的总汇总指标值计算出来之后，再通过group by 维度字段 来进行维度的细分！</font>，这样做会比直接考虑维度要简单的多。</p>
<blockquote>
<p>举个例子：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> IF <span class="keyword">EXISTS</span> ads_visit_stats;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> ads_visit_stats (</span><br><span class="line">  `dt` STRING COMMENT <span class="string">&#x27;统计日期&#x27;</span>,</span><br><span class="line">  `is_new` STRING COMMENT <span class="string">&#x27;新老标识,1:新,0:老&#x27;</span>,</span><br><span class="line">  `recent_days` <span class="type">BIGINT</span> COMMENT <span class="string">&#x27;最近天数,1:最近1天,7:最近7天,30:最近30天&#x27;</span>,</span><br><span class="line">  `channel` STRING COMMENT <span class="string">&#x27;渠道&#x27;</span>,</span><br><span class="line">  `uv_count` <span class="type">BIGINT</span> COMMENT <span class="string">&#x27;日活(访问人数)&#x27;</span>,</span><br><span class="line">  `duration_sec` <span class="type">BIGINT</span> COMMENT <span class="string">&#x27;页面停留总时长&#x27;</span>,</span><br><span class="line">  `avg_duration_sec` <span class="type">BIGINT</span> COMMENT <span class="string">&#x27;一次会话，页面停留平均时长,单位为描述&#x27;</span>,</span><br><span class="line">  `page_count` <span class="type">BIGINT</span> COMMENT <span class="string">&#x27;页面总浏览数&#x27;</span>,</span><br><span class="line">  `avg_page_count` <span class="type">BIGINT</span> COMMENT <span class="string">&#x27;一次会话，页面平均浏览数&#x27;</span>,</span><br><span class="line">  `sv_count` <span class="type">BIGINT</span> COMMENT <span class="string">&#x27;会话次数&#x27;</span>,</span><br><span class="line">  `bounce_count` <span class="type">BIGINT</span> COMMENT <span class="string">&#x27;跳出数&#x27;</span>,</span><br><span class="line">  `bounce_rate` <span class="type">DECIMAL</span>(<span class="number">16</span>,<span class="number">2</span>) COMMENT <span class="string">&#x27;跳出率&#x27;</span></span><br><span class="line">) COMMENT <span class="string">&#x27;访客统计&#x27;</span></span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">LOCATION <span class="string">&#x27;/warehouse/gmall/ads/ads_visit_stats/&#x27;</span>;</span><br></pre></td></tr></table></figure>


</blockquote>
<p>区分上述表中的维度与指标</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>dt</th>
<th>统计时间</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>is_new</td>
<td>新老标识</td>
</tr>
<tr>
<td></td>
<td>recent_days</td>
<td>最近天数</td>
</tr>
<tr>
<td></td>
<td>channel</td>
<td>渠道</td>
</tr>
<tr>
<td><strong>指标</strong></td>
<td>uv_count</td>
<td>日活</td>
</tr>
<tr>
<td></td>
<td>duration_sec</td>
<td>页面停留时长（所有会话）</td>
</tr>
<tr>
<td></td>
<td>avg_duration_sec</td>
<td>一次会话，页面停留平均时长</td>
</tr>
<tr>
<td></td>
<td>page_count</td>
<td>页面总浏览数（所有会话）</td>
</tr>
<tr>
<td></td>
<td>avg_page_count</td>
<td>一次会话，页面平均浏览数</td>
</tr>
<tr>
<td></td>
<td>sv_count</td>
<td>会话次数</td>
</tr>
<tr>
<td></td>
<td>bounce_count</td>
<td>跳出数：一次会话中只浏览一个页面</td>
</tr>
<tr>
<td></td>
<td>bounce_rate</td>
<td>跳出数/会话次数</td>
</tr>
</tbody></table>
<ol>
<li>先不考虑维度，计算出总的会话指标值（最近一天的数据）</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="built_in">count</span>(<span class="keyword">distinct</span> mid_id) uv_count</span><br><span class="line">    ,<span class="built_in">sum</span>(duration)<span class="operator">/</span><span class="number">1000</span> duration_sec</span><br><span class="line">    ,<span class="built_in">avg</span>(duration)<span class="operator">/</span><span class="number">1000</span> avg_duration_sec</span><br><span class="line">    ,<span class="built_in">sum</span>(page_count) page_count</span><br><span class="line">    ,<span class="built_in">avg</span>(page_count) avg_page_count</span><br><span class="line">    ,<span class="built_in">count</span>(<span class="keyword">distinct</span> session_id) sv_count</span><br><span class="line">    ,<span class="built_in">sum</span>(if(page_count<span class="operator">=</span><span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>)) bounce_count</span><br><span class="line">    ,<span class="built_in">sum</span>(if(page_count<span class="operator">=</span><span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>)) <span class="operator">/</span> <span class="built_in">count</span>(<span class="keyword">distinct</span> session_id) bounce_rate</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        session_id <span class="comment">-- 每次会话</span></span><br><span class="line">        ,mid_id</span><br><span class="line">        ,<span class="built_in">count</span>(page_id) page_count</span><br><span class="line">        ,<span class="built_in">sum</span>(during_time)<span class="operator">/</span><span class="number">1000</span> duration</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">select</span> mid_id       <span class="comment">-- 设备id</span></span><br><span class="line">             , last_page_id <span class="comment">-- 上一页id</span></span><br><span class="line">             , page_id      <span class="comment">-- 本页id</span></span><br><span class="line">             , during_time  <span class="comment">-- 本页面停留时间</span></span><br><span class="line">             , ts           <span class="comment">-- 跳入当前页面的时间</span></span><br><span class="line">            , concat(mid_id,<span class="string">&#x27;-&#x27;</span>,<span class="built_in">last_value</span>(session_start_point,<span class="literal">true</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> mid_id <span class="keyword">order</span> <span class="keyword">by</span> ts)) session_id</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">        (</span><br><span class="line">            <span class="keyword">select</span> mid_id       <span class="comment">-- 设备id</span></span><br><span class="line">                 , last_page_id <span class="comment">-- 上一页id</span></span><br><span class="line">                 , page_id      <span class="comment">-- 本页id</span></span><br><span class="line">                 , during_time  <span class="comment">-- 本页面停留时间</span></span><br><span class="line">                 , ts           <span class="comment">-- 跳入当前页面的时间</span></span><br><span class="line">                ,if(last_page_id <span class="keyword">is</span> <span class="keyword">null</span>, ts, <span class="keyword">null</span>) session_start_point</span><br><span class="line">            <span class="keyword">from</span> dwd_page_log</span><br><span class="line">            <span class="keyword">where</span> dt <span class="operator">=</span> <span class="string">&#x27;2020-06-14&#x27;</span></span><br><span class="line">        ) t1</span><br><span class="line">    ) t2</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> session_id,mid_id</span><br><span class="line">) t3</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>添加维度</p>
<ol>
<li><p>添加渠道维度</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    channel</span><br><span class="line">    ,<span class="built_in">count</span>(<span class="keyword">distinct</span> mid_id) uv_count</span><br><span class="line">    ,<span class="built_in">sum</span>(duration)<span class="operator">/</span><span class="number">1000</span> duration_sec</span><br><span class="line">    ,<span class="built_in">avg</span>(duration)<span class="operator">/</span><span class="number">1000</span> avg_duration_sec</span><br><span class="line">    ,<span class="built_in">sum</span>(page_count) page_count</span><br><span class="line">    ,<span class="built_in">avg</span>(page_count) avg_page_count</span><br><span class="line">    ,<span class="built_in">count</span>(<span class="keyword">distinct</span> session_id) sv_count</span><br><span class="line">    ,<span class="built_in">sum</span>(if(page_count<span class="operator">=</span><span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>)) bounce_count</span><br><span class="line">    ,<span class="built_in">sum</span>(if(page_count<span class="operator">=</span><span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>)) <span class="operator">/</span> <span class="built_in">count</span>(<span class="keyword">distinct</span> session_id) bounce_rate</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        session_id <span class="comment">-- 每次会话</span></span><br><span class="line">         ,channel</span><br><span class="line">        ,mid_id</span><br><span class="line">        ,<span class="built_in">count</span>(page_id) page_count</span><br><span class="line">        ,<span class="built_in">sum</span>(during_time)<span class="operator">/</span><span class="number">1000</span> duration</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">select</span> mid_id       <span class="comment">-- 设备id</span></span><br><span class="line">             , last_page_id <span class="comment">-- 上一页id</span></span><br><span class="line">             , page_id      <span class="comment">-- 本页id</span></span><br><span class="line">             , during_time  <span class="comment">-- 本页面停留时间</span></span><br><span class="line">             , ts           <span class="comment">-- 跳入当前页面的时间</span></span><br><span class="line">             ,channel</span><br><span class="line">            , concat(mid_id,<span class="string">&#x27;-&#x27;</span>,<span class="built_in">last_value</span>(session_start_point,<span class="literal">true</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> mid_id <span class="keyword">order</span> <span class="keyword">by</span> ts)) session_id</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">        (</span><br><span class="line">            <span class="keyword">select</span> mid_id       <span class="comment">-- 设备id</span></span><br><span class="line">                 , last_page_id <span class="comment">-- 上一页id</span></span><br><span class="line">                 , page_id      <span class="comment">-- 本页id</span></span><br><span class="line">                 , during_time  <span class="comment">-- 本页面停留时间</span></span><br><span class="line">                 , ts           <span class="comment">-- 跳入当前页面的时间</span></span><br><span class="line">                 , channel      <span class="comment">-- 渠道</span></span><br><span class="line">                ,if(last_page_id <span class="keyword">is</span> <span class="keyword">null</span>, ts, <span class="keyword">null</span>) session_start_point</span><br><span class="line">            <span class="keyword">from</span> dwd_page_log</span><br><span class="line">            <span class="keyword">where</span> dt <span class="operator">=</span> <span class="string">&#x27;2020-06-14&#x27;</span></span><br><span class="line">        ) t1</span><br><span class="line">    ) t2</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> session_id,mid_id,channel</span><br><span class="line">) t3</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> channel</span><br></pre></td></tr></table></figure></li>
<li><p>以此类推，添加其他维度字段，实现，将总的数据进行维度的划分。</p>
</li>
</ol>
</li>
</ol>
<h3 id="同时求-近1天，近7天，-近30天的指标值（通过一个字段的不同取值来区分）"><a href="#同时求-近1天，近7天，-近30天的指标值（通过一个字段的不同取值来区分）" class="headerlink" title="同时求 近1天，近7天， 近30天的指标值（通过一个字段的不同取值来区分）"></a>同时求 近1天，近7天， 近30天的指标值（通过一个字段的不同取值来区分）</h3><p>当我们在一张表中同时求近1天，近7天和近30天的指标累积值时，一般有两种思路：</p>
<ol>
<li><p>先按近1天的逻辑进行分区过滤，计算指标值，然后将相同的逻辑拷贝出3份，每份按照相应的分区进行过滤，包括近1天的分区，近7天的分区，近30天的分组，然后将这三部分的数据最终进行<code>union all</code>操作，即可完成。</p>
<blockquote>
<p>这种方式虽然比较直接，但是同样的逻辑需要复制出3份，当修改逻辑代码时，需要同时修改3份代码，容易出错。而且代码不简洁！</p>
</blockquote>
</li>
<li><p>可以通过UDTF函数（一近多出）的方式将基础表中的数据进行扩容，分别生成近1天，近7天和近30天的这3类数据，然后通过对不同类别的数据分别进行统计，完成指标值的计算。</p>
<blockquote>
<p>![未命名文件 (1)](source/大数据学习之路/未命名文件 (1).png)</p>
</blockquote>
</li>
</ol>
<p>样例代码：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    TODO 会话的划分</span></span><br><span class="line"><span class="comment">            每个mid_id 的上一页page_id为null的时候就是每次会话的开始</span></span><br><span class="line"><span class="comment">            会话划分，新增一个字段，表示会话的id ： mid_id  +  会话开始时间</span></span><br><span class="line"><span class="comment">        over() 中 存在order by， 则窗口中的数据行数是  上边界 到 当前行。</span></span><br><span class="line"><span class="comment">        last_value() 接收两个参数：一个是要选择的字段， 另一个参数是一个bool类型的，false：表示不跳过null， true表示跳过空值</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="string">&#x27;2020-06-14&#x27;</span>                                               dt               <span class="comment">-- 统计日期</span></span><br><span class="line">     , is_new                                                                      <span class="comment">-- 新老标识</span></span><br><span class="line">     , recent_days                                                                 <span class="comment">-- 最近天数</span></span><br><span class="line">     , channel                                                                     <span class="comment">-- 渠道</span></span><br><span class="line">     , <span class="built_in">count</span>(<span class="keyword">distinct</span> mid_id)                                     uv_count         <span class="comment">-- 日活</span></span><br><span class="line">     , <span class="built_in">sum</span>(duration) <span class="operator">/</span> <span class="number">1000</span>                                       duration_sec     <span class="comment">-- 页面停留总时长</span></span><br><span class="line">     , <span class="built_in">avg</span>(duration) <span class="operator">/</span> <span class="number">1000</span>                                       avg_duration_sec <span class="comment">-- 一次会话 页面停留平均时长</span></span><br><span class="line">     , <span class="built_in">sum</span>(page_count)                                            page_count       <span class="comment">-- 页面浏览总数</span></span><br><span class="line">     , <span class="built_in">avg</span>(page_count)                                            avg_page_count   <span class="comment">-- 每次会话，平均页面浏览数</span></span><br><span class="line">     , <span class="built_in">count</span>(<span class="keyword">distinct</span> session_id)                                 sv_count         <span class="comment">-- 会话次数</span></span><br><span class="line">     , <span class="built_in">sum</span>(if(page_count <span class="operator">=</span> <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>))                              bounce_count     <span class="comment">-- 跳出数</span></span><br><span class="line">     , <span class="built_in">sum</span>(if(page_count <span class="operator">=</span> <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)) <span class="operator">/</span> <span class="built_in">count</span>(<span class="keyword">distinct</span> session_id) bounce_rate      <span class="comment">-- 跳出率</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">         <span class="keyword">select</span> session_id <span class="comment">-- 每次会话</span></span><br><span class="line">              , channel</span><br><span class="line">              , recent_days</span><br><span class="line">              , is_new</span><br><span class="line">              , mid_id</span><br><span class="line">              , <span class="built_in">count</span>(page_id)          page_count</span><br><span class="line">              , <span class="built_in">sum</span>(during_time) <span class="operator">/</span> <span class="number">1000</span> duration</span><br><span class="line">         <span class="keyword">from</span> (</span><br><span class="line">                  <span class="keyword">select</span> t1.mid_id    <span class="comment">-- 设备id</span></span><br><span class="line">                       , last_page_id <span class="comment">-- 上一页id</span></span><br><span class="line">                       , page_id      <span class="comment">-- 本页id</span></span><br><span class="line">                       , during_time  <span class="comment">-- 本页面停留时间</span></span><br><span class="line">                       , ts           <span class="comment">-- 跳入当前页面的时间</span></span><br><span class="line">                       , channel</span><br><span class="line">                       , recent_days</span><br><span class="line">                       , if(t0.visit_date_first <span class="operator">&gt;=</span> date_add(<span class="string">&#x27;2020-06-14&#x27;</span>, <span class="operator">-</span>recent_days <span class="operator">+</span> <span class="number">1</span>), <span class="number">1</span>, <span class="number">0</span>)                   is_new</span><br><span class="line">                       , concat(t1.mid_id, <span class="string">&#x27;-&#x27;</span>, <span class="built_in">last_value</span>(session_start_point, <span class="literal">true</span>)</span><br><span class="line">                                                           <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> t1.recent_days,t1.mid_id <span class="keyword">order</span> <span class="keyword">by</span> ts)) session_id</span><br><span class="line">                  <span class="keyword">from</span> (</span><br><span class="line">                           <span class="keyword">select</span> mid_id       <span class="comment">-- 设备id</span></span><br><span class="line">                                , last_page_id <span class="comment">-- 上一页id</span></span><br><span class="line">                                , page_id      <span class="comment">-- 本页id</span></span><br><span class="line">                                , during_time  <span class="comment">-- 本页面停留时间</span></span><br><span class="line">                                , ts           <span class="comment">-- 跳入当前页面的时间</span></span><br><span class="line">                                , channel      <span class="comment">-- 渠道</span></span><br><span class="line">                                , recent_days  <span class="comment">-- 最近天数</span></span><br><span class="line">                                , if(last_page_id <span class="keyword">is</span> <span class="keyword">null</span>, ts, <span class="keyword">null</span>) session_start_point</span><br><span class="line">                           <span class="keyword">from</span> dwd_page_log</span><br><span class="line">                                    <span class="keyword">lateral</span> <span class="keyword">view</span> explode(<span class="keyword">array</span>(<span class="number">1</span>, <span class="number">7</span>, <span class="number">30</span>)) tmp <span class="keyword">as</span> recent_days  <span class="comment">-- 此处 通过UDTF函数将原始数据 扩容为原来的3倍</span></span><br><span class="line">                           <span class="keyword">where</span> dt <span class="operator">&gt;=</span> date_add(<span class="string">&#x27;2020-06-14&#x27;</span>, <span class="operator">-</span>recent_days <span class="operator">+</span> <span class="number">1</span>)</span><br><span class="line">                       ) t1</span><br><span class="line">                           <span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">                       (</span><br><span class="line">                           <span class="keyword">select</span> mid_id</span><br><span class="line">                                , visit_date_first</span><br><span class="line">                           <span class="keyword">from</span> dwt_visitor_topic</span><br><span class="line">                           <span class="keyword">where</span> dt <span class="operator">=</span> <span class="string">&#x27;2020-06-14&#x27;</span></span><br><span class="line">                       ) t0</span><br><span class="line">                       <span class="keyword">on</span> t1.mid_id <span class="operator">=</span> t0.mid_id</span><br><span class="line">              ) t2</span><br><span class="line">         <span class="keyword">group</span> <span class="keyword">by</span> session_id, mid_id, channel, is_new, recent_days</span><br><span class="line">     ) t3</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> channel, is_new, recent_days</span><br></pre></td></tr></table></figure>













<h1 id="网络复习"><a href="#网络复习" class="headerlink" title="网络复习"></a>网络复习</h1><h2 id="1-OSI的七层参考模型"><a href="#1-OSI的七层参考模型" class="headerlink" title="1. OSI的七层参考模型"></a>1. OSI的七层参考模型</h2><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/1439096-20201115232936579-1750711590.png" alt="img"></p>
<table>
<thead>
<tr>
<th>应用层</th>
<th>访问网络服务的接口（DNS）</th>
<th>单位</th>
</tr>
</thead>
<tbody><tr>
<td>表示层</td>
<td>提供数据格式转换服务</td>
<td></td>
</tr>
<tr>
<td>会话层</td>
<td>建立端连接并提供访问验证和会话管理</td>
<td></td>
</tr>
<tr>
<td>传输层</td>
<td>提供应用进程之间的逻辑通信<br />常见：TCP、UDP、进程、端口（socket）</td>
<td>数据段（segment）</td>
</tr>
<tr>
<td>网络层</td>
<td>为数据在节点之间传输创建逻辑链路，并分组转发数据<br />例如：对子网间的数据包进行路由选择<br />常见：路由器、多层交换机、防火墙</td>
<td>分组（数据包）（packet）</td>
</tr>
<tr>
<td>数据链路层</td>
<td>在通信的实体间建立数据链路连接<br />例如：将数据分帧，并处理流控制、物理地址寻址、重发等<br />常见：网卡，网桥，二层交换机等。</td>
<td>帧（frame）</td>
</tr>
<tr>
<td>物理层</td>
<td>为数据端设备提供原始比特流（01）的传输大的通路<br />网络通信的传输介质，由电缆与设备共同构成<br />常见：中继器，集线器，网线等</td>
<td>比特（bit）</td>
</tr>
</tbody></table>
<p>7层参考模型是一个标准，而非实现。</p>
<p><strong>五层参考模型</strong></p>
<p>物理层，数据链路层，网络层，传输层，应用层</p>
<h2 id="2-一次完整的Http请求过程"><a href="#2-一次完整的Http请求过程" class="headerlink" title="2. 一次完整的Http请求过程"></a>2. 一次完整的Http请求过程</h2><p>第一种回答：</p>
<ol>
<li>建立客户机与服务器连接</li>
<li>建立连接后，客户机发送一个请求给服务器</li>
<li>服务器收到请求给予相应信息</li>
<li>客户端浏览器将返回的内容解析并呈现，断开连接。</li>
</ol>
<p>第二种回答：</p>
<ol>
<li>DNS域名解析</li>
<li>发起TCP的3次握手</li>
<li>建立TCP连接后发起http请求</li>
<li>服务器响应http请求，浏览器得到html代码</li>
<li>浏览器解析html代码，并请求html代码中的资源（如js、css、图片等）</li>
<li>浏览器对页面进行渲染呈现给用户</li>
</ol>
<h2 id="3-DNS"><a href="#3-DNS" class="headerlink" title="3. DNS"></a>3. DNS</h2><p>DNS：域名系统，因特网上作为<code>域名</code>和<code>IP地址</code>相互映射的一个分布式数据库,能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。</p>
<p>通过主机名，最终得到该主机名对应的ip地址的过程叫做域名解析（或主机名解析）</p>
<h3 id="3-1-DNS的工作原理"><a href="#3-1-DNS的工作原理" class="headerlink" title="3.1 DNS的工作原理"></a>3.1 DNS的工作原理</h3><p>将主机域名转换为ip地址，属于应用层协议，使用UDP传输。</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/70.jpeg" alt="这里写图片描述"></p>
<p>域名到IP地址的解析过程的要点如下：</p>
<ol>
<li>当某一个应用需要把主机名解析为IP地址时，该应用进程就调用解析程序，并称为DNS的一个客户，把待解析的域名放在DNS请求报文中，以<strong>UDP</strong>用户数据报方式发给本地域名服务器。</li>
<li>本地域名服务器在查找域名后，把对应的IP地址放在回答报文中返回。应用程序获得目的主机的IP地址后即可进行通信。</li>
<li>若本地域名服务器不能回答该请求，则此域名服务器就暂时称为DNS的另一个客户，并向其他域名服务器发出查询请求。</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/mocas_wang/article/details/109167660">参考文献</a></p>
<ul>
<li>请求一旦发起，若是chrome浏览器，现在浏览器找之前看看有<strong>没有缓存过的域名对应的ip地址</strong>，有的话，直接跳过DNS解析，若是没有，就会找<strong>硬盘的hosts文件</strong>，有的话，直接找到hosts文件里面的ip</li>
<li>如果本地的hosts文件没有得到对应的ip地址，浏览器会发出一个<code>dns请求到本地DNS服务器,本地dns服务器一般都是你的网络接入服务器商提供</code>，比如中国移动、中国电信等。</li>
<li>查询你输入的网址的DNS请求到本地DNS服务器之后，<strong>本地DNS服务器会首先查询它的缓存记录</strong>，如果缓存中由此条记录，就可以直接返回结果，此过程是<strong>递归的方式进行查询</strong>。如果没有，<strong>本地DNS服务器还要向DNS根服务器</strong>进行查询。</li>
<li>本地DNS服务器继续向域服务器发出请求，在这个例子中，请求的对象是.com域服务器。.com域服务器收到请求之后，也不会直接返回域名和IP地址的对应关系，而是告诉本地DNS服务器，你的域名的解析服务器的地址。</li>
<li>最后，本地DNS服务器向<strong>域名的解析服务器</strong>发出请求，这时就能收到一个域名和IP地址对应关系，本地DNS服务器不仅要把IP地址返回给用户电脑，还要把这个对应关系保存在缓存中，以备下次别的用户查询时，可以直接返回结果，加快网络访问。</li>
</ul>
<h2 id="4-Http长连接与短连接"><a href="#4-Http长连接与短连接" class="headerlink" title="4. Http长连接与短连接"></a>4. Http长连接与短连接</h2><ul>
<li>在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作（实际上就是TCP），就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如JavaScript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。</li>
<li>使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这条已经建立的连接。<code>Keep-Alive</code>不会永久保持连接，它有一个保持时间，可以在不同的服务器软件中设置这个时间。实现长连接需要客户端和服务端都支持长连接。</li>
</ul>
<p>HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。</p>
<h2 id="5-HTTP请求方法"><a href="#5-HTTP请求方法" class="headerlink" title="5. HTTP请求方法"></a>5. HTTP请求方法</h2><table>
<thead>
<tr>
<th>序 号</th>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>GET</td>
<td>请求指定的页面信息，并返回实体主体。</td>
</tr>
<tr>
<td>2</td>
<td>HEAD</td>
<td>类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头</td>
</tr>
<tr>
<td>3</td>
<td>POST</td>
<td>向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST 请求可能会导致新的资源的建立和/或已有资源的修改。</td>
</tr>
<tr>
<td>4</td>
<td>PUT</td>
<td>从客户端向服务器传送的数据取代指定的文档的内容。</td>
</tr>
<tr>
<td>5</td>
<td>DELETE</td>
<td>请求服务器删除指定的页面。</td>
</tr>
<tr>
<td>6</td>
<td>CONNECT</td>
<td>HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。</td>
</tr>
<tr>
<td>7</td>
<td>OPTIONS</td>
<td>允许客户端查看服务器的性能。</td>
</tr>
<tr>
<td>8</td>
<td>TRACE</td>
<td>回显服务器收到的请求，主要用于测试或诊断。</td>
</tr>
<tr>
<td>9</td>
<td>PATCH</td>
<td>是对 PUT 方法的补充，用来对已知资源进行局部更新 。</td>
</tr>
</tbody></table>
<h3 id="GET-与-POST的区别？"><a href="#GET-与-POST的区别？" class="headerlink" title="GET 与 POST的区别？"></a>GET 与 POST的区别？</h3><ol>
<li>get把请求的数据放在url上，参数之间以&amp;相连，所以get不太安全；post把数据放在HTTP的包体内（request body）</li>
<li>get提交数据最大是2k（限制实际上取决于浏览器），post理论上没有限制。</li>
<li>GET请求会被浏览器主动缓存，而POST不会，除非手动设置。</li>
</ol>
<h2 id="6-HTTPS和HTTP的区别"><a href="#6-HTTPS和HTTP的区别" class="headerlink" title="6. HTTPS和HTTP的区别"></a>6. HTTPS和HTTP的区别</h2><ol>
<li>HTTP协议传输的数据都是未加密的，也就是明文的，因此使用HTTP协议传输隐私信息非常不安全，HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全。</li>
<li>https协议需要到CA申请证书，一般免费证书较少，因而需要一定费用。</li>
<li>http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443</li>
</ol>
<h2 id="7-什么是RARP？工作原理"><a href="#7-什么是RARP？工作原理" class="headerlink" title="7. 什么是RARP？工作原理"></a>7. 什么是RARP？工作原理</h2><p>概括：反向地址转换协议，网络层协议，<code>RARP</code>与<code>ARP</code>工作方式相反。RARP使只知道自己硬件地址的主机能够知道IP的地址。RARP发出要反向解释的物理地址并希望返回其IP地址，应答包括能够提供所需信息的RARP服务器发出的IP地址。</p>
<p><strong>原理</strong></p>
<ol>
<li>往上上的每台设备都会有一个独一无二的硬件地址，通常是由设备厂商分配的MAC地址。主机从网卡上读取MAC地址，然后在网络上发送一个<code>RARP</code>请求的广播数据包,请求RARP服务器回复该主机的IP地址</li>
<li>RARP服务器收到RARP请求数据包，为其分配IP地址，并将RARP回应发送给主机</li>
<li>PC1收到RARP回应后，就使用得到的IP地址进行通讯。</li>
</ol>
<h2 id="8-什么是ARP？"><a href="#8-什么是ARP？" class="headerlink" title="8. 什么是ARP？"></a>8. 什么是ARP？</h2><p><code>ARP</code>地址解析协议，其基本功能为透过目标设备的IP地址，查询目标设备的MAC地址，以保证通信的顺利进行。它是IPV4中网络层必不可少的协议，不过在IPV6中已不再适用，并被邻居发现协议（NDP）所替代。</p>
<p><strong>工作流程</strong></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/70.png" alt="这里写图片描述"></p>
<p>假设主机A和B在同一个网段，主机A要向主机B发送信息，具体的地址解析过程如下：<br>(1)  主机A首先查看自己的ARP表，确定其中是否包含有主机B对应的ARP表项。如果找到了对应的MAC地址，则主机A直接利用ARP表中的MAC地址，对IP数据包进行帧封装，并将数据包发送给主机B。</p>
<p>(2) 如果主机A在ARP表中找不到对应的MAC地址，则将缓存该数据报文，然后以广播方式发送一个ARP请求报文。ARP请求报文中的发送端IP地址和发送端MAC地址为主机A的IP地址和MAC地址，目标IP地址和目标MAC地址为主机B的IP地址和全0的MAC地址。由于ARP请求报文以广播方式发送，该网段上的所有主机都可以接收到该请求，但只有被请求的主机（即主机B）会对该请求进行处理。</p>
<p>(3) 主机B比较自己的IP地址和ARP请求报文中的目标IP地址，当两者相同时进行如下处理：将ARP请求报文中的发送端（即主机A）的IP地址和MAC地址存入自己的ARP表中。之后以单播方式发送ARP响应报文给主机A，其中包含了自己的MAC地址。</p>
<p>(4) 主机A收到ARP响应报文后，将主机B的MAC地址加入到自己的ARP表中以用于后续报文的转发，同时将IP数据包进行封装后发送出去。</p>
<h2 id="9-TCP-头部信息"><a href="#9-TCP-头部信息" class="headerlink" title="9. TCP 头部信息"></a>9. TCP 头部信息</h2><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/20160406120904661.png" alt="这里写图片描述"></p>
<ul>
<li><p><strong>序号</strong>（32bit）：传输方向上字节流的字节编号。初始时序号会被设置一个随机的初始值（ISN），之后每次发送数据时，<code>序号值=ISN+数据在整个字节流中的偏移</code>。</p>
<p>假设A-&gt;B且ISN=1024，第一段数据512字节已经到B，则第二段数据发送时序号为1024+512.用于解决网络包乱序问题。</p>
</li>
<li><p><strong>确认号</strong>（32bit）：接收方对发送方TCP报文段的响应，其值是收到的序号值+1.（ack）</p>
</li>
<li><p><strong>首部长</strong>（4bit）：标识首部有多少个4字节*首部长，最大为15，即60字节。</p>
</li>
<li><p><strong>标志位</strong>（6bit）：</p>
<ul>
<li>URG：标志紧急指针是否有效</li>
<li><strong>ACK</strong>：标志确认号是否有效（确认报文段）。用于解决丢包问题。</li>
<li>PSH：提示接收端立即从缓冲读走数据。</li>
<li>RST：表示要求对方重新建立连接（复位报文段）。</li>
<li><strong>SYN</strong>：表示请求建立一个连接（连接报文段）。</li>
<li><strong>FIN</strong>：表示关闭连接（断开报文段）。</li>
</ul>
</li>
<li><p><strong>窗口</strong>（16bit）：接收窗口。用于告知对方（发送方）本方的缓冲还能接收多少字节数据。用于解决流控。</p>
</li>
<li><p><strong>检验和</strong>（16bit）：接收端用CRC检验整个报文段有无损失。</p>
</li>
</ul>
<p><strong>常见TCP的连接状态有哪些？</strong></p>
<ul>
<li><strong>CLOSED</strong>：初始状态</li>
<li><strong>LISTEN</strong>：服务器处于监听状态。</li>
<li><strong>SYN_SEND</strong>：客户端socket执行connect连接，发送SYN包，进入此状态。</li>
<li><strong>SYN_RECV</strong>：服务端收到SYN包并发送服务端SYN包，进入此状态。</li>
<li><strong>ESTABLISH</strong>：表示连接建立。客户端发送了最后一个ACK包后进入此状态，服务端接收到ACK包后进入此状态。</li>
<li><strong>FIN_WAIT_1</strong>：终止连接的一方（通常是客户机），发送了FIN报文后进入此状态，等待对方FIN。</li>
<li><strong>CLOSE_WAIT</strong>：（假设服务器）接收到客户机FIN包之后等待关闭的阶段。在接收到对方的FIN包之后，自然是需要立即回复ACK包的，表示已经知道断开请求。但是本方是否立即断开连接（发送FIN包）取决于是否还有数据需要发送给客户端，若有，则在发送FIN包之前均为此状态。</li>
<li><strong>FIN_WAIT_2</strong>：此时是半连接状态，即有一方要求关闭连接，等待另一方关闭。客户端接收到服务器的ACK包，但并没有立即接收到服务端的FIN包，进入FIN_WAIT_2状态。</li>
<li><strong>LAST_ACK</strong>：服务端发动最后的FIN包，等待最后的客户端ACK响应，进入此状态。</li>
<li><strong>TIME_WAIT</strong>：客户端收到服务端的FIN包，并立即发出ACK包做最后的确认，在此之后的2MSL时间称为TIME_WAIT状态。</li>
</ul>
<h2 id="10-TCP-三次握手🤝与四次挥手👋🏻"><a href="#10-TCP-三次握手🤝与四次挥手👋🏻" class="headerlink" title="10. TCP 三次握手🤝与四次挥手👋🏻"></a>10. TCP 三次握手🤝与四次挥手👋🏻</h2><p><strong>三次握手</strong></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.png"></p>
<h2 id="课程沉淀"><a href="#课程沉淀" class="headerlink" title="课程沉淀"></a>课程沉淀</h2><h3 id="1-物理层"><a href="#1-物理层" class="headerlink" title="1. 物理层"></a>1. 物理层</h3><p><strong>物理层的作用：</strong></p>
<ul>
<li>连接不同的物理设备</li>
<li>传输比特流（<strong>比特</strong>是物理层的基本单位）</li>
</ul>
<p><strong>信道</strong></p>
<ul>
<li>单工信道：单向发送</li>
<li>半双工信道：同一时间只能单向发送</li>
<li>全双工信道：同一时间可以双向发送</li>
</ul>
<p><strong>分用-复用技术：</strong></p>
<ul>
<li>频分复用</li>
<li>时分复用</li>
<li>波分复用</li>
<li>码分复用</li>
</ul>
<h3 id="2-数据链路层"><a href="#2-数据链路层" class="headerlink" title="2. 数据链路层"></a>2. 数据链路层</h3><ul>
<li><p>封装成帧</p>
</li>
<li><p>帧：是数据链路层数据的基本单位</p>
</li>
<li><p>透明传输</p>
<ul>
<li>“透明“</li>
<li>控制字符在帧数据中，但是要当做不存在的去处理</li>
<li><code>SOH  --&gt;  EOT --&gt; EOT</code> 通过转义字符转换帧头、帧尾</li>
</ul>
</li>
<li><p>差错监测</p>
<ul>
<li>物理层只管传输比特流，无法控制是否出错</li>
<li>数据链路层负责起”差错监测“的工作</li>
<li><strong>奇偶校验码</strong><ul>
<li>如果出错位置为偶数，那么就无法判断是否出错</li>
</ul>
</li>
<li><strong>CRC</strong><ul>
<li>一种根据传输或保存的数据而<strong>产生固定位数校验码</strong>的方法</li>
<li><strong>检测</strong>数据传输或者保存后可能出现的错误</li>
<li>生成的数字计算出来并且附加到<strong>数据后面</strong></li>
</ul>
</li>
</ul>
<p><strong>CRC计算</strong></p>
</li>
</ul>
<img src="source/2021年8月份秋招复习笔记/image-20210826145958572-16299612004811-16299612155912.png" alt="image-20210826145958572" style="zoom:50%;" />

<p>最高阶是几，就在后面填几个0；</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210826150352497-16299614343404.png" alt="image-20210826150352497"></p>
<p>模2运算后的余数，添加到原始串的末尾</p>
<p>发送端会计算出上述的位串，发送给接收端，接收端也会做模2运算，来对比最终得到的余数是否为0，来判断信息在传输过程中，是否出错了。</p>
<p><strong>CRC的错误检测能力与位串的阶数r有关</strong></p>
<p>数据链路层只进行数据的检测，不进行纠正。</p>
<img src="source/2021年8月份秋招复习笔记/image-20210826150649365-16299616136935.png" alt="image-20210826150649365" style="zoom:50%;" />

<h3 id="3-网络层"><a href="#3-网络层" class="headerlink" title="3. 网络层"></a>3. 网络层</h3><table>
<thead>
<tr>
<th>应用层</th>
<th>为计算机用户提供接口和服务</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>表示层</td>
<td>数据处理（编码解码、加密解密）</td>
<td></td>
</tr>
<tr>
<td>会话层</td>
<td>管理（建立、维护、重连）通信会话</td>
<td></td>
</tr>
<tr>
<td>传输层</td>
<td>管理端到端的通信连接</td>
<td></td>
</tr>
<tr>
<td>网络层</td>
<td>数据路由（决定数据在网络的路径）</td>
<td></td>
</tr>
<tr>
<td>数据链路层</td>
<td>管理相邻节点之间的数据通信</td>
<td></td>
</tr>
<tr>
<td>物理层</td>
<td>数据通信的光电物理特性</td>
<td></td>
</tr>
</tbody></table>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210826152556164-16299627579376.png" alt="image-20210826152556164"></p>
<h4 id="IP协议详解"><a href="#IP协议详解" class="headerlink" title="IP协议详解"></a>IP协议详解</h4><ul>
<li>Ip协议使得复杂的实际网络变为一个虚拟互连的网络</li>
<li>IP协议使得网络层可以屏蔽底层细节而专注网络层的数据转发</li>
<li><font color=red>IP协议解决了在虚拟网络中数据报传输路径的问题</font></li>
</ul>
<p><strong>IP协议：</strong></p>
<ul>
<li>IP地址长度为32位，常分成4个8位</li>
<li>IP地址常使用点分十进制来表示（0<del>255. 0</del>255. 0<del>255. 0</del>255）</li>
</ul>
<table>
<thead>
<tr>
<th>协议名</th>
<th>字段值</th>
</tr>
</thead>
<tbody><tr>
<td>ICMP</td>
<td>1</td>
</tr>
<tr>
<td>IGMP</td>
<td>2</td>
</tr>
<tr>
<td>IP</td>
<td>4</td>
</tr>
<tr>
<td>TCP</td>
<td>6</td>
</tr>
<tr>
<td>UDP</td>
<td>17</td>
</tr>
<tr>
<td>OSPF</td>
<td>89</td>
</tr>
</tbody></table>
<h4 id="IP协议的转发流程"><a href="#IP协议的转发流程" class="headerlink" title="IP协议的转发流程"></a>IP协议的转发流程</h4><ul>
<li><strong>逐跳</strong>（hop-by-hop）</li>
</ul>
<h5 id="1-路由表简介"><a href="#1-路由表简介" class="headerlink" title="1. 路由表简介"></a>1. 路由表简介</h5><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830141834926-16303043159223.png" alt="image-20210830141834926"></p>
<ul>
<li>A通过网卡发出数据帧</li>
<li>数据帧到达路由器，路由器取出前6字节</li>
<li>路由器匹配MAC地址表，找到对应的网络接口</li>
<li>路由器往该网络接口发送数据帧</li>
</ul>
<table>
<thead>
<tr>
<th>目的IP地址</th>
<th>下一跳IP地址</th>
</tr>
</thead>
<tbody><tr>
<td>IP1</td>
<td>IP4</td>
</tr>
<tr>
<td>IP2</td>
<td>IP5</td>
</tr>
<tr>
<td>。。。</td>
<td>。。。</td>
</tr>
</tbody></table>
<h5 id="2-IP协议的转发流程"><a href="#2-IP协议的转发流程" class="headerlink" title="2.IP协议的转发流程"></a>2.IP协议的转发流程</h5><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830141932789.png" alt="image-20210830141932789"></p>
<p>​    网络层：</p>
<ul>
<li>A发出目的地为C的IP数据报，<font color=red>查询路由表发现下一跳为E</font></li>
<li>A将数据报发送给E</li>
<li><font color=red>E查询路由表发现下一步跳为F</font>，将数据报发送给F</li>
<li><font color=red>F查询路由表发现目的地C直接连接</font>，将数据报发送给C</li>
</ul>
<p>数据链路层：</p>
<ul>
<li>A发出目的地为C的IP数据报，查询路由表发现下一跳为E</li>
<li>A将IP数据报交给数据链路层，并告知目的MAC地址是E</li>
<li>数据链路层填充源MAC地址和目的MAC地址E</li>
<li>数据链路层通过物理层将数据发送给E</li>
</ul>
<ul>
<li>E的数据链路层接收到数据帧，<font color=red>把帧数据交给网络层</font></li>
<li>E<font color=red>查询路由表，发现下一跳为F</font></li>
<li>E<font color=red>把数据报交给数据链路层，并告知目的MAC地址为F</font></li>
<li>E的数据链路层<font color=red>封装数据帧并发送</font></li>
</ul>
<ul>
<li>F的数据链路层接收到数据帧，把<font color=red>帧数据交给网络层</font></li>
<li>F<font color=red>查询路由表，发现下一跳为C</font></li>
<li>F<font color=red>把数据报交给数据链路层</font>，<font color=red>并告知目的MAC地址为C</font></li>
<li>F的数据链路层<font color=red>封装数据帧并发送</font></li>
</ul>
<p>整个过程中：</p>
<ol>
<li>数据帧每一跳的MAC地址都在变化</li>
<li>IP数据报每一跳的IP地址始终不变</li>
</ol>
<h4 id="ARP协议和RARP协议"><a href="#ARP协议和RARP协议" class="headerlink" title="ARP协议和RARP协议"></a>ARP协议和RARP协议</h4><h5 id="ARP地址解析协议"><a href="#ARP地址解析协议" class="headerlink" title="ARP地址解析协议"></a>ARP地址解析协议</h5><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830143707374-16303054283854.png" alt="image-20210830143707374"></p>
<p><strong>ARP缓存表</strong></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830143741682-16303054635275.png" alt="image-20210830143741682"></p>
<ul>
<li>ARP缓存表是ARP协议和RARP协议运行的关键</li>
<li>ARP缓存表缓存了IP地址到硬件地址之间的映射关系</li>
<li>ARP缓存表中的记录并不是永久有效的，有一定的期限</li>
</ul>
<img src="source/2021年8月份秋招复习笔记/image-20210830144740667.png" alt="image-20210830144740667" style="zoom:80%;" />



<p>当ARP缓存表中<font color=green>没有对应的IP地址与MAC地址的映射</font>的时候</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830144246148.png" alt="image-20210830144246148"></p>
<ul>
<li>E检查MAC地址表，发现没有C的信息</li>
<li>E将广播A的数据包到除A以外的端口</li>
<li>E将收到来自B、C的回应，并将地址记录</li>
</ul>
<h5 id="RARP逆地址解析协议"><a href="#RARP逆地址解析协议" class="headerlink" title="RARP逆地址解析协议"></a>RARP逆地址解析协议</h5><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830145003625.png" alt="image-20210830145003625"></p>
<p>将MAC地址转换成IP地址的协议</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830141932789.png" alt="image-20210830141932789"></p>
<ul>
<li>A发出目的地为C的IP数据报，<font color=red>查询路由表发现下一跳为E</font></li>
<li>A将IP数据报<font color=red>交给数据链路层(查询ARP缓存表，得到目的MAC地址)，并告知目的MAC地址是E</font></li>
<li>数据链路层<font color=red>填充源MAC地址</font>A和<font color=red>目的MAC地址E</font></li>
<li>数据链路层通过物理层将数据发送给E</li>
</ul>
<ul>
<li>E的数据链路层接收到数据帧，<font color=red>把帧数据交给网络层</font></li>
<li>E<font color=red>查询路由表(RARP 根据MAC地址，查询出IP地址)，发现下一跳为F</font></li>
<li>E<font color=red>把数据报交给数据链路层，并告知目的MAC地址为F</font></li>
<li>E的数据链路层<font color=red>封装数据帧并发送</font></li>
</ul>
<h4 id="IP地址的子网划分"><a href="#IP地址的子网划分" class="headerlink" title="IP地址的子网划分"></a>IP地址的子网划分</h4><h5 id="1-分类的IP地址"><a href="#1-分类的IP地址" class="headerlink" title="1. 分类的IP地址"></a>1. 分类的IP地址</h5><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830150452674.png" alt="image-20210830150452674"></p>
<p>IP地址可分为两个部分：<strong>网络号</strong>、<strong>主机号</strong></p>
<p>根据网络与主机号所占位数的不同，可以将IP地址分成：A类地址、B类地址、C类地址三种</p>
<table>
<thead>
<tr>
<th>分类</th>
<th>特点</th>
</tr>
</thead>
<tbody><tr>
<td>A类地址</td>
<td>网络号：8位；主机号：24位；首位为0</td>
</tr>
<tr>
<td>B类地址</td>
<td>网络号：16位；主机号：16位；首位为10</td>
</tr>
<tr>
<td>C类地址</td>
<td>网络号：24位；主机号：8位；首位为110</td>
</tr>
</tbody></table>
<img src="source/2021年8月份秋招复习笔记/image-20210830150719139.png" alt="image-20210830150719139" style="zoom:80%;" />



<table>
<thead>
<tr>
<th></th>
<th>最小网络号</th>
<th>最大网络号</th>
<th>子网数</th>
<th>最小主机号</th>
<th>最大主机号</th>
<th>主机数量</th>
</tr>
</thead>
<tbody><tr>
<td>A</td>
<td>0<br />(0000 0000)</td>
<td>127<br />(0111 1111)</td>
<td>2^7</td>
<td>0.0.0</td>
<td>255.255.255</td>
<td>2^24</td>
</tr>
<tr>
<td>B</td>
<td>128.0<br />(1000 0000.|<br />0000 0000)</td>
<td>191.255<br />(1011 1111.<br />1111 1111)</td>
<td>2^14</td>
<td>0.0<br /></td>
<td>255.255</td>
<td>2^16</td>
</tr>
<tr>
<td>C</td>
<td>192.0.0<br />(1100 0000.<br />0000 0000<br />)</td>
<td>223.255.255</td>
<td>2^21</td>
<td>0</td>
<td>255</td>
<td>2^8</td>
</tr>
</tbody></table>
<h6 id="特殊的主机号"><a href="#特殊的主机号" class="headerlink" title="特殊的主机号"></a>特殊的主机号</h6><ul>
<li>主机号全0表示当前网络段，不可分配为特定主机</li>
<li>主机号为全1表示<strong>广播地址</strong>，向当前网络段所有主机发送消息</li>
</ul>
<blockquote>
<p>举个例子：</p>
<p>1.2.3.4：A类地址</p>
<p>1.0.0.0：网络段</p>
<p>1.255.255.255：广播地址</p>
</blockquote>
<p>特殊地址不能被分配给主机的。</p>
<h6 id="特殊的网络号"><a href="#特殊的网络号" class="headerlink" title="特殊的网络号"></a>特殊的网络号</h6><ul>
<li><p>A类地址<font color=red>网络段全0（0000 0000） 表示特殊网络</font></p>
</li>
<li><p>A类地址<font color=red>网络段后7位全1（0111 1111:127）表示回环地址</font></p>
</li>
<li><p>B类地址<font color=red>网络段（1000 0000.0000 0000：:128.0）是不可使用的</font></p>
</li>
<li><p>C类地址<font color=red>网络段（192.0.0）</font>是不可使用的</p>
</li>
</ul>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210830153927092.png" alt="image-20210830153927092"></p>
<p>如何判断一个IP地址是哪一类的IP地址呢？</p>
<blockquote>
<p>将网络号的前8位取出来，转化成二进制，通过判断前3个二进制位，来判断</p>
<p>0：A类</p>
<p>10：B类</p>
<p>110：C类</p>
</blockquote>
<h5 id="2-划分子网"><a href="#2-划分子网" class="headerlink" title="2. 划分子网"></a>2. 划分子网</h5><h5 id="3-无分类编址CIDR"><a href="#3-无分类编址CIDR" class="headerlink" title="3. 无分类编址CIDR"></a>3. 无分类编址CIDR</h5><h1 id="操作系统复习"><a href="#操作系统复习" class="headerlink" title="操作系统复习"></a>操作系统复习</h1><h1 id="Redis复习"><a href="#Redis复习" class="headerlink" title="Redis复习"></a>Redis复习</h1><h2 id="1-常用数据类型"><a href="#1-常用数据类型" class="headerlink" title="1.  常用数据类型"></a>1.  常用数据类型</h2><h3 id="String"><a href="#String" class="headerlink" title="String"></a>String</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set key value</span><br><span class="line">get key</span><br><span class="line"></span><br><span class="line">setnx key  value  # 当key不存在的时候，才创建此key的数据</span><br></pre></td></tr></table></figure>

<h3 id="List"><a href="#List" class="headerlink" title="List"></a>List</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">lpush：相当于从左侧压栈，rpush相当于从右侧压栈</span><br><span class="line">lrange：相当于从左侧开始读元素。没有rrange</span><br><span class="line"></span><br><span class="line">lpop：从list左侧移除第一个元素；rpop从右侧移除第一个元素</span><br><span class="line"></span><br><span class="line">lindex list 1：获取list从左侧开始的索引为1的值</span><br><span class="line"></span><br><span class="line">llen list：获取list长度</span><br><span class="line">lrem list count（1,2，3.） value：从list移除指定的值，如果有重复数据，可以指定移除的个数 。精确匹配</span><br></pre></td></tr></table></figure>



<h3 id="set"><a href="#set" class="headerlink" title="set"></a>set</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sadd myset &quot;hello&quot;</span><br><span class="line">smembers myset</span><br><span class="line">sismember myset &quot;hashdf&quot;</span><br><span class="line">srem myset &quot;hello&quot; # 移除集合中指定的元素</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Hash"><a href="#Hash" class="headerlink" title="Hash"></a>Hash</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hset myhash name ha # 创建hash myhash 并指定field： name    value：ha</span><br><span class="line">hget myhash name</span><br></pre></td></tr></table></figure>

<h3 id="Zset"><a href="#Zset" class="headerlink" title="Zset"></a>Zset</h3><p>有序集合</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; zadd salary 2500 xiaohong 5000 zhangsan 500 xiaozhang # 有序集合添加元素，需要指定每个元素的权重</span><br><span class="line">127.0.0.1:6379&gt; ZRANGE salary 0 -1 withscores # 查看元素，可以指定查询结果是否带权重</span><br></pre></td></tr></table></figure>

<h3 id="BitMap位图"><a href="#BitMap位图" class="headerlink" title="BitMap位图"></a>BitMap位图</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">使用bitmap来记录 周一至周日的打卡</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">周一：1 周二：0 周三：0 周四：1…………</span></span><br><span class="line">127.0.0.1:6379&gt; setbit sign 0 1</span><br><span class="line">(integer) 0</span><br><span class="line">127.0.0.1:6379&gt; setbit sign 1 0</span><br><span class="line">(integer) 0</span><br><span class="line">127.0.0.1:6379&gt; setbit sign 2 0</span><br><span class="line">(integer) 0</span><br><span class="line">127.0.0.1:6379&gt; setbit sign 3 1</span><br><span class="line">(integer) 0</span><br><span class="line">127.0.0.1:6379&gt; setbit sign 4 1</span><br><span class="line">(integer) 0</span><br><span class="line">127.0.0.1:6379&gt; setbit sign 5 0</span><br><span class="line">(integer) 0</span><br><span class="line">127.0.0.1:6379&gt; setbit sign 6 0</span><br><span class="line">(integer) 0</span><br><span class="line">====================</span><br><span class="line">127.0.0.1:6379&gt; getbit sign 3 # 查看周四是否打卡</span><br><span class="line">(integer) 1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="language-bash"><span class="comment">####################</span></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">统计 打卡的天数</span></span><br><span class="line">127.0.0.1:6379&gt; bitcount sign 0 6 #统计周一至周日的打开天数</span><br><span class="line">(integer) 3</span><br></pre></td></tr></table></figure>



<h2 id="2-Redis持久化"><a href="#2-Redis持久化" class="headerlink" title="2. Redis持久化"></a>2. Redis持久化</h2><p>Redis是内存数据库，如果不将内存中的数据库状态保存到磁盘，那么一旦服务器进程退出，服务器中的数据库状态也会消失。所以Redis提供了持久化功能！</p>
<h3 id="RDB（Redis-DataBase）"><a href="#RDB（Redis-DataBase）" class="headerlink" title="RDB（Redis DataBase）"></a>RDB（Redis DataBase）</h3><p>在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的Snapshot快照，它恢复时将快照文件直接读到内存里。</p>
<p>Redis会单独创建（<code>fork</code>）一个进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO操作的。这就确保了极高的性能。如果需要进行大规模的数据恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加高效。<code>RDB的缺点是最后一次持久化后的数据可能会丢失</code>。<strong>RDB保存的文件是<code>dump.rdb</code></strong></p>
<p><strong>优点：</strong></p>
<ol>
<li>适合大规模的数据恢复</li>
<li>对数据的完整性和一致性要求不高</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>在一定时间间隔做一次备份，所以如果redis意外宕机的话，就会丢失最后一次快照的所有修改</li>
<li>fork的时候，内存中的数据被克隆了一份，大致2倍的内存膨胀。</li>
</ol>
<p><strong>RDB持久化触发规则</strong></p>
<ol>
<li>一分钟内改了1万次</li>
<li>5分钟内改了10次</li>
<li>15分钟内改了1次</li>
</ol>
<h3 id="AOF（Append-Only-File）"><a href="#AOF（Append-Only-File）" class="headerlink" title="AOF（Append Only File）"></a>AOF（Append Only File）</h3><p>AOF是以日志的形式来记录每个写操作，将redis执行过的所有写指令记录下来（读操作不记录），只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis重启的话就根据日志文件的内容将写执行从前到后执行一遍，以完成数据的恢复工作。</p>
<p>AOF保存的是<code>appendonly.aof</code></p>
<p><strong>注意：</strong></p>
<p><code>appendonly.aof</code>与<code>dump.rdb</code>两者是可以共存的，但是，当redis启动的时候，会先加载aof文件恢复数据。</p>
<p><strong>Appendfsync：数据同步策略</strong></p>
<ol>
<li>Always：同步持久化 每次发生数据变更会被立即记录到磁盘， 这种同步方式性能较差但数据完整性比较好</li>
<li>Everysec： 出厂默认推荐， 异步操作，每秒记录 如果一秒内宕机，有数据丢失</li>
<li>No：不进行数据同步</li>
</ol>
<p><strong>AOF优势</strong></p>
<ul>
<li>每修改同步：appendfsync always 同步持久化，每次发生数据变更会被立即记录到磁盘，性能较差但是数据完整性比较高</li>
<li>每秒同步： appendfsync everysec 异步操作，每秒记录 ，如果一秒内宕机，有数据丢失</li>
<li>不同步：appendfsync no 从不同步</li>
</ul>
<p><strong>劣势</strong></p>
<ul>
<li>对于相同数据集的数据而言，aof文件要远大于rdb文件，恢复速度慢于rdb</li>
<li>AOF运行效率要慢于rdb，每秒同步策略效率较好，不同步效率和rdb相同</li>
</ul>
<h2 id="3-哨兵Sentine模式（反客为主的自动版）"><a href="#3-哨兵Sentine模式（反客为主的自动版）" class="headerlink" title="3. 哨兵Sentine模式（反客为主的自动版）"></a>3. 哨兵Sentine模式（反客为主的自动版）</h2><p>主从切换技术的方法是：当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费时费力，还会造成一段时间内服务不可用。Redis2.8开始正式提供了 <strong>Sentinel</strong>（哨兵）架构来解决这个问题</p>
<p>哨兵模式下，能后后台监视主机是否宕机故障，如果故障了根据投票数自动将从库切换成主库。</p>
<p>哨兵模式是一种特殊的模式，首先Redis提供了哨兵的命令，哨兵是一个独立的进程，作为进程，他会独立的运行。<strong>其原理是：哨兵通过发送命令，等待Redis服务器响应，从而监控运行的多个Redis实例。</strong></p>
<img src="source/2021年8月份秋招复习笔记/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16289931893091" alt="在这里插入图片描述" style="zoom:50%;" />

<p><strong>哨兵的两个作用</strong></p>
<ul>
<li>通过发送命令，让Redis服务器返回监控其运行状态，包括主服务器和从服务器</li>
<li>当哨兵检测到master宕机，会自动将slave切换到master，然后通过<strong>发布订阅模式</strong>通知其他的从服务器，修改配置文件，让他们切换主机</li>
</ul>
<p>然而一个哨兵进程对Redis服务器进行监控，可能会出现问题，为此，我们可以使用多个哨兵进行监控。各个哨兵之间还会进行监控，这样就形成了多哨兵模式。</p>
<img src="source/2021年8月份秋招复习笔记/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3Mjc2ODM1,size_16,color_FFFFFF,t_70#pic_center-16289933054913" alt="在这里插入图片描述" style="zoom:50%;" />



<h2 id="4-Redis-中-Key的过期淘汰机制"><a href="#4-Redis-中-Key的过期淘汰机制" class="headerlink" title="4. Redis 中 Key的过期淘汰机制"></a>4. Redis 中 Key的过期淘汰机制</h2><h3 id="定期删除"><a href="#定期删除" class="headerlink" title="定期删除"></a>定期删除</h3><p>Redis默认是每隔100ms就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除</p>
<p>为什么是随机而不是检查所有key？因为如果你设置的key成千上万，没100ms豆浆所有存在的key检查一遍，会给cpu带来较大的压力。</p>
<h3 id="惰性删除"><a href="#惰性删除" class="headerlink" title="惰性删除"></a>惰性删除</h3><p>定期删除由于是随机抽取可能会导致很多过期key到了过期时间并没有被删除。</p>
<p>所以用户在从缓存获取数据的时候，redis会检查这个key是否过期了，如果过期就删除这个key，这时候就会将过期key从缓存中清除</p>
<h2 id="5-内存淘汰机制"><a href="#5-内存淘汰机制" class="headerlink" title="5. 内存淘汰机制"></a>5. 内存淘汰机制</h2><p>如果仅仅使用**<code>定期删除+惰性删除</code>**机制还是会留下一个严重的隐患：如果定期删除保留下了很多已经过期的key，而且用户长时间都没有使用过这些过期key，就会导致过期的key无法被<code>惰性删除</code>，从而导致过期的key一直堆积在内存中，最终造成Redis内存块被消耗殆尽。</p>
<p>Redis的<strong>内存淘汰机制</strong>应运而生</p>
<h3 id="8种内存淘汰机制"><a href="#8种内存淘汰机制" class="headerlink" title="8种内存淘汰机制"></a>8种内存淘汰机制</h3><ol>
<li><code>noeviction</code><ul>
<li>不驱逐任何key</li>
</ul>
</li>
<li><code>allkeys-lru</code><ul>
<li>对所有key使用LRU算法进行删除（<strong>默认</strong>）</li>
</ul>
</li>
<li><code>volatile-lru</code><ul>
<li>对所有设置了过期时间的key使用lru算法进行删除</li>
</ul>
</li>
<li><code>allkeys-random</code><ul>
<li>对所有key随机删除</li>
</ul>
</li>
<li><code>volatile-random</code><ul>
<li>对所有设置了过期时间的key随机删除</li>
</ul>
</li>
<li><code>volatile-ttl</code><ul>
<li>删除马上要过期的key</li>
</ul>
</li>
<li><code>allkeys-lfu</code><ul>
<li>对所有key使用LFU算法进行删除</li>
</ul>
</li>
<li><code>volatile-lfu</code><ul>
<li>对所有设置了过期时间的key使用LFU算法进行删除</li>
</ul>
</li>
</ol>
<h1 id="Git复习"><a href="#Git复习" class="headerlink" title="Git复习"></a>Git复习</h1><h3 id="-1"><a href="#-1" class="headerlink" title=""></a></h3><h1 id="Docker复习"><a href="#Docker复习" class="headerlink" title="Docker复习"></a>Docker复习</h1><h1 id="经典算法复习"><a href="#经典算法复习" class="headerlink" title="经典算法复习"></a>经典算法复习</h1><h2 id="1-经典排序及部分优化"><a href="#1-经典排序及部分优化" class="headerlink" title="1. 经典排序及部分优化"></a>1. 经典排序及部分优化</h2><h3 id="1-Bubble-Sort"><a href="#1-Bubble-Sort" class="headerlink" title="1. Bubble Sort"></a>1. Bubble Sort</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">int</span>[] bubbleSort(<span class="type">int</span>[] array)&#123;</span><br><span class="line">	<span class="keyword">if</span>(array.length == <span class="number">0</span>)&#123;</span><br><span class="line">		<span class="keyword">return</span> array;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;array.length-<span class="number">1</span>;i++)&#123;		<span class="comment">//比较a.length-1轮即可，比较一轮找到一个</span></span><br><span class="line">		<span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;array.length-<span class="number">1</span>-i;j++)&#123;		<span class="comment">//无序区间[0,a.length-1-i)</span></span><br><span class="line">			<span class="keyword">if</span>(array[j+<span class="number">1</span>]&lt;array[j])&#123;</span><br><span class="line">				<span class="type">int</span> <span class="variable">temp</span> <span class="operator">=</span> array[j+<span class="number">1</span>];</span><br><span class="line">				array[j+<span class="number">1</span>] = array[j];</span><br><span class="line">				array[j] = temp;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> array;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>优化</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 冒泡排序</span></span><br><span class="line"><span class="comment">   *  优化点： 如果一趟排序中没有任何一对元素交换位置，那么整个序列已经是有序的，就不需要再进行排序了</span></span><br><span class="line"><span class="comment">   * <span class="doctag">@param</span> array</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sort</span><span class="params">(<span class="type">int</span>[] array)</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(array == <span class="literal">null</span> || array.length==<span class="number">0</span>)&#123;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">boolean</span> flag;</span><br><span class="line">    <span class="type">int</span> <span class="variable">changeIndex</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> <span class="variable">lastChangeIndex</span> <span class="operator">=</span> array.length;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;array.length;i++)&#123;</span><br><span class="line">      flag = <span class="literal">true</span>;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">0</span>;j&lt;lastChangeIndex-<span class="number">1</span>;j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(array[j]&gt;array[j+<span class="number">1</span>])&#123;</span><br><span class="line">          flag = <span class="literal">false</span>;  <span class="comment">// 优化点：如果当前排序趟中并没有两个元素的交换，那么说明整个序列已经有序了，无需再进行比较了</span></span><br><span class="line">          <span class="type">int</span> <span class="variable">temp</span> <span class="operator">=</span> array[j+<span class="number">1</span>];</span><br><span class="line">          array[j+<span class="number">1</span>] =  array[j];</span><br><span class="line">          array[j] = temp;</span><br><span class="line">          changeIndex = j; <span class="comment">// 记录每次交换的位置</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      lastChangeIndex = changeIndex; <span class="comment">// 记录最后一次交换的位置</span></span><br><span class="line">      <span class="keyword">if</span> (flag)&#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"><span class="comment">//        break;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>







<h3 id="2-Selection-Sort"><a href="#2-Selection-Sort" class="headerlink" title="2. Selection Sort"></a>2. Selection Sort</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>优化</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 同时确定 最大值和最小值这两个位置上的数</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h3 id="3-Insertion-Sort"><a href="#3-Insertion-Sort" class="headerlink" title="3. Insertion Sort"></a>3. Insertion Sort</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 基础版</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> nums</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">sort</span><span class="params">(<span class="type">int</span>[] nums)</span>&#123;</span><br><span class="line">  <span class="type">int</span> current;</span><br><span class="line">  <span class="type">int</span> preIndex;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;nums.length-<span class="number">1</span>;i++)&#123;</span><br><span class="line">    current = nums[i+<span class="number">1</span>];</span><br><span class="line">    preIndex=i;</span><br><span class="line">    <span class="keyword">while</span> (preIndex&gt;=<span class="number">0</span>&amp;&amp;current&lt;nums[preIndex])&#123;</span><br><span class="line">      <span class="comment">//TODO 一次向前插入比较，如果当前元素小于前面的元素，那么就将前面的元素移动到后边</span></span><br><span class="line">      nums[preIndex+<span class="number">1</span>] = nums[preIndex];</span><br><span class="line">      preIndex--;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// TODO 最后一次将当前插入元素赋值到最终的位置</span></span><br><span class="line">    nums[preIndex+<span class="number">1</span>]=current;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><strong>优化</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * 优化版: 折半插入排序</span></span><br><span class="line"><span class="comment">  *  当代插入元素往前插入的时候，没有必要一个一个进行比较，可以通过折半查找（二分查找）的方式进行定位</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@param</span> nums</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">sort2</span><span class="params">(<span class="type">int</span>[] nums)</span>&#123;</span><br><span class="line">  <span class="type">int</span> current;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;nums.length;i++)&#123;</span><br><span class="line">    <span class="type">int</span> <span class="variable">low</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> <span class="variable">high</span> <span class="operator">=</span> i-<span class="number">1</span>;</span><br><span class="line">    current = nums[i];</span><br><span class="line">    <span class="comment">// TODO 折半查找寻找待插入的最终位置</span></span><br><span class="line">    <span class="keyword">while</span>(low&lt;=high)&#123;</span><br><span class="line">      <span class="type">int</span> <span class="variable">mid</span> <span class="operator">=</span> low + (high-low)/<span class="number">2</span>;</span><br><span class="line">      <span class="keyword">if</span>(nums[mid]&gt;current)&#123;</span><br><span class="line">        high = mid - <span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span>&#123;</span><br><span class="line">        low = mid + <span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// TODO 将hight前面的数据往后移动一位</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j=i-<span class="number">1</span>;j&gt;high;j--)&#123;</span><br><span class="line">      nums[j + <span class="number">1</span>] = nums[j];</span><br><span class="line">    &#125;</span><br><span class="line">    nums[high+<span class="number">1</span>] = current; <span class="comment">//TODO 最终将待插入元素插入到指定位置</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<h3 id="4-Quick-Sort"><a href="#4-Quick-Sort" class="headerlink" title="4. Quick Sort"></a>4. Quick Sort</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">QuickSort</span> &#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">sort</span><span class="params">(<span class="type">int</span>[] nums)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">size</span> <span class="operator">=</span> nums.length;</span><br><span class="line">        sort(nums, <span class="number">0</span>, size - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">sort</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> low, <span class="type">int</span> high)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.length &lt;= <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">        <span class="keyword">if</span> (low &gt; high) <span class="keyword">return</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">left</span> <span class="operator">=</span> low;</span><br><span class="line">        <span class="type">int</span> <span class="variable">right</span> <span class="operator">=</span> high;</span><br><span class="line">        <span class="type">int</span> <span class="variable">base</span> <span class="operator">=</span> nums[left];</span><br><span class="line">        <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="comment">//TODO 先从后往前找比基准小的数</span></span><br><span class="line">            <span class="keyword">while</span> (left &lt; right &amp;&amp; nums[right] &gt;= base) &#123; <span class="comment">// 等于的时候也需要考虑</span></span><br><span class="line">                right--;</span><br><span class="line">            &#125;</span><br><span class="line">            nums[left] = nums[right]; <span class="comment">// 移动到前半部分</span></span><br><span class="line">            <span class="comment">//TODO 在从前往后找比基准大的数</span></span><br><span class="line">            <span class="keyword">while</span> (left &lt; right &amp;&amp; nums[left] &lt;= base) &#123; <span class="comment">// 等于的时候也需要考虑</span></span><br><span class="line">                left++;</span><br><span class="line">            &#125;</span><br><span class="line">            nums[right] = nums[left]; <span class="comment">// 移动到后半部分</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (left == right) &#123;</span><br><span class="line">            nums[left] = base; <span class="comment">// 当 left==right 时，将基准填写到此处</span></span><br><span class="line">            sort(nums, low, left - <span class="number">1</span>);    <span class="comment">// 继续排序前半部分</span></span><br><span class="line">            sort(nums, left + <span class="number">1</span>, high);   <span class="comment">//  继续排序后半部分</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">int</span>[] array = <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;<span class="number">6</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">1</span>&#125;;</span><br><span class="line">        sort(array);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i : array) &#123;</span><br><span class="line">            System.out.print(i + <span class="string">&quot;,&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="5-HeapSort"><a href="#5-HeapSort" class="headerlink" title="5. HeapSort"></a>5. HeapSort</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 大顶堆类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Heap</span> &#123;</span><br><span class="line">    <span class="type">int</span>[] array;</span><br><span class="line">    <span class="type">int</span> size;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">Heap</span><span class="params">(<span class="type">int</span>[] arr)</span> &#123;</span><br><span class="line">        array = arr;</span><br><span class="line">        size = arr.length;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">buildHeap</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="built_in">this</span>.size / <span class="number">2</span> - <span class="number">1</span>;  <span class="comment">// 最后一个非叶子节点</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">index</span> <span class="operator">=</span> i; index &gt;= <span class="number">0</span>; index--) &#123;</span><br><span class="line">            adjHeap(index);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">adjHeap</span><span class="params">(<span class="type">int</span> rootIndex)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">leftIndex</span> <span class="operator">=</span> <span class="number">2</span> * rootIndex + <span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">rightIndex</span> <span class="operator">=</span> <span class="number">2</span> * rootIndex + <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">maxIndex</span> <span class="operator">=</span> rootIndex;</span><br><span class="line">        <span class="comment">//TODO 找出 根元素，左子树元素， 右子树元素三者的最大的值的那个索引</span></span><br><span class="line">        <span class="keyword">if</span> (leftIndex &lt; <span class="built_in">this</span>.size &amp;&amp; array[leftIndex] &gt; array[maxIndex]) &#123;</span><br><span class="line">            maxIndex = leftIndex;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (rightIndex &lt; <span class="built_in">this</span>.size &amp;&amp; array[rightIndex] &gt; array[maxIndex]) &#123;</span><br><span class="line">            maxIndex = rightIndex;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (maxIndex != rootIndex) &#123;</span><br><span class="line">            <span class="comment">//交换两个节点的值</span></span><br><span class="line">            swap(maxIndex, rootIndex);</span><br><span class="line">            <span class="comment">//TODO 继续调整交换完数据的子树</span></span><br><span class="line">            adjHeap(maxIndex);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">swap</span><span class="params">(<span class="type">int</span> i, <span class="type">int</span> j)</span>&#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">temp</span> <span class="operator">=</span> array[i];</span><br><span class="line">        array[i] = array[j];</span><br><span class="line">        array[j] = temp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sort</span><span class="params">()</span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>;i&lt;array.length;i++)&#123;</span><br><span class="line">            <span class="comment">// 先构建堆</span></span><br><span class="line">            buildHeap();</span><br><span class="line">            <span class="comment">// 交换堆顶元素</span></span><br><span class="line">            swap(<span class="number">0</span>, size-<span class="number">1</span>);</span><br><span class="line">            size--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HeapSort</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">sort</span><span class="params">(<span class="type">int</span>[] nums)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.length &lt;= <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">        <span class="type">Heap</span> <span class="variable">heap</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Heap</span>(nums);</span><br><span class="line">        heap.sort();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">int</span>[] array = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>&#125;;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i : array) &#123;</span><br><span class="line">            System.out.println(i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3 id="6-归并排序"><a href="#6-归并排序" class="headerlink" title="6. 归并排序"></a>6. 归并排序</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MergeSort1</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">int</span>[] array = <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;<span class="number">8</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">2</span>&#125;;</span><br><span class="line">        <span class="type">int</span>[] temp = <span class="keyword">new</span> <span class="title class_">int</span>[array.length];</span><br><span class="line">        mergeSort(array, <span class="number">0</span>, array.length - <span class="number">1</span>, temp);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i : array) &#123;</span><br><span class="line">            System.out.print(i + <span class="string">&quot;,&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 归并排序主要有两个重要的步骤：分+合</span></span><br><span class="line"><span class="comment">     * 分：即将数组分为一块块更小的数组，直至之后一个元素</span></span><br><span class="line"><span class="comment">     * 合：进行将两个小分区的数据进行合并排序</span></span><br><span class="line"><span class="comment">     * 整个过程中，将排序的数据存放到一个临时数组中temp。</span></span><br><span class="line"><span class="comment">     * 最终将temp数组中的数据赋值到原始数组中。</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> array</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> left</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> right</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> temp</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">mergeSort</span><span class="params">(<span class="type">int</span>[] array, <span class="type">int</span> left, <span class="type">int</span> right, <span class="type">int</span>[] temp)</span> &#123;</span><br><span class="line">        <span class="comment">// 当left小于right的时候，进行分+合</span></span><br><span class="line">        <span class="keyword">if</span> (left &lt; right) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">mid</span> <span class="operator">=</span> left + (right - left) / <span class="number">2</span>;</span><br><span class="line">            <span class="comment">// 分</span></span><br><span class="line">            <span class="comment">// 分左</span></span><br><span class="line">            mergeSort(array, left, mid, temp);</span><br><span class="line">            <span class="comment">// 分右</span></span><br><span class="line">            mergeSort(array, mid + <span class="number">1</span>, right, temp);</span><br><span class="line">            <span class="comment">// 合</span></span><br><span class="line">            merge(array, left, mid, right, temp);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">merge</span><span class="params">(<span class="type">int</span>[] array, <span class="type">int</span> left, <span class="type">int</span> mid, <span class="type">int</span> right, <span class="type">int</span>[] temp)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> left;</span><br><span class="line">        <span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> mid + <span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">t</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 判断条件：左侧索引不要超过mid；右侧索引不要超过右边界</span></span><br><span class="line">        <span class="keyword">while</span> (i &lt;= mid &amp;&amp; j &lt;= right) &#123;</span><br><span class="line">            <span class="comment">// 将数据顺序的传入到temp数组中</span></span><br><span class="line">            <span class="keyword">if</span> (array[i] &lt;= array[j]) &#123;</span><br><span class="line">                temp[t] = array[i];</span><br><span class="line">                t += <span class="number">1</span>;</span><br><span class="line">                i += <span class="number">1</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                temp[t] = array[j];</span><br><span class="line">                t += <span class="number">1</span>;</span><br><span class="line">                j += <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 上述循环执行完成后，有可能会存在一个区间中的数据还有剩余数据</span></span><br><span class="line">        <span class="comment">// 处理区间剩余元素（因为会存在一个区间还有剩余数据）</span></span><br><span class="line">        <span class="keyword">while</span> (i &lt;= mid) &#123;</span><br><span class="line">            temp[t] = array[i];</span><br><span class="line">            t += <span class="number">1</span>;</span><br><span class="line">            i += <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span> (j &lt;= right) &#123;</span><br><span class="line">            temp[t] = array[j];</span><br><span class="line">            t += <span class="number">1</span>;</span><br><span class="line">            j += <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 上述循环处理完成后，temp数组中的数据即为合并完之后的有序的数据</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">leftTemp</span> <span class="operator">=</span> left;</span><br><span class="line">        t = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">/* 限定当前区间的右边界 */</span></span><br><span class="line">        <span class="keyword">while</span> (leftTemp &lt;= right) &#123;</span><br><span class="line">            array[leftTemp++] = temp[t++];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="7-ShellSort"><a href="#7-ShellSort" class="headerlink" title="7. ShellSort"></a>7. ShellSort</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ShellSort</span> &#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">sort</span><span class="params">(<span class="type">int</span>[] nums)</span>&#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">size</span> <span class="operator">=</span> nums.length;</span><br><span class="line">        <span class="comment">//TODO gap 递减</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> gap=size/<span class="number">2</span>;gap&gt;<span class="number">0</span>;gap /= <span class="number">2</span>)&#123;</span><br><span class="line">            <span class="comment">//TODO 根据gap确定每组中的后边的元素，然后根据此元素-gap，就可以得到前面的数</span></span><br><span class="line">            <span class="comment">//TODO i:代表即将插入的元素角标，作为每一组比较数据的最后一个元素角标</span></span><br><span class="line">            <span class="comment">//TODO j:代表与i同一组的数组元素角标</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> i=gap;i&lt;size;i++)&#123;</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> j=i; j-gap&gt;=<span class="number">0</span>&amp;&amp;nums[j-gap]&gt;nums[j]; j -= gap)&#123;</span><br><span class="line">                    <span class="type">int</span> <span class="variable">temp</span> <span class="operator">=</span> nums[j];</span><br><span class="line">                    nums[j] = nums[j-gap];</span><br><span class="line">                    nums[j-gap] = temp;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">int</span>[] nums = &#123;<span class="number">5</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">6</span>,<span class="number">4</span>&#125;;</span><br><span class="line">        sort(nums);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> num : nums) &#123;</span><br><span class="line">            System.out.println(num);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h2 id="2-分布式算法"><a href="#2-分布式算法" class="headerlink" title="2. 分布式算法"></a>2. 分布式算法</h2><h3 id="1-分布式id生成算法（雪花算法）"><a href="#1-分布式id生成算法（雪花算法）" class="headerlink" title="1. 分布式id生成算法（雪花算法）"></a>1. 分布式id生成算法（雪花算法）</h3><h4 id="SnowFlake"><a href="#SnowFlake" class="headerlink" title="SnowFlake"></a><strong>SnowFlake</strong></h4><p>其核心思想就是：使用一个64bit的long型的数字作为全局唯一id。这个ID引入了时间戳，基本上保持自增的。</p>
<p><strong>64bit各段所代表的意义</strong></p>
<ol>
<li>第一部分： <strong>1bit</strong><ul>
<li>高位第一位通常为符号位，而生成的ID一般都是正整数的，所以第一位保持为0</li>
</ul>
</li>
<li>第二部分：<strong>41bit</strong><ul>
<li>41bit表示时间戳，单位是毫秒</li>
<li>换算成年就是表示69年的时间</li>
</ul>
</li>
<li>第三部分：<strong>10bit</strong><ul>
<li>5bit：表示机房id</li>
<li>5bit：表示机器id</li>
</ul>
</li>
<li>第四部分：<strong>12bit</strong><ul>
<li>12bit表示序号，也就是某个机房某台机器上在这一毫秒内同时生成的id序号，0000-0000-0000</li>
</ul>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">IdWorker</span> &#123;</span><br><span class="line"> </span><br><span class="line">	<span class="comment">//因为二进制里第一个 bit 为如果是 1，那么都是负数，但是我们生成的 id 都是正数，所以第一个 bit 统一都是 0。</span></span><br><span class="line"> </span><br><span class="line">	<span class="comment">//机器ID  2进制5位  32位减掉1位 31个</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> workerId;</span><br><span class="line">	<span class="comment">//机房ID 2进制5位  32位减掉1位 31个</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> datacenterId;</span><br><span class="line">	<span class="comment">//代表一毫秒内生成的多个id的最新序号  12位 4096 -1 = 4095 个</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> sequence;</span><br><span class="line">	<span class="comment">//设置一个时间初始值    2^41 - 1   差不多可以用69年</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> <span class="variable">twepoch</span> <span class="operator">=</span> <span class="number">1585644268888L</span>;</span><br><span class="line">	<span class="comment">//5位的机器id</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> <span class="variable">workerIdBits</span> <span class="operator">=</span> <span class="number">5L</span>;</span><br><span class="line">	<span class="comment">//5位的机房id</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> <span class="variable">datacenterIdBits</span> <span class="operator">=</span> <span class="number">5L</span>;</span><br><span class="line">	<span class="comment">//每毫秒内产生的id数 2 的 12次方</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> <span class="variable">sequenceBits</span> <span class="operator">=</span> <span class="number">12L</span>;</span><br><span class="line">	<span class="comment">// 这个是二进制运算，就是5 bit最多只能有31个数字，也就是说机器id最多只能是32以内</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> <span class="variable">maxWorkerId</span> <span class="operator">=</span> -<span class="number">1L</span> ^ (-<span class="number">1L</span> &lt;&lt; workerIdBits);</span><br><span class="line">	<span class="comment">// 这个是一个意思，就是5 bit最多只能有31个数字，机房id最多只能是32以内</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> <span class="variable">maxDatacenterId</span> <span class="operator">=</span> -<span class="number">1L</span> ^ (-<span class="number">1L</span> &lt;&lt; datacenterIdBits);</span><br><span class="line"> </span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> <span class="variable">workerIdShift</span> <span class="operator">=</span> sequenceBits; <span class="comment">//12</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> <span class="variable">datacenterIdShift</span> <span class="operator">=</span> sequenceBits + workerIdBits; <span class="comment">// 12+5=17</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> <span class="variable">timestampLeftShift</span> <span class="operator">=</span> sequenceBits + workerIdBits + datacenterIdBits; <span class="comment">// 12 + 5 + 5 = 22</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> <span class="variable">sequenceMask</span> <span class="operator">=</span> -<span class="number">1L</span> ^ (-<span class="number">1L</span> &lt;&lt; sequenceBits);</span><br><span class="line">	<span class="comment">//记录产生时间毫秒数，判断是否是同1毫秒</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> <span class="variable">lastTimestamp</span> <span class="operator">=</span> -<span class="number">1L</span>;</span><br><span class="line">	<span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getWorkerId</span><span class="params">()</span>&#123;</span><br><span class="line">		<span class="keyword">return</span> workerId;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getDatacenterId</span><span class="params">()</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> datacenterId;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getTimestamp</span><span class="params">()</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> System.currentTimeMillis();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 *</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> workerId	机器id</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> datacenterId	数据中心id（机房id）</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> sequence		序列号</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="keyword">public</span> <span class="title function_">IdWorker</span><span class="params">(<span class="type">long</span> workerId, <span class="type">long</span> datacenterId, <span class="type">long</span> sequence)</span> &#123;</span><br><span class="line"> </span><br><span class="line">		<span class="comment">// 检查机房id和机器id是否超过31 不能小于0</span></span><br><span class="line">		<span class="keyword">if</span> (workerId &gt; maxWorkerId || workerId &lt; <span class="number">0</span>) &#123;</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalArgumentException</span>(</span><br><span class="line">					String.format(<span class="string">&quot;worker Id can&#x27;t be greater than %d or less than 0&quot;</span>,maxWorkerId));</span><br><span class="line">		&#125;</span><br><span class="line"> </span><br><span class="line">		<span class="keyword">if</span> (datacenterId &gt; maxDatacenterId || datacenterId &lt; <span class="number">0</span>) &#123;</span><br><span class="line"> </span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">IllegalArgumentException</span>(</span><br><span class="line">					String.format(<span class="string">&quot;datacenter Id can&#x27;t be greater than %d or less than 0&quot;</span>,maxDatacenterId));</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="built_in">this</span>.workerId = workerId;</span><br><span class="line">		<span class="built_in">this</span>.datacenterId = datacenterId;</span><br><span class="line">		<span class="built_in">this</span>.sequence = sequence;</span><br><span class="line">	&#125;</span><br><span class="line"> </span><br><span class="line">	<span class="comment">//TODO 这个是核心方法，通过调用nextId()方法，让当前这台机器上的snowflake算法程序生成一个全局唯一的id</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="type">long</span> <span class="title function_">nextId</span><span class="params">()</span> &#123;</span><br><span class="line">		<span class="comment">// 这儿就是获取当前时间戳，单位是毫秒</span></span><br><span class="line">		<span class="type">long</span> <span class="variable">timestamp</span> <span class="operator">=</span> timeGen();</span><br><span class="line">		<span class="keyword">if</span> (timestamp &lt; lastTimestamp) &#123;</span><br><span class="line"> </span><br><span class="line">			System.err.printf(</span><br><span class="line">					<span class="string">&quot;clock is moving backwards. Rejecting requests until %d.&quot;</span>, lastTimestamp);</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(</span><br><span class="line">					String.format(<span class="string">&quot;Clock moved backwards. Refusing to generate id for %d milliseconds&quot;</span>,</span><br><span class="line">							lastTimestamp - timestamp));</span><br><span class="line">		&#125;</span><br><span class="line"> </span><br><span class="line">		<span class="comment">// 下面是说假设在同一个毫秒内，又发送了一个请求生成一个id</span></span><br><span class="line">		<span class="comment">// 这个时候就得把seqence序号给递增1，最多就是4096</span></span><br><span class="line">		<span class="keyword">if</span> (lastTimestamp == timestamp) &#123;</span><br><span class="line"> </span><br><span class="line">			<span class="comment">// 这个意思是说一个毫秒内最多只能有4096个数字，无论你传递多少进来，</span></span><br><span class="line">			<span class="comment">//这个位运算保证始终就是在4096这个范围内，避免你自己传递个sequence超过了4096这个范围</span></span><br><span class="line">			sequence = (sequence + <span class="number">1</span>) &amp; sequenceMask;</span><br><span class="line">			<span class="comment">//当某一毫秒的时间，产生的id数 超过4095，系统会进入等待，直到下一毫秒，系统继续产生ID</span></span><br><span class="line">			<span class="keyword">if</span> (sequence == <span class="number">0</span>) &#123;</span><br><span class="line">				timestamp = tilNextMillis(lastTimestamp);</span><br><span class="line">			&#125;</span><br><span class="line"> </span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="comment">// 每个时间戳内，序列号都从0开始</span></span><br><span class="line">			sequence = <span class="number">0</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">// 这儿记录一下最近一次生成id的时间戳，单位是毫秒</span></span><br><span class="line">		lastTimestamp = timestamp;</span><br><span class="line">		<span class="comment">// 这儿就是最核心的二进制位运算操作，生成一个64bit的id</span></span><br><span class="line">		<span class="comment">// 先将当前时间戳左移，放到41 bit那儿；将机房id左移放到5 bit那儿；将机器id左移放到5 bit那儿；将序号放最后12 bit</span></span><br><span class="line">		<span class="comment">// 最后拼接起来成一个64 bit的二进制数字，转换成10进制就是个long型</span></span><br><span class="line">		<span class="keyword">return</span> ((timestamp - twepoch) &lt;&lt; timestampLeftShift) |</span><br><span class="line">				(datacenterId &lt;&lt; datacenterIdShift) |</span><br><span class="line">				(workerId &lt;&lt; workerIdShift) | sequence;</span><br><span class="line">	&#125;</span><br><span class="line"> </span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * 当某一毫秒的时间，产生的id数 超过4095，系统会进入等待，直到下一毫秒，系统继续产生ID</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> lastTimestamp</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> <span class="title function_">tilNextMillis</span><span class="params">(<span class="type">long</span> lastTimestamp)</span> &#123;</span><br><span class="line"> </span><br><span class="line">		<span class="type">long</span> <span class="variable">timestamp</span> <span class="operator">=</span> timeGen();</span><br><span class="line"> 		<span class="comment">//TODO 知道当前时间戳大于上一次时间戳的时候，退出循环</span></span><br><span class="line">		<span class="keyword">while</span> (timestamp &lt;= lastTimestamp) &#123;</span><br><span class="line">			timestamp = timeGen();</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> timestamp;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//获取当前时间戳</span></span><br><span class="line">	<span class="keyword">private</span> <span class="type">long</span> <span class="title function_">timeGen</span><span class="params">()</span>&#123;</span><br><span class="line">		<span class="keyword">return</span> System.currentTimeMillis();</span><br><span class="line">	&#125;</span><br><span class="line"> </span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 *  main 测试类</span></span><br><span class="line"><span class="comment">	 * <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">		System.out.println(<span class="number">1</span>&amp;<span class="number">4596</span>);</span><br><span class="line">		System.out.println(<span class="number">2</span>&amp;<span class="number">4596</span>);</span><br><span class="line">		System.out.println(<span class="number">6</span>&amp;<span class="number">4596</span>);</span><br><span class="line">		System.out.println(<span class="number">6</span>&amp;<span class="number">4596</span>);</span><br><span class="line">		System.out.println(<span class="number">6</span>&amp;<span class="number">4596</span>);</span><br><span class="line">		System.out.println(<span class="number">6</span>&amp;<span class="number">4596</span>);</span><br><span class="line"><span class="comment">//		IdWorker worker = new IdWorker(1,1,1);</span></span><br><span class="line"><span class="comment">//		for (int i = 0; i &lt; 22; i++) &#123;</span></span><br><span class="line"><span class="comment">//			System.out.println(worker.nextId());</span></span><br><span class="line"><span class="comment">//		&#125;</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>













<h3 id="2-Flink中的检查点一致性算法"><a href="#2-Flink中的检查点一致性算法" class="headerlink" title="2. Flink中的检查点一致性算法"></a>2. Flink中的检查点一致性算法</h3><p><strong>关键：</strong>检查点<code>barrier</code>对齐，上游所有分区的<code>barrier</code>都被处理完之后，才做<code>checkpoint</code>，保证barrier之前的数据都处理完。</p>
<p>其中有的分区的数据处理的快，barrier处理完成后，还会继续处理接收到数据，这些数据可以被<code>缓存</code>起来，等待所有的barrier对齐之后，把当前的状态保存到<code>状态后端</code>,然后继续下下游传递。</p>
<h3 id="3-分布式一致性算法"><a href="#3-分布式一致性算法" class="headerlink" title="3. 分布式一致性算法"></a>3. 分布式一致性算法</h3><h4 id="CAP理论的定义"><a href="#CAP理论的定义" class="headerlink" title="CAP理论的定义"></a>CAP理论的定义</h4><p>CAP指的是：<strong>Consistency一致性；Availability可用性；Partition tolerance分区容错性</strong>（分区容错性要保证）</p>
<p>CAP理论中，CP与AP二选一</p>
<p>强调的客户端发出请求之后，服务端先去保证一致性还是先返回</p>
<p>Redis是AP架构：客户端发出请求之后，服务端不侧重一致性，而是先返回相应信息</p>
<p>Zookeeper是CP架构：即客户端发出请求之后，服务端需要先保证一致性，（其他节点需要同步，保持一致）才响应客户端；（内部使用的paxos算法，多数派只要同步了，就返回响应）</p>
<p><strong>分布式环境一定要保证一致性</strong></p>
<p>分布式一致性问题解决的一般方案：<font color=red>state machine replication（状态机复制）</font></p>
<p>通俗的说，每个操作都是一条日志其他节点同步到这些日志执行来实现集群一致性的。</p>
<p><strong>弱一致性模型</strong>（其他节点不立即同步后才响应，但是最终会保证一致性的）</p>
<pre><code>- **DNS**
- GOSSIP
</code></pre>
<p><strong>强一致性模型</strong>（先保证其他节点同步）</p>
<ul>
<li><strong>Paxos</strong>（多数派）</li>
<li><strong>Raft（multi paxos）</strong></li>
<li><strong>ZAB（multi paxos）</strong></li>
<li>同步（只要有一个节点没有同步成功，整个集群就会不可用）</li>
</ul>
<img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210817101206793-9166328.png" alt="image-20210817101206793" style="zoom:50%;" />





<h4 id="决策模型"><a href="#决策模型" class="headerlink" title="决策模型"></a>决策模型</h4><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210817101527618-9166529.png" alt="image-20210817101527618"></p>
<p><strong>Client</strong>：系统外部角色，请求发起者。像民众。</p>
<p><strong>Proposer</strong>：提议一个值，用于投票表决。向议员，替民众提出议案</p>
<p>**Acceptor(Voter)**：对每个提议的值进行投票，并存储接收的值</p>
<p><strong>Learner：</strong>被告知投票的结果，接收达成共识的值，存储保存，不参与投票的过程。像记录员。（做备份）</p>
<h4 id="Paxos"><a href="#Paxos" class="headerlink" title="Paxos"></a>Paxos</h4><h5 id="Basic-Paxos基本流程"><a href="#Basic-Paxos基本流程" class="headerlink" title="Basic Paxos基本流程"></a>Basic Paxos基本流程</h5><p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210817102317518-9167001.png" alt="image-20210817102317518"></p>
<p><strong>tips：</strong></p>
<ol>
<li>每个提案都会有一个<strong>唯一的id</strong>，而且是<strong>全局自增</strong>的。<ul>
<li>全局自增的目的是保证日志数据的有序性</li>
<li>例如：当前提案id为5，如果来了一个4号的提案，就会认为这个提案是过时的，不会再处理</li>
</ul>
</li>
<li>二阶段提交</li>
</ol>
<p><strong>角色介绍</strong></p>
<p>一阶段：</p>
<ol>
<li><font color=red>Phase 1a：Prepare</font><ul>
<li>proposer提出一个提案，编号为N，此N大于这个proposer之前提出的提案编号。请求Accpetor的quorum（最大的允许数量）接受。</li>
</ul>
</li>
<li><font color=red>Phase 1b：Promise</font><ul>
<li>如果N大于此acceptor之前接收的任何提案编号则接收，否则拒绝</li>
</ul>
</li>
</ol>
<p>二阶段：</p>
<ol start="3">
<li><font color=red>Phase 2a：Accept</font><ul>
<li>如果达到了多数派（多数派都接收到了这个大于之前提案的请求，响应给proposer）,proposer会发出accept请求，此请求包含提案编号<strong>N</strong>，以及<strong>提案内容</strong>。</li>
</ul>
</li>
<li><font color=red>Phase 2b：Accepted</font><ul>
<li>如果此acceptor在此期间没有收到任何编号大于N的提案，则接收此提案的内容，否则忽略。</li>
</ul>
</li>
</ol>
<p><strong>Basic Paxos部分节点失败，但达到quoroms</strong></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210817155911310-9187152.png" alt="image-20210817155911310"></p>
<p>只要promise过半即可</p>
<p><strong>Basic Paxos Proposer失败</strong></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210817160123682-9187286.png" alt="image-20210817160123682"></p>
<p>Proposer故障，Proposer可以配置HA，然后客户端需要提出一个新的提案，然后内容是上一次提案的内容，但是提案的ID需要自增。</p>
<p><strong>Basic Paxos<font color=red>活锁问题</font></strong></p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210817160516330-9187518.png" alt="image-20210817160516330"></p>
<p>1号提案提出后，很短的时间内，2号提案又提出，这样会导致先发出的1号的提案不会被Promise</p>
<p>然后1号提案不甘心，有重新提交了一次，此次提案id为3，但是2号提案的id只小于3，2号提案就会被拒绝。</p>
<p>解决活锁的方式：随机增加超时时间，就是一次提案失败之后，可以稍稍等待一段时候之后再去提交。</p>
<p><strong>Basic Paxos模型其他问题</strong>：一次提案，多次RPC请求。</p>
<h5 id="Multi-Paxos"><a href="#Multi-Paxos" class="headerlink" title="Multi Paxos"></a>Multi Paxos</h5><p><code>Multi Paxos</code>是在多个<code>Accpetor</code>中选举出来一个<code>主节点</code>，然后所有的提案都会先经过这个<code>主 Accpetor</code>进行处理后，同步给其他<code>Accpetor</code>。</p>
<p><img src="source/2021%E5%B9%B48%E6%9C%88%E4%BB%BD%E7%A7%8B%E6%8B%9B%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20210817162816837-9188898.png" alt="image-20210817162816837"></p>
<h5 id="强一致性算法–Raft"><a href="#强一致性算法–Raft" class="headerlink" title="强一致性算法–Raft"></a>强一致性算法–Raft</h5><p><strong>三个子问题</strong></p>
<ol>
<li><strong>Leader Election</strong>（领导者选举）</li>
<li><strong>Log replication</strong>（日志复制）</li>
<li><strong>Safety</strong>（安全、恢复）</li>
</ol>
<p><strong>重新定义角色（状态）</strong></p>
<ol>
<li><p><strong>Leader</strong>（领导者）</p>
</li>
<li><p><strong>Follower</strong>（跟随者）</p>
</li>
<li><p><strong>Candidate</strong>（参选者）</p>
</li>
</ol>
<p><strong>Raft算法流程动画演示：</strong><a target="_blank" rel="noopener" href="https://raft.github.io/">https://raft.github.io/</a></p>
<p><a target="_blank" rel="noopener" href="http://thesecretlivesofdata.com/raft/">http://thesecretlivesofdata.com/raft/</a></p>
<p>流程说明前提：每个节点都会有一个<font color=green>election timeout</font>,如果时间倒计时达到了这个时间，该节点就会变为Condidate。处于<code>Condidate</code>状态（角色）的节点会给自己投一票，然后给其他节点发送消息，询问是否投自己一票；</p>
<p>如果其他节点收到上述消息，那么<font color=green>election timeout</font>倒计时就会重置，然后会应答给<code>Candidate</code>节点，当<code>Candidate</code>节点收到半数以上的应答就会转换为<code>Leader</code>状态，并向其它节点发送<code>心跳信息</code>，其他节点也要返回应答信息，其他节点只要收到信息就会重置<font color=green>election timeout</font>倒计时。</p>
<p>如果有Leader挂了，那么就重新进行选举。</p>
<p>具体步骤入校</p>
<ol>
<li><p>刚开始的时候，所有节点均处于<code>Follower</code>状态。每个节点都会有一个<code>timeout</code>超时时间，处于<code>Follower</code>状态的节点一旦节点倒计时超过timeout的时候，就会变为Condidate状态，然后向其他节点发送信息，竞选<code>Leader</code>，只要收到过半的应答信息，此节点就会晋胜为<code>Leader</code>。随后<code>Leader</code>节点会不断的向集群中的其他<code>Follower</code>节点发送心跳信息。集群中的每个节点只要收到消息，其本身的超时倒计时都会重置。</p>
</li>
<li><p>当集群中的节点<code>Leader</code>宕机（挂了），那么其他<code>Follower</code>就收不到心跳信息了，所以会重新选取Leader，过程跟第一步是一样的。</p>
<ul>
<li>如果原始<code>Leader</code>恢复之后，就会变成<code>Follower</code>，然后与此时的Follower同步数据，（历史数据也会同步过来）。</li>
</ul>
</li>
<li><p>集群中的请求都是通过<code>Leader</code>来进行处理的。</p>
<ul>
<li>比如客户端想<code>Leader</code>发送一的请求，请求写一条数据，然后<code>Leader</code>会便随着心跳信息转发这个写请求到集群中的其他节点，此时<code>Leader</code>的数据还没有写入到log中，等待集群中其他节点的回应。如果回应数达到了<strong>半数以上</strong>，那么此时<code>Leader</code>才会commit，真正的将数据写入到log中。此时其他的节点还没有同步到Log中。</li>
<li>伴随着下一次的心跳信息的接收，其他节点才会同步数据到log上。</li>
</ul>
</li>
<li><p>Raft会出现脑裂问题：</p>
<ul>
<li><p>当此时的<code>Leader</code>以大部分节点之间的通信被阻断后，其它大部分节点接收不到了<code>Leader</code>的心跳信息，他们之间就会重新进行<code>Leader</code>的竞选，竞选成功后就会出现<code>两个Leader</code>的情况（脑裂问题）</p>
</li>
<li><p><strong>脑裂的解决方案</strong></p>
<ul>
<li><p><font color=red>引入一个新的概念：region leader。region leader是一个逻辑上的概念，任意时刻对于某一个region来说，一定只拥有一个region leader，每个region leader在任期期间之内尝试每隔t时间间隔，在raft group内部更新一下region leader的lease。所有的读写请求都必须通过region leader完成</font></p>
</li>
<li><p>但是指的注意的是，region leader和raft leader可能不是一个节点，当region leader和raft leader不重合的时候，region leader会将请求转发给当前的raft leader，当网络出现分区时，会出现以下情况：</p>
<ol>
<li><strong>region leader落在多数派，老raft leader在多数派这边</strong><ul>
<li>对于第一种情况，<code>region leader</code>的lease不会过期，因为<code>region leader</code>的心跳仍然能更新到多数派的节点上，老的<code>raft leader</code>任然能同步到大多数节点上，少数派这边也不会选举出新的<code>Leader</code>，这种情况下不会出现stale read</li>
</ul>
</li>
<li><strong>region leader落在多数派，老raft leader在少数派这边</strong><ul>
<li>第二种情况，他的<code>raft leader</code>被分到了少数派，多数派这边选举出新的<code>raft leader</code>，如果此时的<code>region leader</code>在多数派。</li>
</ul>
</li>
<li><strong>region leader落在少数派，老raft leader在多数派这边</strong><ul>
<li>第三种情况，<code>region leader</code>落在了少数派这边，老<code>raft leader</code>在多数派这边，这种情况下客户端请求到<code>region leader</code>，它发现无法联系到<code>leader</code>，（因为在少数派这边没有办法选举出新的leader），请求会失败，直到本次<code>region leader</code>的lease过期，同时新的<code>region leader</code>会在多数派那边产生（因为新的region leader 需要尝试走一遍raft流程）。因为老的<code>region leader</code>没办法写入成功，所以也不会出现stale read。但是付出的代价是在<code>region leader lease</code>期间的系统可用性</li>
</ul>
</li>
<li><strong>region leader落在多数派，老raft leader在少数派这边</strong><ul>
<li>第四种情况与第三种情况类似，多数派那边会产生一个新的<code>raft leader</code>和<code>region leader</code></li>
</ul>
</li>
</ol>
<p>总体来说，这种方法牺牲了一定的可用性（在脑裂时部分客户端的可用性）换取了一致性的保证。</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h5 id="强一致性算法-ZAB"><a href="#强一致性算法-ZAB" class="headerlink" title="强一致性算法-ZAB"></a>强一致性算法-ZAB</h5><p>原理与Raft基本上相同。</p>
<p>在一些名词叫法上有些区别：如ZAB将某个leader的周期称为<code>epoch</code>，而raft则称之为<code>term</code>。</p>
<p>实现上也有些许不同：如raft的心跳方向是有leader值follower。而ZAB则是相反的。</p>
<p><strong>状态机复制的共识算法</strong></p>
<h1 id="面试中遇到的问题"><a href="#面试中遇到的问题" class="headerlink" title="面试中遇到的问题"></a>面试中遇到的问题</h1><h2 id="1-自己的亮点"><a href="#1-自己的亮点" class="headerlink" title="1. 自己的亮点"></a>1. 自己的亮点</h2><ol>
<li>自主学习能力强，有记笔记，写文档，CSDN博客，刷题的习惯。对于刷题这方面，维护了一个github库。</li>
<li>会不定时的进行阶段性的总结，每做过的一个小任务，做完之后都会复盘，找到其中遇到的问题，下次再遇到如何避免。</li>
<li>对于需求的开发，每次自己都会尝试给自己定一个排期，比如一个需求技术文档产出需要多长时间，任务开发需要多长时间、数据测试上线需要多长时间、看板配置需要多长时间。。。等等。</li>
<li>沟通上，乐于交流，沟通，对于下派不懂的任务我会首先自己去思考怎么去做，会，然后把疑问汇总起来，请教身边的哥哥姐姐。</li>
</ol>
<h2 id="2-面试谈薪资技巧"><a href="#2-面试谈薪资技巧" class="headerlink" title="2. 面试谈薪资技巧"></a>2. 面试谈薪资技巧</h2><h3 id="1-面试官问你的期望薪资要求（期望薪资是多少？）"><a href="#1-面试官问你的期望薪资要求（期望薪资是多少？）" class="headerlink" title="1. 面试官问你的期望薪资要求（期望薪资是多少？）"></a>1. 面试官问你的期望薪资要求（期望薪资是多少？）</h3><p>不要直接报出你的期望薪资，不要直接就报出具体的薪资，把问题抛回去：<strong>咱们公司对我这个岗位的薪资预算大概是多少？</strong></p>
<h3 id="2-我觉得你要的薪酬有点高？？？"><a href="#2-我觉得你要的薪酬有点高？？？" class="headerlink" title="2. 我觉得你要的薪酬有点高？？？"></a>2. 我觉得你要的薪酬有点高？？？</h3><p>这是明显的打压式问题。</p>
<p><font color=red>就今年的行情啊，各家公司都在控制相应岗位的成本，但是我对我自己的能力和贡献值还是有信心的</font></p>
<ol>
<li>一来，我有我所应聘的这个岗位的实习经历，从需求对接，技术评审到任务开发，数据测试，上线，看板配置，以及需求二次迭代，我自己都独自的完成过。</li>
<li>二来，我对Spark SQL任务的有实际的调优经验，而且是独自完成，动手能力我还是很有自信的。</li>
<li>三来，我平常学习工作会有<strong>记笔记</strong>和<strong>复盘</strong>的习惯，github上自己维护了一个<strong>blog</strong>，会定期的去把阶段性的学习成果push到上面，定期总结，回顾，还有重构（不同时期对待一个问题的看法会有所改变）。</li>
<li>最后一点就是沟通方面，不管是与产品进行沟通，还是与身边的哥哥姐姐这些大佬进行沟通，毫无压力。而且酒量也不错，社交这方面也是强项。</li>
</ol>
<h3 id="3-如果HR没有和你谈薪资"><a href="#3-如果HR没有和你谈薪资" class="headerlink" title="3. 如果HR没有和你谈薪资"></a>3. 如果HR没有和你谈薪资</h3><p>不要直接问你能给我多少钱，可以换个方法问：</p>
<p><strong>咱们这公司的薪资结构和绩效是怎么样的？</strong></p>
<h2 id="2-针对于实习项目，业务上缺乏自己独立的思考"><a href="#2-针对于实习项目，业务上缺乏自己独立的思考" class="headerlink" title="2. 针对于实习项目，业务上缺乏自己独立的思考"></a>2. 针对于实习项目，业务上缺乏自己独立的思考</h2><h3 id="2-1-可视化全平台产品运营数据分析"><a href="#2-1-可视化全平台产品运营数据分析" class="headerlink" title="2.1 可视化全平台产品运营数据分析"></a>2.1 可视化全平台产品运营数据分析</h3><p>需求背景：统计各方向运营数据报表的访问信息，看清报表的实际使用情况。</p>
<h4 id="一期需求"><a href="#一期需求" class="headerlink" title="一期需求"></a>一期需求</h4><p>一期需求只做数易可视化平台，乘客方向的运营报表统计。</p>
<p>在这个阶段，我自己有过独自思考的两个点：</p>
<ol>
<li><p><strong>一个是对于可视化平台这块的，在日后的需求迭代中，有没有可能会扩展到其他平台？</strong></p>
<ul>
<li>针对于第一点，我这里独自做了一个集齐某滴内部所有可视化平台的流量数据的统一性建设，将埋点数据进行抽取，隔离出一张可视化全平台的报表流量DWD数据明细层。</li>
<li>每类报表为其分配了特定的id进行区分，report_type</li>
</ul>
</li>
<li><p><strong>第二点是有没有可能会扩展到除了乘客方向的其他业务方向？</strong></p>
<ul>
<li><p>这里需要说明 一点，每个平台配置好的报表会统一的配置到一个叫做某花筒的门户下，但是这个门户下没有相应报表的维度信息</p>
</li>
<li><p>其他方向包括</p>
<blockquote>
<p>某司机；某特惠月；某网开台，某MP，某UT，某拼车，某安全，某决策，某乘客等9个方向</p>
</blockquote>
</li>
<li><p><strong>短期方案</strong>，是每个业务方向的报表维度信息会汇总到2个产品姐姐那边，然后最后会统一的按照我的dim层设计的字段，统一维护到线上的一个文档中，然后每天以全量的方式导入至dim维度层。</p>
</li>
<li><p><strong>长期方案</strong>，产品方与某花筒平台的后台负责方进行报表维度数据的落表，这样就不需要我们手动来维护整个报表</p>
</li>
</ul>
</li>
</ol>
<h4 id="二期需求"><a href="#二期需求" class="headerlink" title="二期需求"></a>二期需求</h4><p>从数易的可视化平台扩展到某易、某表王、某制化这3个可视化平台，业务方向从1个乘客方向，慢慢扩展到了：某司机；某特惠月；某网开台，某MP，某UT，某拼车，某安全，某决策，某乘客等9个业务方向。</p>
<h3 id="2-2-全生态数据增长建设"><a href="#2-2-全生态数据增长建设" class="headerlink" title="2.2 全生态数据增长建设"></a>2.2 全生态数据增长建设</h3><p>需求：<strong>某网约车在微信，支付宝，QQ等生态下均有相应的业务</strong></p>
<p>我主要做的工作是：<strong>对于冒泡，订单的场景归因的建设</strong></p>
<h4 id="一期需求-1"><a href="#一期需求-1" class="headerlink" title="一期需求"></a>一期需求</h4><p>只做<strong>微信</strong>这一个生态的。</p>
<p>场景归因采用<strong>末次归因</strong>的方法：</p>
<ul>
<li>冒泡前的最近一次的场景入口曝光</li>
<li>呼单前的最近一次的场景入口曝光</li>
</ul>
<p>自我思考：</p>
<ul>
<li><strong>日后可不可能会扩展到其他生态</strong>？</li>
<li><strong>采用末次归因，跨天的如何解决</strong>？</li>
</ul>
<p>对于可不可能扩展到其他生态这个问题，我独自设计了一个包括全生态下的小程序的流量数据明细层，然后通过分区字段进行区分。DWD明细层；会给表的说明信息打上生态的标签。</p>
<p>不管是哪个生态的场景归因，都依赖于这个统一的DWD层</p>
<h4 id="二期需求-1"><a href="#二期需求-1" class="headerlink" title="二期需求"></a>二期需求</h4><p>生态从微信，扩展到 支付宝，QQ、银行等生态</p>
<h2 id="3-回答问题逻辑绕，啰嗦"><a href="#3-回答问题逻辑绕，啰嗦" class="headerlink" title="3. 回答问题逻辑绕，啰嗦"></a>3. 回答问题逻辑绕，啰嗦</h2><p>简明扼要的进行回答，总结性的回答。</p>
<h2 id="4-大数据组件了解少"><a href="#4-大数据组件了解少" class="headerlink" title="4. 大数据组件了解少"></a>4. 大数据组件了解少</h2><h2 id="5-为什么要分层"><a href="#5-为什么要分层" class="headerlink" title="5. 为什么要分层"></a>5. 为什么要分层</h2><p>分层的好处</p>
<ol>
<li>隔离原始数据，原始数据中可能会包含大量的敏感数据，类似于：身份证号，手机号，经纬度，</li>
<li>减少重复开发，开发人员也会减少很多不必要的工作量</li>
<li>在一定程度上也会解耦</li>
<li>把复杂问题简单化</li>
</ol>
<h2 id="6-要展现自己的亮点，保持主动性"><a href="#6-要展现自己的亮点，保持主动性" class="headerlink" title="6. 要展现自己的亮点，保持主动性"></a>6. 要展现自己的亮点，保持主动性</h2><ol>
<li>善于沟通</li>
<li>阶段性总结复盘</li>
<li>养成了书写文档的习惯</li>
<li>定期复盘</li>
</ol>
<h2 id="7-自身存在的缺点"><a href="#7-自身存在的缺点" class="headerlink" title="7. 自身存在的缺点"></a>7. 自身存在的缺点</h2><ol>
<li>有时候会比较粗心<ul>
<li>随意会在学习和工作中我会提前列好 to-do list ，以免遗漏任务</li>
<li>会定期进行阶段性的总结和复盘，某个任务中遇到了哪些问题，如何解决的，下次如何避免</li>
</ul>
</li>
<li>记忆力不是很强，东西一学多了，之前的东西就会忘记<ul>
<li>自己搭建博客，定期将阶段性的学习成果push到上面，定期进行过回顾，或者知识梳理，重新编排等</li>
</ul>
</li>
</ol>
<h2 id="8-自身存在的优点"><a href="#8-自身存在的优点" class="headerlink" title="8. 自身存在的优点"></a>8. 自身存在的优点</h2><ol>
<li>乐于沟通，敢于沟通，不管哪个年龄段的人，都能聊到一起去</li>
<li>业务上手能力比较快，热爱大数据这个方向</li>
<li>具有一定的自学能力</li>
<li>坚持一直是我这个人最大的优点</li>
</ol>
<h1 id="归因分析（多渠道归因）"><a href="#归因分析（多渠道归因）" class="headerlink" title="归因分析（多渠道归因）"></a>归因分析（多渠道归因）</h1><h2 id="1-末次归因"><a href="#1-末次归因" class="headerlink" title="1. 末次归因"></a>1. 末次归因</h2><h2 id="2-首次归因"><a href="#2-首次归因" class="headerlink" title="2. 首次归因"></a>2. 首次归因</h2><h2 id="3-位置归因"><a href="#3-位置归因" class="headerlink" title="3.位置归因"></a>3.位置归因</h2><p>首次，末次各占40%；</p>
<p>中间占20%；</p>
<h2 id="4-时间衰减"><a href="#4-时间衰减" class="headerlink" title="4.时间衰减"></a>4.时间衰减</h2><h2 id="5-自定义比重"><a href="#5-自定义比重" class="headerlink" title="5.自定义比重"></a>5.自定义比重</h2><h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1>
    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/06/24/Spring/" rel="prev" title="Spring">
      <i class="fa fa-chevron-left"></i> Spring
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Java%E5%9F%BA%E7%A1%80%E5%A4%8D%E4%B9%A0%EF%BC%88JDK1-8%EF%BC%89"><span class="nav-number">1.</span> <span class="nav-text">Java基础复习（JDK1.8）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%B9%E5%99%A8%E7%AF%87"><span class="nav-number">1.1.</span> <span class="nav-text">容器篇</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-ArrayList"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.ArrayList</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-LinkedList"><span class="nav-number">1.1.2.</span> <span class="nav-text">2.LinkedList</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-HashMap"><span class="nav-number">1.1.3.</span> <span class="nav-text">3.HashMap</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#HashMap%E6%8F%92%E5%85%A5%E5%85%83%E7%B4%A0%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">HashMap插入元素底层原理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%E5%AE%B9%E5%99%A8"><span class="nav-number">1.1.4.</span> <span class="nav-text">4.线程安全的容器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84List"><span class="nav-number">1.1.4.1.</span> <span class="nav-text">线程安全的List</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84HashMap"><span class="nav-number">1.1.4.2.</span> <span class="nav-text">线程安全的HashMap</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ConCurrentHashMap"><span class="nav-number">1.1.4.2.1.</span> <span class="nav-text">ConCurrentHashMap</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#JVM%E7%AF%87"><span class="nav-number">1.2.</span> <span class="nav-text">JVM篇</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-JVM%E7%9A%84%E5%86%85%E5%AD%98%E6%9E%84%E6%88%90"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.JVM的内存构成</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-%E7%A8%8B%E5%BA%8F%E8%AE%A1%E6%95%B0%E5%99%A8%EF%BC%88PC%E5%AF%84%E5%AD%98%E5%99%A8%EF%BC%89"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">1.1 程序计数器（PC寄存器）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BD%9C%E7%94%A8"><span class="nav-number">1.2.1.1.1.</span> <span class="nav-text">作用</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%89%B9%E7%82%B9"><span class="nav-number">1.2.1.1.2.</span> <span class="nav-text">特点</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%A0%88%EF%BC%88Java%E6%A0%88%EF%BC%89"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">1.2 Java虚拟机栈（Java栈）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89"><span class="nav-number">1.2.1.2.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8E%8B%E6%A0%88%E5%87%BA%E6%A0%88%E8%BF%87%E7%A8%8B"><span class="nav-number">1.2.1.2.2.</span> <span class="nav-text">压栈出栈过程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Java%E6%A0%88%E7%9A%84%E7%89%B9%E7%82%B9"><span class="nav-number">1.2.1.2.3.</span> <span class="nav-text">Java栈的特点</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-%E6%9C%AC%E5%9C%B0%E6%96%B9%E6%B3%95%E6%A0%88%EF%BC%88C%E6%A0%88%EF%BC%89"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">1.3 本地方法栈（C栈）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9C%AC%E5%9C%B0%E6%96%B9%E6%B3%95%E6%A0%88%E7%9A%84%E5%AE%9A%E4%B9%89%EF%BC%9A"><span class="nav-number">1.2.1.3.1.</span> <span class="nav-text">本地方法栈的定义：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A0%88%E5%B8%A7%E5%8F%98%E5%8C%96%E8%BF%87%E7%A8%8B"><span class="nav-number">1.2.1.3.2.</span> <span class="nav-text">栈帧变化过程</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-Java%E5%A0%86Heap"><span class="nav-number">1.2.1.4.</span> <span class="nav-text">1.4 Java堆Heap</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-1"><span class="nav-number">1.2.1.4.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%89%B9%E7%82%B9-1"><span class="nav-number">1.2.1.4.2.</span> <span class="nav-text">特点</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A0%86%E7%9A%84%E5%88%92%E5%88%86"><span class="nav-number">1.2.1.4.3.</span> <span class="nav-text">堆的划分</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5"><span class="nav-number">1.2.1.4.4.</span> <span class="nav-text">内存分配策略</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-%E5%AF%B9%E8%B1%A1%E4%BC%98%E5%85%88%E5%88%86%E9%85%8D%E5%9C%A8Eden%E5%8C%BA"><span class="nav-number">1.2.1.4.4.1.</span> <span class="nav-text">1. 对象优先分配在Eden区</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-%E5%A4%A7%E5%AF%B9%E8%B1%A1%E7%9B%B4%E6%8E%A5%E8%BF%9B%E5%85%A5%E8%80%81%E5%B9%B4%E4%BB%A3"><span class="nav-number">1.2.1.4.4.2.</span> <span class="nav-text">2.大对象直接进入老年代</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-%E9%95%BF%E6%9C%9F%E5%AD%98%E6%B4%BB%E7%9A%84%E5%AF%B9%E8%B1%A1%E5%B0%86%E8%BF%9B%E5%85%A5%E8%80%81%E5%B9%B4%E4%BB%A3%EF%BC%88%E9%BB%98%E8%AE%A415%EF%BC%89"><span class="nav-number">1.2.1.4.4.3.</span> <span class="nav-text">3. 长期存活的对象将进入老年代（默认15）</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#4-%E5%8A%A8%E6%80%81%E5%B9%B4%E9%BE%84%E5%88%A4%E5%AE%9A"><span class="nav-number">1.2.1.4.4.4.</span> <span class="nav-text">4. 动态年龄判定</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%80%81%E5%B9%B4%E4%BB%A3%E7%A9%BA%E9%97%B4%E5%88%86%E9%85%8D%E6%8B%85%E4%BF%9D"><span class="nav-number">1.2.1.4.5.</span> <span class="nav-text">老年代空间分配担保</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-%E6%96%B9%E6%B3%95%E5%8C%BA"><span class="nav-number">1.2.1.5.</span> <span class="nav-text">1.5 方法区</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E5%8C%BA%E5%AE%9A%E4%B9%89"><span class="nav-number">1.2.1.5.1.</span> <span class="nav-text">方法区定义</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E5%8C%BA%E7%9A%84%E7%89%B9%E7%82%B9"><span class="nav-number">1.2.1.5.2.</span> <span class="nav-text">方法区的特点</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C%E6%97%B6%E5%B8%B8%E9%87%8F%E6%B1%A0"><span class="nav-number">1.2.1.5.3.</span> <span class="nav-text">运行时常量池</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AD%96%E7%95%A5-amp-%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.2.</span> <span class="nav-text">2. 垃圾收集策略&amp;算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-%E5%88%A4%E6%96%AD%E5%AF%B9%E8%B1%A1%E6%98%AF%E5%90%A6%E5%AD%98%E6%B4%BB"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">2.1 判断对象是否存活</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%93%AA%E4%BA%9B%E5%AF%B9%E8%B1%A1%E5%8F%AF%E4%BB%A5%E4%BD%9C%E4%B8%BAGC-Roots"><span class="nav-number">1.2.2.1.0.1.</span> <span class="nav-text">哪些对象可以作为GC Roots</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">2.1 垃圾收集算法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%A0%87%E8%AE%B0-%E6%B8%85%E9%99%A4%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.2.2.0.1.</span> <span class="nav-text">标记-清除算法</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%A0%87%E8%AE%B0-%E5%A4%8D%E5%88%B6"><span class="nav-number">1.2.2.2.0.2.</span> <span class="nav-text">标记-复制</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%A0%87%E8%AE%B0-%E6%95%B4%E7%90%86"><span class="nav-number">1.2.2.2.0.3.</span> <span class="nav-text">标记-整理</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%88%86%E4%BB%A3%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.2.2.0.4.</span> <span class="nav-text">分代收集算法</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-HotSpot%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%EF%BC%887%E7%A7%8D%EF%BC%89"><span class="nav-number">1.2.3.</span> <span class="nav-text">3. HotSpot垃圾收集器（7种）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-%E6%96%B0%E7%94%9F%E4%BB%A3%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">3.1 新生代垃圾收集器</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-Serial-GC%E6%94%B6%E9%9B%86%E5%99%A8"><span class="nav-number">1.2.3.1.1.</span> <span class="nav-text">1. Serial GC收集器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-ParNew%E6%94%B6%E9%9B%86%E5%99%A8"><span class="nav-number">1.2.3.1.2.</span> <span class="nav-text">2. ParNew收集器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-Parallel-Scavenge%E6%94%B6%E9%9B%86%E5%99%A8%EF%BC%88%E5%90%9E%E5%90%90%E9%87%8F%E4%BC%98%E5%85%88%EF%BC%89"><span class="nav-number">1.2.3.1.3.</span> <span class="nav-text">3. Parallel Scavenge收集器（吞吐量优先）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-%E8%80%81%E5%B9%B4%E4%BB%A3%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">3.2 老年代垃圾收集器</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-Serial-Old%E6%94%B6%E9%9B%86%E5%99%A8"><span class="nav-number">1.2.3.2.1.</span> <span class="nav-text">1. Serial Old收集器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-Parallel-Old"><span class="nav-number">1.2.3.2.2.</span> <span class="nav-text">2. Parallel Old</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-CMS%E5%B9%B6%E5%8F%91%E6%B8%85%E9%99%A4%EF%BC%88Concurrent-Mark-Sweep%EF%BC%89"><span class="nav-number">1.2.3.2.3.</span> <span class="nav-text">3. CMS并发清除（Concurrent Mark Sweep）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-G1%E6%94%B6%E9%9B%86%E5%99%A8"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">3.3 G1收集器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93%EF%BC%9A"><span class="nav-number">1.2.3.4.</span> <span class="nav-text">小结：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.类加载过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE%E6%9C%BA%E5%88%B6"><span class="nav-number">1.2.5.</span> <span class="nav-text">3. 双亲委派机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%88%9B%E5%BB%BA%EF%BC%88new%EF%BC%89%E5%AF%B9%E8%B1%A1%E7%9A%84%E8%BF%87%E7%A8%8B"><span class="nav-number">1.2.6.</span> <span class="nav-text">3.创建（new）对象的过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%AF%B9%E8%B1%A1%E7%9A%84%E5%86%85%E5%AD%98%E5%B8%83%E5%B1%80"><span class="nav-number">1.2.7.</span> <span class="nav-text">4.对象的内存布局</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E5%9B%9B%E5%A4%A7%E5%BC%95%E7%94%A8%E7%B1%BB%E5%9E%8B"><span class="nav-number">1.2.8.</span> <span class="nav-text">5. 四大引用类型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%BC%BA%E5%BC%95%E7%94%A8"><span class="nav-number">1.2.8.1.</span> <span class="nav-text">1. 强引用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%BD%AF%E5%BC%95%E7%94%A8"><span class="nav-number">1.2.8.2.</span> <span class="nav-text">2. 软引用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E5%BC%B1%E5%BC%95%E7%94%A8"><span class="nav-number">1.2.8.3.</span> <span class="nav-text">3. 弱引用</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ThreadLocal-%E5%BC%B1%E5%BC%95%E7%94%A8%E9%80%A0%E6%88%90%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B3%84%E6%BC%8F%E9%97%AE%E9%A2%98"><span class="nav-number">1.2.8.3.1.</span> <span class="nav-text">ThreadLocal 弱引用造成的数据泄漏问题</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E8%99%9A%E5%BC%95%E7%94%A8"><span class="nav-number">1.2.8.4.</span> <span class="nav-text">4. 虚引用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-JVM%E5%B8%B8%E7%94%A8%E8%B0%83%E4%BC%98%E5%8F%82%E6%95%B0"><span class="nav-number">1.2.9.</span> <span class="nav-text">6. JVM常用调优参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%B6%E5%8F%91%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%AF%87"><span class="nav-number">1.3.</span> <span class="nav-text">并发与多线程篇</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B"><span class="nav-number">1.3.1.</span> <span class="nav-text">1. 进程与线程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%87%A0%E7%A7%8D%E7%8A%B6%E6%80%81"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">线程的几种状态</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Java%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%A4%9A%E7%BA%BF%E7%9A%84%E6%96%B9%E5%BC%8F"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">Java中实现多线的方式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-JUC"><span class="nav-number">1.3.2.</span> <span class="nav-text">2. JUC</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#JUC%E5%BC%BA%E5%A4%A7%E7%9A%84%E8%BE%85%E5%8A%A9%E7%B1%BB%EF%BC%9A"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">JUC强大的辅助类：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-CountDownLatch%E7%B1%BB"><span class="nav-number">1.3.2.1.1.</span> <span class="nav-text">1. CountDownLatch类</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-CyclicBarrier"><span class="nav-number">1.3.2.1.2.</span> <span class="nav-text">2. CyclicBarrier</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BF%A1%E5%8F%B7%E9%87%8F%EF%BC%9ASemaphore%E7%B1%BB%EF%BC%88%E7%B1%BB%E4%BC%BC%E4%BA%8EPV%E6%93%8D%E4%BD%9C%EF%BC%89"><span class="nav-number">1.3.2.1.3.</span> <span class="nav-text">信号量：Semaphore类（类似于PV操作）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97%EF%BC%88BlockingQueue%EF%BC%89"><span class="nav-number">1.3.3.</span> <span class="nav-text">阻塞队列（BlockingQueue）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E7%BA%BF%E7%A8%8B%E6%B1%A0"><span class="nav-number">1.3.4.</span> <span class="nav-text">3. 线程池</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E6%B1%A0%E6%8B%92%E7%BB%9D%E7%AD%96%E7%95%A5"><span class="nav-number">1.3.4.1.</span> <span class="nav-text">线程池拒绝策略</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-volatile-amp-JMM%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.5.</span> <span class="nav-text">4. volatile&amp;JMM内存模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#JMM%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.5.1.</span> <span class="nav-text">JMM内存模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#volatile"><span class="nav-number">1.3.5.2.</span> <span class="nav-text">volatile</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#volatile%E7%89%B9%E6%80%A7%EF%BC%9A"><span class="nav-number">1.3.5.2.1.</span> <span class="nav-text">volatile特性：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-synchronized%E5%8E%9F%E7%90%86"><span class="nav-number">1.3.6.</span> <span class="nav-text">5. synchronized原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E9%94%81-amp-%E9%94%81%E5%8D%87%E7%BA%A7"><span class="nav-number">1.3.7.</span> <span class="nav-text">6.锁&amp;锁升级</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-AQS%EF%BC%88AbstractQueuedSynchronizer%EF%BC%89%EF%BC%9A%E6%8A%BD%E8%B1%A1%E9%98%9F%E5%88%97%E5%90%8C%E6%AD%A5%E5%99%A8"><span class="nav-number">1.3.8.</span> <span class="nav-text">7.AQS（AbstractQueuedSynchronizer）：抽象队列同步器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CLH"><span class="nav-number">1.3.8.1.</span> <span class="nav-text">CLH</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AQS%E5%88%9D%E6%AD%A5"><span class="nav-number">1.3.8.2.</span> <span class="nav-text">AQS初步</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#AQS%E5%88%9D%E8%AF%86"><span class="nav-number">1.3.8.2.1.</span> <span class="nav-text">AQS初识</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#AQS%E5%86%85%E9%83%A8%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84"><span class="nav-number">1.3.8.2.2.</span> <span class="nav-text">AQS内部体系架构</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.4.</span> <span class="nav-text">常用设计模式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.4.1.</span> <span class="nav-text">1.单例模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.4.2.</span> <span class="nav-text">2.工厂模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%A8%A1%E6%9D%BF%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.4.3.</span> <span class="nav-text">3.模板模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.4.4.</span> <span class="nav-text">4.代理模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.4.5.</span> <span class="nav-text">5.建造者模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.4.6.</span> <span class="nav-text">6.观察者模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E5%8E%9F%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.4.7.</span> <span class="nav-text">7. 原型设计模式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F%E7%9A%84%E4%BC%98%E7%82%B9%EF%BC%9A"><span class="nav-number">1.4.7.1.</span> <span class="nav-text">原型模式的优点：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F%E7%9A%84%E7%BC%BA%E7%82%B9%EF%BC%9A"><span class="nav-number">1.4.7.2.</span> <span class="nav-text">原型模式的缺点：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.4.7.3.</span> <span class="nav-text">原型模式的实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%97%E4%B8%BEJava%E5%87%A0%E4%B8%AA%E5%BC%82%E5%B8%B8"><span class="nav-number">1.5.</span> <span class="nav-text">列举Java几个异常</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Linux-%E5%A4%8D%E4%B9%A0"><span class="nav-number">2.</span> <span class="nav-text">Linux 复习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="nav-number">2.1.</span> <span class="nav-text">常用命令</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#top%E8%AF%A6%E8%A7%A3"><span class="nav-number">2.1.1.</span> <span class="nav-text">top详解</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MySql%E5%A4%8D%E4%B9%A0"><span class="nav-number">3.</span> <span class="nav-text">MySql复习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%89%E8%8C%83%E5%BC%8F"><span class="nav-number">3.1.</span> <span class="nav-text">1. 数据库三范式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-MySql%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E"><span class="nav-number">3.2.</span> <span class="nav-text">2.MySql存储引擎</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E7%B4%A2%E5%BC%95"><span class="nav-number">3.3.</span> <span class="nav-text">3. 索引</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E4%BB%80%E4%B9%88%E6%98%AF%E7%B4%A2%E5%BC%95%EF%BC%9F"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.1 什么是索引？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E7%B4%A2%E5%BC%95%E7%9A%84%E4%BC%98%E7%82%B9"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.2 索引的优点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E7%B4%A2%E5%BC%95%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="nav-number">3.3.3.</span> <span class="nav-text">3.3 索引的缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-%E7%B4%A2%E5%BC%95%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">3.3.4.</span> <span class="nav-text">3.4 索引使用注意事项</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-MySql%E6%97%A5%E5%BF%97"><span class="nav-number">3.4.</span> <span class="nav-text">4. MySql日志</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%97%A5%E5%BF%97"><span class="nav-number">3.4.1.</span> <span class="nav-text">4.1 二进制日志</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E4%BA%8B%E5%8A%A1%E6%97%A5%E5%BF%97"><span class="nav-number">3.4.2.</span> <span class="nav-text">4.2 事务日志</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-redo-log"><span class="nav-number">3.4.2.1.</span> <span class="nav-text">1. redo log</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-undo-log"><span class="nav-number">3.4.2.2.</span> <span class="nav-text">2. undo log</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E4%BA%8B%E5%8A%A1"><span class="nav-number">3.4.3.</span> <span class="nav-text">4. 事务</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%8B%E5%8A%A1%EF%BC%9F"><span class="nav-number">3.4.3.1.</span> <span class="nav-text">4.1 什么是事务？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7%EF%BC%88ACID%EF%BC%89"><span class="nav-number">3.4.3.2.</span> <span class="nav-text">4.2 事务的四大特性（ACID）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-1-%E5%8E%9F%E5%AD%90%E6%80%A7"><span class="nav-number">3.4.3.2.1.</span> <span class="nav-text">4.2.1 原子性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-2-%E6%8C%81%E4%B9%85%E6%80%A7"><span class="nav-number">3.4.3.2.2.</span> <span class="nav-text">4.2.2 持久性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-3-%E9%9A%94%E7%A6%BB%E6%80%A7"><span class="nav-number">3.4.3.2.3.</span> <span class="nav-text">4.2.3 隔离性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-4-%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">3.4.3.2.4.</span> <span class="nav-text">4.2.4 一致性</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E9%94%81"><span class="nav-number">3.5.</span> <span class="nav-text">5. 锁</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E9%94%81"><span class="nav-number">3.5.1.</span> <span class="nav-text">全局锁</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%A8%E7%BA%A7%E9%94%81"><span class="nav-number">3.5.2.</span> <span class="nav-text">表级锁</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%8C%E9%94%81"><span class="nav-number">3.5.3.</span> <span class="nav-text">行锁</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%B4%E9%9A%99%E9%94%81"><span class="nav-number">3.5.4.</span> <span class="nav-text">间隙锁</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="nav-number">3.6.</span> <span class="nav-text">6. 性能分析与优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exlain%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92"><span class="nav-number">3.6.1.</span> <span class="nav-text">Exlain查询执行计划</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Mysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6-amp-%E9%9B%86%E7%BE%A4"><span class="nav-number">3.7.</span> <span class="nav-text">7. Mysql主从复制 &amp; 集群</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-MySql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6"><span class="nav-number">3.7.1.</span> <span class="nav-text">7.1 MySql主从复制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-Mysql%E9%9B%86%E7%BE%A4"><span class="nav-number">3.7.2.</span> <span class="nav-text">7.2 Mysql集群</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-%E8%A1%A5%E5%85%85"><span class="nav-number">3.8.</span> <span class="nav-text">8. 补充</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-SQL%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F"><span class="nav-number">3.8.1.</span> <span class="nav-text">8.1 SQL的生命周期</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Hadoop%E5%A4%8D%E4%B9%A0"><span class="nav-number">4.</span> <span class="nav-text">Hadoop复习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9FHDFS"><span class="nav-number">4.1.</span> <span class="nav-text">1. 分布式文件存储系统HDFS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-HDFS%E7%9A%84%E6%9C%BA%E6%9E%B6%E6%84%9F%E7%9F%A5%E7%AD%96%E7%95%A5"><span class="nav-number">4.1.1.</span> <span class="nav-text">1.1 HDFS的机架感知策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-NameNode-amp-DataNode-amp-Secondary-NameNode"><span class="nav-number">4.1.2.</span> <span class="nav-text">1.2 NameNode &amp; DataNode &amp; Secondary NameNode</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#NameNode"><span class="nav-number">4.1.2.1.</span> <span class="nav-text">NameNode</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-SecondaryNameNode%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="nav-number">4.1.2.2.</span> <span class="nav-text">1.3 SecondaryNameNode工作原理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-DataNode"><span class="nav-number">4.1.3.</span> <span class="nav-text">1.4 DataNode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-HDFS-%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87"><span class="nav-number">4.1.4.</span> <span class="nav-text">1.5 HDFS 设计目标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-6-HDFS%E7%9A%84%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F"><span class="nav-number">4.1.5.</span> <span class="nav-text">1.6 HDFS的文件存储格式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%8C%E5%BC%8F%E5%AD%98%E5%82%A8"><span class="nav-number">4.1.5.1.</span> <span class="nav-text">行式存储</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8"><span class="nav-number">4.1.5.2.</span> <span class="nav-text">列式存储</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-7-HDFS%E7%9A%84%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B"><span class="nav-number">4.1.6.</span> <span class="nav-text">1.7 HDFS的读写流程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-7-1-HDFS-%E7%9A%84-%E5%86%99%E6%B5%81%E7%A8%8B"><span class="nav-number">4.1.6.1.</span> <span class="nav-text">1.7.1 HDFS 的 写流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-7-2-HDFS-%E7%9A%84-%E8%AF%BB%E6%B5%81%E7%A8%8B"><span class="nav-number">4.1.6.2.</span> <span class="nav-text">1.7.2 HDFS 的 读流程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6MapReduce"><span class="nav-number">4.2.</span> <span class="nav-text">2. 分布式计算框架MapReduce</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-MapReduce%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="nav-number">4.2.1.</span> <span class="nav-text">2.1 MapReduce工作流程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-MapTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="nav-number">4.2.1.1.</span> <span class="nav-text">2.1.1 MapTask工作机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-ReduceTask%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="nav-number">4.2.1.2.</span> <span class="nav-text">2.1.2 ReduceTask工作机制</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Shuffle%E6%9C%BA%E5%88%B6"><span class="nav-number">4.2.2.</span> <span class="nav-text">2.2 Shuffle机制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%99%A8Yarn"><span class="nav-number">4.3.</span> <span class="nav-text">3. 集群资源管理器Yarn</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Yarn%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84"><span class="nav-number">4.3.1.</span> <span class="nav-text">Yarn的基本架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Yarn%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="nav-number">4.3.2.</span> <span class="nav-text">Yarn工作机制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E8%A1%A5%E5%85%85"><span class="nav-number">4.4.</span> <span class="nav-text">4. 补充</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Hadoop%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E7%AB%AF%E5%8F%A3%E5%8F%B7"><span class="nav-number">4.4.1.</span> <span class="nav-text">4.1 Hadoop中的常用端口号</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%AB%98%E5%8F%AF%E7%94%A8"><span class="nav-number">4.5.</span> <span class="nav-text">高可用</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark%E5%A4%8D%E4%B9%A0"><span class="nav-number">5.</span> <span class="nav-text">Spark复习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Spark-Core"><span class="nav-number">5.1.</span> <span class="nav-text">1. Spark Core</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90"><span class="nav-number">5.1.1.</span> <span class="nav-text">1.1 转换算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90"><span class="nav-number">5.1.2.</span> <span class="nav-text">1.2 行动算子</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-RDD%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="nav-number">5.2.</span> <span class="nav-text">2. RDD的依赖关系</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%AA%84%E4%BE%9D%E8%B5%96"><span class="nav-number">5.2.1.</span> <span class="nav-text">1. 窄依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%AE%BD%E4%BE%9D%E8%B5%96"><span class="nav-number">5.2.2.</span> <span class="nav-text">2. 宽依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Spark-Job%E7%9A%84%E5%88%92%E5%88%86"><span class="nav-number">5.2.3.</span> <span class="nav-text">3. Spark Job的划分</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#job"><span class="nav-number">5.2.3.1.</span> <span class="nav-text">job</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#stages"><span class="nav-number">5.2.3.2.</span> <span class="nav-text">stages</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tasks"><span class="nav-number">5.2.3.3.</span> <span class="nav-text">Tasks</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-RDD%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">5.2.4.</span> <span class="nav-text">4. RDD的持久化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5%E7%82%B9checkpoint"><span class="nav-number">5.2.4.1.</span> <span class="nav-text">检查点checkpoint</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%81%E4%B9%85%E5%8C%96%E4%B8%8Echeckpoint%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">5.2.4.2.</span> <span class="nav-text">持久化与checkpoint的区别</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Key-Value%E7%B1%BB%E5%9E%8BRDD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%86%E5%8C%BA%E5%99%A8"><span class="nav-number">5.3.</span> <span class="nav-text">3. Key-Value类型RDD的数据分区器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-HashPartitioner"><span class="nav-number">5.3.1.</span> <span class="nav-text">3.1 HashPartitioner</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-RangePartitioner%E8%8C%83%E5%9B%B4%E5%88%86%E5%8C%BA%E5%99%A8"><span class="nav-number">5.3.2.</span> <span class="nav-text">3.2 RangePartitioner范围分区器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F"><span class="nav-number">5.4.</span> <span class="nav-text">4. 共享变量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">5.4.0.1.</span> <span class="nav-text">累加器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="nav-number">5.4.0.2.</span> <span class="nav-text">广播变量</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-SparkSQL"><span class="nav-number">5.5.</span> <span class="nav-text">2. SparkSQL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD%EF%BC%8C-DataFrame%E5%92%8CDataSet%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-number">5.5.1.</span> <span class="nav-text">RDD， DataFrame和DataSet之间的关系</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD%EF%BC%8CDataFrame%E5%92%8CDataSet%E4%B8%89%E8%80%85%E7%9A%84%E5%85%B1%E6%80%A7"><span class="nav-number">5.5.1.1.</span> <span class="nav-text">RDD，DataFrame和DataSet三者的共性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%89%E8%80%85%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">5.5.1.2.</span> <span class="nav-text">三者的区别</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-on-Hive-%E4%B8%8E-Hive-on-Spark"><span class="nav-number">5.5.2.</span> <span class="nav-text">Spark on Hive 与 Hive on Spark</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-SQL-Text-%E8%BD%AC%E5%8C%96%E4%B8%BA%E5%AE%9E%E9%99%85%E7%9A%84%E7%89%A9%E7%90%86%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%B5%81%E7%A8%8B"><span class="nav-number">5.5.3.</span> <span class="nav-text">Spark SQL Text 转化为实际的物理任务的流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-SQL-%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88version%EF%BC%9ASpark2-amp-amp-Spark3-1-2%EF%BC%89"><span class="nav-number">5.5.4.</span> <span class="nav-text">Spark SQL 源码分析（version：Spark2 &amp;&amp; Spark3.1.2）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86%EF%BC%9AAntlr4-%E8%AF%AD%E6%B3%95%E7%94%9F%E6%88%90%E5%99%A8%E5%B7%A5%E5%85%B7"><span class="nav-number">5.5.4.1.</span> <span class="nav-text">前置知识：Antlr4 语法生成器工具</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90"><span class="nav-number">5.5.4.2.</span> <span class="nav-text">Spark2源码分析</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Spark-%E5%86%85%E6%A0%B8"><span class="nav-number">5.6.</span> <span class="nav-text">3. Spark 内核</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1%E6%B5%81%E7%A8%8B"><span class="nav-number">5.6.1.</span> <span class="nav-text">Spark提交任务流程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-Yarn-Cluster-%E6%A8%A1%E5%BC%8F"><span class="nav-number">5.6.1.1.</span> <span class="nav-text">1. Yarn Cluster 模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Yarn-Client%E6%A8%A1%E5%BC%8F"><span class="nav-number">5.6.1.2.</span> <span class="nav-text">Yarn Client模式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">5.6.2.</span> <span class="nav-text"></span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Spark%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%9C%BA%E5%88%B6"><span class="nav-number">5.7.</span> <span class="nav-text">4. Spark任务调度机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Spark-%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E6%A6%82%E8%BF%B0"><span class="nav-number">5.7.1.</span> <span class="nav-text">4.1 Spark 任务调度概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Spark-Stage%E7%BA%A7%E5%88%AB%E7%9A%84%E8%B0%83%E5%BA%A6"><span class="nav-number">5.7.2.</span> <span class="nav-text">4.2 Spark Stage级别的调度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Spark-Task%E7%BA%A7%E5%88%AB%E7%9A%84%E8%B0%83%E5%BA%A6"><span class="nav-number">5.7.3.</span> <span class="nav-text">4.3 Spark Task级别的调度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-%E8%B0%83%E5%BA%A6%E7%AD%96%E7%95%A5"><span class="nav-number">5.7.4.</span> <span class="nav-text">4.4 调度策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-%E6%9C%AC%E5%9C%B0%E5%8C%96%E8%B0%83%E5%BA%A6"><span class="nav-number">5.7.5.</span> <span class="nav-text">4.5 本地化调度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Spark-Shuffle%E8%A7%A3%E6%9E%90"><span class="nav-number">5.8.</span> <span class="nav-text">5. Spark Shuffle解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-HashShuffle"><span class="nav-number">5.8.1.</span> <span class="nav-text">5.1 HashShuffle</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-1-%E6%9C%AA%E4%BC%98%E5%8C%96%E7%9A%84HashShuffle"><span class="nav-number">5.8.1.1.</span> <span class="nav-text">5.1.1 未优化的HashShuffle</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-2-%E4%BC%98%E5%8C%96%E5%90%8E%E7%9A%84HashShuffle"><span class="nav-number">5.8.1.2.</span> <span class="nav-text">5.1.2 优化后的HashShuffle</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-SortShuffle"><span class="nav-number">5.8.2.</span> <span class="nav-text">5.2 SortShuffle</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-1-%E6%99%AE%E9%80%9A%E7%9A%84SortShuffle"><span class="nav-number">5.8.2.1.</span> <span class="nav-text">5.2.1 普通的SortShuffle</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-2-bypassSortShuffle"><span class="nav-number">5.8.2.2.</span> <span class="nav-text">5.2.2 bypassSortShuffle</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="nav-number">5.9.</span> <span class="nav-text">6. Spark内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E7%A9%BA%E9%97%B4%E5%88%86%E9%85%8D"><span class="nav-number">5.9.1.</span> <span class="nav-text">内存空间分配</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E9%9D%99%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="nav-number">5.9.1.1.</span> <span class="nav-text">1.  静态内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-1-%E5%A0%86%E5%86%85%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="nav-number">5.9.1.1.1.</span> <span class="nav-text">1.1 堆内内存管理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="nav-number">5.9.1.1.2.</span> <span class="nav-text">1.2 堆外内存管理</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="nav-number">5.9.1.2.</span> <span class="nav-text">2. 统一内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1-%E7%BB%9F%E4%B8%80%E5%A0%86%E5%86%85%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="nav-number">5.9.1.2.1.</span> <span class="nav-text">2.1 统一堆内内存管理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-%E7%BB%9F%E4%B8%80%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="nav-number">5.9.1.2.2.</span> <span class="nav-text">2.2 统一堆外内存管理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%98%E5%82%A8%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="nav-number">5.9.2.</span> <span class="nav-text">存储内存管理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96"><span class="nav-number">5.10.</span> <span class="nav-text">7. Spark性能优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-Spark-%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0"><span class="nav-number">5.10.1.</span> <span class="nav-text">7.1 Spark 常用配置参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-%E5%B8%B8%E8%A7%84%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E4%B8%80%EF%BC%9ARDD%E5%A4%8D%E7%94%A8"><span class="nav-number">5.10.2.</span> <span class="nav-text">7.2 常规性能调优一：RDD复用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-%E5%B8%B8%E8%A7%84%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E4%BA%8C%EF%BC%9ARDD%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">5.10.3.</span> <span class="nav-text">7.2 常规性能调优二：RDD持久化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-%E5%B8%B8%E8%A7%84%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E4%B8%89%EF%BC%9A%E5%B9%B6%E8%A1%8C%E5%BA%A6%E8%B0%83%E8%8A%82"><span class="nav-number">5.10.4.</span> <span class="nav-text">7.3 常规性能调优三：并行度调节</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-%E5%B8%B8%E8%A7%84%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%9B%9B%EF%BC%9A%E5%B9%BF%E6%92%AD%E5%A4%A7%E5%8F%98%E9%87%8F"><span class="nav-number">5.10.5.</span> <span class="nav-text">7.4 常规性能调优四：广播大变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-%E7%AE%97%E5%AD%90%E8%B0%83%E4%BC%98"><span class="nav-number">5.10.6.</span> <span class="nav-text">7.5 算子调优</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-mapPartitions"><span class="nav-number">5.10.6.1.</span> <span class="nav-text">1. mapPartitions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-foreachPartition%E4%BC%98%E5%8C%96%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C"><span class="nav-number">5.10.6.2.</span> <span class="nav-text">2. foreachPartition优化数据库操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-filter%E4%B8%8Ecoalesce%E7%9A%84%E9%85%8D%E5%90%88%E4%BD%BF%E7%94%A8"><span class="nav-number">5.10.6.3.</span> <span class="nav-text">3. filter与coalesce的配合使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-repartition-%E8%A7%A3%E5%86%B3SparkSQL%E4%BD%8E%E5%B9%B6%E8%A1%8C%E5%BA%A6%E9%97%AE%E9%A2%98"><span class="nav-number">5.10.6.4.</span> <span class="nav-text">4. repartition 解决SparkSQL低并行度问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-reduceByKey%E9%A2%84%E8%81%9A%E5%90%88"><span class="nav-number">5.10.6.5.</span> <span class="nav-text">5. reduceByKey预聚合</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-6-Shuffle%E8%B0%83%E4%BC%98"><span class="nav-number">5.10.7.</span> <span class="nav-text">7.6 Shuffle调优</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%B0%83%E8%8A%82map%E7%AB%AF%E7%BC%93%E5%86%B2%E5%8C%BA%E5%A4%A7%E5%B0%8F"><span class="nav-number">5.10.7.1.</span> <span class="nav-text">1. 调节map端缓冲区大小</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%B0%83%E8%8A%82reduce%E7%AB%AF%E7%BC%93%E5%86%B2%E5%8C%BA%E5%A4%A7%E5%B0%8F"><span class="nav-number">5.10.7.2.</span> <span class="nav-text">2. 调节reduce端缓冲区大小</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E8%B0%83%E8%8A%82reduce%E7%AB%AF%E6%8B%89%E5%8F%96%E6%95%B0%E6%8D%AE%E9%87%8D%E8%AF%95%E6%AC%A1%E6%95%B0"><span class="nav-number">5.10.7.3.</span> <span class="nav-text">3. 调节reduce端拉取数据重试次数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E8%B0%83%E8%8A%82reduce%E7%AB%AF%E6%8B%89%E5%8F%96%E6%95%B0%E6%8D%AE%E7%AD%89%E5%BE%85%E9%97%B4%E9%9A%94"><span class="nav-number">5.10.7.4.</span> <span class="nav-text">4. 调节reduce端拉取数据等待间隔</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E8%B0%83%E8%8A%82SortShuffle%E6%8E%92%E5%BA%8F%E6%93%8D%E4%BD%9C%E9%98%88%E5%80%BC"><span class="nav-number">5.10.7.5.</span> <span class="nav-text">5. 调节SortShuffle排序操作阈值</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-7-%E5%86%85%E5%AD%98%E8%B0%83%E4%BC%98"><span class="nav-number">5.10.8.</span> <span class="nav-text">7.7 内存调优</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">5.11.</span> <span class="nav-text">8. Spark数据倾斜解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-%E8%81%9A%E5%90%88%E5%8E%9F%E6%95%B0%E6%8D%AE"><span class="nav-number">5.11.1.</span> <span class="nav-text">8.1 聚合原数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-%E8%BF%87%E6%BB%A4%E6%8E%89%E5%AF%BC%E8%87%B4%E5%80%BE%E6%96%9C%E7%9A%84key"><span class="nav-number">5.11.2.</span> <span class="nav-text">8.2 过滤掉导致倾斜的key</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-%E6%8F%90%E9%AB%98shuffle%E6%93%8D%E4%BD%9C%E4%B8%AD%E7%9A%84reduce%E5%B9%B6%E8%A1%8C%E5%BA%A6"><span class="nav-number">5.11.3.</span> <span class="nav-text">8.3 提高shuffle操作中的reduce并行度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-%E4%BD%BF%E7%94%A8%E9%9A%8F%E6%9C%BAkey%E5%AE%9E%E7%8E%B0%E5%8F%8C%E9%87%8D%E8%81%9A%E5%90%88"><span class="nav-number">5.11.4.</span> <span class="nav-text">8.4 使用随机key实现双重聚合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-5-%E5%B0%86reduce-join-%E8%BD%AC%E6%8D%A2%E6%88%90map-join"><span class="nav-number">5.11.5.</span> <span class="nav-text">8.5 将reduce join 转换成map join</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-6-sample%E9%87%87%E6%A0%B7%E5%AF%B9%E5%80%BE%E6%96%9Ckey%E5%8D%95%E7%8B%AC%E8%BF%9B%E8%A1%8Cjoin"><span class="nav-number">5.11.6.</span> <span class="nav-text">8.6 sample采样对倾斜key单独进行join</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-7-%E4%BD%BF%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%95%B0%E4%BB%A5%E5%8F%8A%E6%89%A9%E5%AE%B9%E8%BF%9B%E8%A1%8Cjoin"><span class="nav-number">5.11.7.</span> <span class="nav-text">8.7 使用随机数以及扩容进行join</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-8-%E6%9B%B4%E6%8D%A2%E5%88%86%E5%8C%BA%E5%99%A8%EF%BC%88RangePartitioner%EF%BC%89"><span class="nav-number">5.11.8.</span> <span class="nav-text">8.8 更换分区器（RangePartitioner）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-9-%E4%B8%A4%E5%BC%A0%E5%A4%A7%E7%9A%84%E8%A1%A8%E8%BF%9B%E8%A1%8Cjoin%EF%BC%88%E6%97%A0%E6%B3%95%E5%B9%BF%E6%92%AD%EF%BC%89%EF%BC%8C%E4%BA%A7%E7%94%9F%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C"><span class="nav-number">5.11.9.</span> <span class="nav-text">8.9 两张大的表进行join（无法广播），产生数据倾斜</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-10-%E5%A2%9E%E5%A4%A7%E5%B9%B6%E8%A1%8C%E5%BA%A6%EF%BC%8C%E5%A2%9E%E5%A4%A7%E8%B5%84%E6%BA%90"><span class="nav-number">5.11.10.</span> <span class="nav-text">8.10 增大并行度，增大资源</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-Spark%E4%B8%AD%E6%B6%89%E5%8F%8A%E5%88%B0%E7%9A%84join"><span class="nav-number">5.12.</span> <span class="nav-text">9. Spark中涉及到的join</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Hash-Join"><span class="nav-number">5.12.1.</span> <span class="nav-text">Hash Join</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Broadcast-Hash-Join"><span class="nav-number">5.12.1.1.</span> <span class="nav-text">Broadcast Hash Join</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Shuffle-Hash-Join"><span class="nav-number">5.12.1.2.</span> <span class="nav-text">Shuffle Hash Join</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sort-Merge-Join"><span class="nav-number">5.12.2.</span> <span class="nav-text">Sort-Merge Join</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Join%E6%97%B6%E9%80%9A%E8%BF%87BloomFilter%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%E8%BF%9B%E8%A1%8C-%E8%BF%9B%E8%A1%8C%E8%B0%93%E8%AF%8D%E4%B8%8B%E6%8E%A8"><span class="nav-number">5.12.3.</span> <span class="nav-text">Join时通过BloomFilter布隆过滤器进行 进行谓词下推</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-Spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90"><span class="nav-number">5.13.</span> <span class="nav-text">10. Spark源码分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87%EF%BC%88Yarn-%E9%9B%86%E7%BE%A4%EF%BC%89"><span class="nav-number">5.13.1.</span> <span class="nav-text">1. 环境准备（Yarn 集群）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%BB%84%E4%BB%B6%E9%80%9A%E4%BF%A1"><span class="nav-number">5.13.2.</span> <span class="nav-text">2. 组件通信</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%89%A7%E8%A1%8C"><span class="nav-number">5.13.3.</span> <span class="nav-text">3. 应用程序的执行</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-RDD%E4%BE%9D%E8%B5%96"><span class="nav-number">5.13.3.1.</span> <span class="nav-text">1. RDD依赖</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%AA%84%E4%BE%9D%E8%B5%96NarrowDependency"><span class="nav-number">5.13.3.1.1.</span> <span class="nav-text">窄依赖NarrowDependency</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%BD%E4%BE%9D%E8%B5%96ShuffleDependency"><span class="nav-number">5.13.3.1.2.</span> <span class="nav-text">宽依赖ShuffleDependency</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B0%8F%E6%80%BB%E7%BB%93"><span class="nav-number">5.13.3.1.3.</span> <span class="nav-text">小总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E9%98%B6%E6%AE%B5%E7%9A%84%E5%88%92%E5%88%86"><span class="nav-number">5.13.3.2.</span> <span class="nav-text">2. 阶段的划分</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B0%8F%E6%80%BB%E7%BB%93-1"><span class="nav-number">5.13.3.2.1.</span> <span class="nav-text">小总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%88%87%E5%88%86"><span class="nav-number">5.13.3.3.</span> <span class="nav-text">3. 任务的切分</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B0%8F%E6%80%BB%E7%BB%93-2"><span class="nav-number">5.13.3.3.1.</span> <span class="nav-text">小总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%B0%83%E5%BA%A6"><span class="nav-number">5.13.3.4.</span> <span class="nav-text">4. 任务的调度</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B0%8F%E6%80%BB%E7%BB%93-3"><span class="nav-number">5.13.3.4.1.</span> <span class="nav-text">小总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%89%A7%E8%A1%8C"><span class="nav-number">5.13.3.5.</span> <span class="nav-text">5. 任务的执行</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B0%8F%E6%80%BB%E7%BB%93-4"><span class="nav-number">5.13.3.5.1.</span> <span class="nav-text">小总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">5.13.3.6.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Shuffle"><span class="nav-number">5.13.4.</span> <span class="nav-text">4. Shuffle</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-Shuffle%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B"><span class="nav-number">5.13.4.1.</span> <span class="nav-text">1. Shuffle的原理和执行过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Shuffle%E5%86%99%E7%A3%81%E7%9B%98"><span class="nav-number">5.13.4.2.</span> <span class="nav-text">2. Shuffle写磁盘</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-Shuffle%E8%AF%BB%E5%8F%96%E7%A3%81%E7%9B%98"><span class="nav-number">5.13.4.3.</span> <span class="nav-text">3. Shuffle读取磁盘</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E5%86%85%E5%AD%98%E7%9A%84%E7%AE%A1%E7%90%86"><span class="nav-number">5.13.5.</span> <span class="nav-text">5. 内存的管理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%86%85%E5%AD%98%E7%9A%84%E5%88%86%E7%B1%BB"><span class="nav-number">5.13.5.1.</span> <span class="nav-text">1. 内存的分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%86%85%E5%AD%98%E7%9A%84%E9%85%8D%E7%BD%AE"><span class="nav-number">5.13.5.2.</span> <span class="nav-text">2. 内存的配置</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-%E9%9D%A2%E8%AF%95%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">5.14.</span> <span class="nav-text">11. 面试中遇到的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%A6%82%E4%BD%95%E5%87%8F%E5%B0%91%E4%BB%BB%E5%8A%A1%E7%9A%84%E7%BD%91%E7%BB%9CIO"><span class="nav-number">5.14.1.</span> <span class="nav-text">1. 如何减少任务的网络IO</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Hive-%E5%A4%8D%E4%B9%A0"><span class="nav-number">6.</span> <span class="nav-text">Hive 复习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%A4%96%E9%83%A8%E8%A1%A8%E4%B8%8E%E5%86%85%E9%83%A8%E8%A1%A8"><span class="nav-number">6.0.1.</span> <span class="nav-text">1. 外部表与内部表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%88%86%E5%8C%BA%E4%B8%8E%E5%88%86%E6%A1%B6"><span class="nav-number">6.0.2.</span> <span class="nav-text">2. 分区与分桶</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Hive-%E6%8A%BD%E6%A0%B7"><span class="nav-number">6.0.3.</span> <span class="nav-text">3. Hive 抽样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-order-by-%E3%80%81-distribute-by-%E3%80%81sort-by-%E5%92%8C-cluster-by-%E5%9B%9B%E4%B8%AAby%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">6.0.4.</span> <span class="nav-text">4. order by 、 distribute by 、sort by 和 cluster by 四个by的区别</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-order-by"><span class="nav-number">6.0.4.1.</span> <span class="nav-text">1. order by</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-sort-by"><span class="nav-number">6.0.4.2.</span> <span class="nav-text">2. sort by</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-distribute-by"><span class="nav-number">6.0.4.3.</span> <span class="nav-text">3. distribute by</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-Cluster-By"><span class="nav-number">6.0.4.4.</span> <span class="nav-text">4. Cluster By</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E5%87%BD%E6%95%B0"><span class="nav-number">6.0.5.</span> <span class="nav-text">5. 函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-%E7%B3%BB%E7%BB%9F%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0"><span class="nav-number">6.0.5.1.</span> <span class="nav-text">5.1 系统内置函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0over"><span class="nav-number">6.0.5.2.</span> <span class="nav-text">5.2 窗口函数over()</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#LAG-col-n-default-val"><span class="nav-number">6.0.5.2.1.</span> <span class="nav-text">LAG(col, n, default_val)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#LEAD-col-n-default-val"><span class="nav-number">6.0.5.2.2.</span> <span class="nav-text">LEAD(col, n, default_val)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ntile-n"><span class="nav-number">6.0.5.2.3.</span> <span class="nav-text">ntile(n)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#rank%E6%8E%92%E5%90%8D%E5%87%BD%E6%95%B0"><span class="nav-number">6.0.5.2.4.</span> <span class="nav-text">rank排名函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Hive%E8%B0%83%E4%BC%98"><span class="nav-number">6.0.6.</span> <span class="nav-text">6. Hive调优</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-%E5%BC%80%E5%90%AFmap%E7%AB%AF%E9%A2%84%E8%81%9A%E5%90%88"><span class="nav-number">6.0.6.1.</span> <span class="nav-text">6.1 开启map端预聚合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-count%EF%BC%88distinct-%EF%BC%89-%E4%BC%98%E5%8C%96"><span class="nav-number">6.0.6.2.</span> <span class="nav-text">6.2 count（distinct ） 优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-%E9%81%BF%E5%85%8D%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF"><span class="nav-number">6.0.6.3.</span> <span class="nav-text">6.3 避免笛卡尔积</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-%E8%A1%8C%E5%88%97%E8%BF%87%E6%BB%A4"><span class="nav-number">6.0.6.4.</span> <span class="nav-text">6.4 行列过滤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-5-%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AEmap%E5%92%8Creduce%E7%9A%84%E6%95%B0%E9%87%8F"><span class="nav-number">6.0.6.5.</span> <span class="nav-text">6.5 合理设置map和reduce的数量</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%8D%E6%9D%82%E6%96%87%E4%BB%B6%E5%A2%9E%E5%8A%A0map%E6%95%B0"><span class="nav-number">6.0.6.5.1.</span> <span class="nav-text">复杂文件增加map数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B0%8F%E6%96%87%E4%BB%B6%E8%A1%8C%E5%90%88%E5%B9%B6"><span class="nav-number">6.0.6.5.2.</span> <span class="nav-text">小文件行合并</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-6-%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C"><span class="nav-number">6.0.6.6.</span> <span class="nav-text">6.6 并行执行</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-Hive%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0"><span class="nav-number">6.0.7.</span> <span class="nav-text">7. Hive源码学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-HOL%E8%BD%AC%E6%8D%A2%E6%88%90MR%E4%BB%BB%E5%8A%A1%E6%B5%81%E7%A8%8B%E8%AF%B4%E6%98%8E"><span class="nav-number">6.0.7.1.</span> <span class="nav-text">7.1 HOL转换成MR任务流程说明</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-HQL%E7%BB%83%E4%B9%A0"><span class="nav-number">6.0.8.</span> <span class="nav-text">8. HQL练习</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Zookeeper%E5%A4%8D%E4%B9%A0"><span class="nav-number">7.</span> <span class="nav-text">Zookeeper复习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Zookeeper%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86"><span class="nav-number">7.1.</span> <span class="nav-text">1. Zookeeper内部原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6"><span class="nav-number">7.1.1.</span> <span class="nav-text">1.1 选举机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E8%8A%82%E7%82%B9%E7%B1%BB%E5%9E%8B"><span class="nav-number">7.1.2.</span> <span class="nav-text">2. 节点类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Stat%E7%BB%93%E6%9E%84%E4%BD%93"><span class="nav-number">7.1.3.</span> <span class="nav-text">3. Stat结构体</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E7%9B%91%E5%90%AC%E5%99%A8%E5%8E%9F%E7%90%86"><span class="nav-number">7.1.4.</span> <span class="nav-text">4. 监听器原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Zookeeper%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="nav-number">7.1.5.</span> <span class="nav-text">5. Zookeeper写数据流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Zookeeper%E7%9A%84%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="nav-number">7.1.6.</span> <span class="nav-text">6. Zookeeper的常用命令</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Kafka%E5%A4%8D%E4%B9%A0"><span class="nav-number">8.</span> <span class="nav-text">Kafka复习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E4%BD%9C%E7%94%A8%EF%BC%88%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%9F%EF%BC%89"><span class="nav-number">8.1.</span> <span class="nav-text">1. 消息队列的作用（为什么使用消息队列？）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8A%E5%B3%B0"><span class="nav-number">8.1.1.</span> <span class="nav-text">削峰</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E8%80%A6"><span class="nav-number">8.1.2.</span> <span class="nav-text">解耦</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%82%E6%AD%A5"><span class="nav-number">8.1.3.</span> <span class="nav-text">异步</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Kafka%E7%9A%84%E7%89%B9%E7%82%B9"><span class="nav-number">8.2.</span> <span class="nav-text">2. Kafka的特点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Kafka%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84"><span class="nav-number">8.3.</span> <span class="nav-text">3. Kafka基础架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Kafka%E9%83%A8%E7%BD%B2%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-number">8.4.</span> <span class="nav-text">4. Kafka部署注意事项</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Kafka%E6%9E%B6%E6%9E%84%E6%B7%B1%E5%85%A5"><span class="nav-number">8.5.</span> <span class="nav-text">5. Kafka架构深入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-Kafka%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E4%BB%A5%E5%8F%8A%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6"><span class="nav-number">8.5.1.</span> <span class="nav-text">5.1 Kafka工作流程以及文件存储机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6"><span class="nav-number">8.5.1.1.</span> <span class="nav-text">文件存储机制</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-%E7%94%9F%E4%BA%A7%E8%80%85"><span class="nav-number">8.5.2.</span> <span class="nav-text">5.2 生产者</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%AF%E9%9D%A0%E6%80%A7"><span class="nav-number">8.5.2.1.</span> <span class="nav-text">数据可靠性</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ISR%E5%90%8C%E6%AD%A5%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6"><span class="nav-number">8.5.2.1.1.</span> <span class="nav-text">ISR同步副本机制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ack%E5%BA%94%E7%AD%94%E6%9C%BA%E5%88%B6"><span class="nav-number">8.5.2.1.2.</span> <span class="nav-text">ack应答机制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E7%BB%86%E8%8A%82"><span class="nav-number">8.5.2.1.3.</span> <span class="nav-text">故障处理细节</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#1-follower%E6%95%85%E9%9A%9C"><span class="nav-number">8.5.2.1.3.1.</span> <span class="nav-text">1. follower故障</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#2-leader%E6%95%85%E9%9A%9C"><span class="nav-number">8.5.2.1.3.2.</span> <span class="nav-text">2. leader故障</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Exactly-Once%E8%AF%AD%E4%B9%89"><span class="nav-number">8.5.2.1.4.</span> <span class="nav-text">Exactly Once语义</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5"><span class="nav-number">8.5.3.</span> <span class="nav-text">5.3 分区分配策略</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Range"><span class="nav-number">8.5.3.1.</span> <span class="nav-text">Range</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RoundRobin"><span class="nav-number">8.5.3.2.</span> <span class="nav-text">RoundRobin</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-kafka-%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E6%B5%81%E7%A8%8B"><span class="nav-number">8.5.4.</span> <span class="nav-text">5.4 kafka 消息发送流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-Kafka%E4%B8%AD%E7%9A%84%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6"><span class="nav-number">8.5.5.</span> <span class="nav-text">5.5 Kafka中的选举机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-Kafka%E7%9A%84%E6%8E%A7%E5%88%B6%E5%99%A8Controller%E6%98%AF%E5%A6%82%E4%BD%95%E8%A2%AB%E9%80%89%E4%B8%BE%E5%87%BA%E6%9D%A5%E7%9A%84%EF%BC%9F"><span class="nav-number">8.5.5.1.</span> <span class="nav-text">1. Kafka的控制器Controller是如何被选举出来的？</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8E%A7%E5%88%B6%E5%99%A8%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="nav-number">8.5.5.1.1.</span> <span class="nav-text">控制器的作用</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8E%A7%E5%88%B6%E5%99%A8%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB"><span class="nav-number">8.5.5.1.2.</span> <span class="nav-text">控制器故障转移</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%8C%BALeader%E7%9A%84%E9%80%89%E5%8F%96"><span class="nav-number">8.5.5.2.</span> <span class="nav-text">分区Leader的选取</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98"><span class="nav-number">8.6.</span> <span class="nav-text">常见面试题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%B6%E6%8B%B7%E8%B4%9D"><span class="nav-number">8.6.1.</span> <span class="nav-text">零拷贝</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E7%94%9F%E4%BA%A7%E8%80%85%E7%9A%84%E5%90%9E%E5%90%90%E9%87%8F%EF%BC%9F"><span class="nav-number">8.6.2.</span> <span class="nav-text">如何提高生产者的吞吐量？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Flink%E5%A4%8D%E4%B9%A0"><span class="nav-number">9.</span> <span class="nav-text">Flink复习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-WaterMark"><span class="nav-number">9.1.</span> <span class="nav-text">1. WaterMark</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-CheckPoint%EF%BC%88%E4%B8%80%E8%87%B4%E6%80%A7%E6%A3%80%E6%9F%A5%E7%82%B9%EF%BC%89"><span class="nav-number">9.2.</span> <span class="nav-text">2. CheckPoint（一致性检查点）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A3%80%E6%9F%A5%E7%82%B9%E4%BF%9D%E5%AD%98%EF%BC%88Checkpoint%EF%BC%89"><span class="nav-number">9.2.0.1.</span> <span class="nav-text">检查点保存（Checkpoint）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8E%E6%A3%80%E6%9F%A5%E7%82%B9%E6%81%A2%E5%A4%8D%E7%8A%B6%E6%80%81"><span class="nav-number">9.2.0.2.</span> <span class="nav-text">从检查点恢复状态</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CheckPoint%E7%AE%97%E6%B3%95"><span class="nav-number">9.2.0.3.</span> <span class="nav-text">CheckPoint算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E5%90%8E%E7%AB%AF"><span class="nav-number">9.2.0.4.</span> <span class="nav-text">状态后端</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Flink%E4%B8%AD%E7%9A%84%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">9.3.</span> <span class="nav-text">3.Flink中的状态一致性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E8%87%B4%E6%80%A7%E7%BA%A7%E5%88%AB"><span class="nav-number">9.3.1.</span> <span class="nav-text">一致性级别</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AB%AF%E5%88%B0%E7%AB%AF%EF%BC%88end-to-end%EF%BC%89%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">9.3.1.1.</span> <span class="nav-text">端到端（end to end）状态一致性</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1"><span class="nav-number">9.4.</span> <span class="nav-text">4.分布式事务</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%9C%AC%E5%9C%B0%E4%BA%8B%E5%8A%A1"><span class="nav-number">9.4.1.</span> <span class="nav-text">1. 本地事务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1"><span class="nav-number">9.4.2.</span> <span class="nav-text">2.分布式事务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-CAP-%E7%90%86%E8%AE%BA"><span class="nav-number">9.4.3.</span> <span class="nav-text">3. CAP 理论</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E8%87%B4%E6%80%A7C"><span class="nav-number">9.4.3.1.</span> <span class="nav-text">一致性C</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%AF%E7%94%A8%E6%80%A7A"><span class="nav-number">9.4.3.2.</span> <span class="nav-text">可用性A</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E6%9C%9F%E5%AE%B9%E5%BF%8D%E6%80%A7"><span class="nav-number">9.4.3.3.</span> <span class="nav-text">分期容忍性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CAP-%E7%BB%84%E5%90%88%E6%96%B9%E5%BC%8F"><span class="nav-number">9.4.3.4.</span> <span class="nav-text">CAP 组合方式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BASE%E7%90%86%E8%AE%BA"><span class="nav-number">9.4.3.5.</span> <span class="nav-text">BASE理论</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E2%98%9E2PC%EF%BC%88%E4%B8%A4%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4%EF%BC%89"><span class="nav-number">9.4.4.</span> <span class="nav-text">4. 分布式事务解决方案☞2PC（两阶段提交）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF2PC"><span class="nav-number">9.4.4.1.</span> <span class="nav-text">1. 什么是2PC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88XA"><span class="nav-number">9.4.4.2.</span> <span class="nav-text">2. 解决方案XA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88Seata%E6%96%B9%E6%A1%88"><span class="nav-number">9.4.4.3.</span> <span class="nav-text">3.解决方案Seata方案</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%B0%E4%BB%93%E5%A4%8D%E4%B9%A0"><span class="nav-number">10.</span> <span class="nav-text">数仓复习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E4%BB%93%E7%90%86%E8%AE%BA"><span class="nav-number">10.1.</span> <span class="nav-text">数仓理论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BE%85%E8%A1%A5%E5%85%85%E3%80%82%E3%80%82%E3%80%82"><span class="nav-number">10.1.1.</span> <span class="nav-text">待补充。。。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90-%E4%B8%8E-%E6%A1%91%E5%9F%BA%E5%9B%BE"><span class="nav-number">10.1.2.</span> <span class="nav-text">路径分析  与  桑基图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%80%E5%8F%91%E5%B0%8F%E6%8A%80%E5%B7%A7"><span class="nav-number">10.2.</span> <span class="nav-text">开发小技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8C%BA%E5%88%86-%E7%BB%B4%E5%BA%A6-%E4%B8%8E-%E6%8C%87%E6%A0%87"><span class="nav-number">10.2.1.</span> <span class="nav-text">区分 维度 与 指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8C%E6%97%B6%E6%B1%82-%E8%BF%911%E5%A4%A9%EF%BC%8C%E8%BF%917%E5%A4%A9%EF%BC%8C-%E8%BF%9130%E5%A4%A9%E7%9A%84%E6%8C%87%E6%A0%87%E5%80%BC%EF%BC%88%E9%80%9A%E8%BF%87%E4%B8%80%E4%B8%AA%E5%AD%97%E6%AE%B5%E7%9A%84%E4%B8%8D%E5%90%8C%E5%8F%96%E5%80%BC%E6%9D%A5%E5%8C%BA%E5%88%86%EF%BC%89"><span class="nav-number">10.2.2.</span> <span class="nav-text">同时求 近1天，近7天， 近30天的指标值（通过一个字段的不同取值来区分）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E5%A4%8D%E4%B9%A0"><span class="nav-number">11.</span> <span class="nav-text">网络复习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-OSI%E7%9A%84%E4%B8%83%E5%B1%82%E5%8F%82%E8%80%83%E6%A8%A1%E5%9E%8B"><span class="nav-number">11.1.</span> <span class="nav-text">1. OSI的七层参考模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E4%B8%80%E6%AC%A1%E5%AE%8C%E6%95%B4%E7%9A%84Http%E8%AF%B7%E6%B1%82%E8%BF%87%E7%A8%8B"><span class="nav-number">11.2.</span> <span class="nav-text">2. 一次完整的Http请求过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-DNS"><span class="nav-number">11.3.</span> <span class="nav-text">3. DNS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-DNS%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="nav-number">11.3.1.</span> <span class="nav-text">3.1 DNS的工作原理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Http%E9%95%BF%E8%BF%9E%E6%8E%A5%E4%B8%8E%E7%9F%AD%E8%BF%9E%E6%8E%A5"><span class="nav-number">11.4.</span> <span class="nav-text">4. Http长连接与短连接</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-HTTP%E8%AF%B7%E6%B1%82%E6%96%B9%E6%B3%95"><span class="nav-number">11.5.</span> <span class="nav-text">5. HTTP请求方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GET-%E4%B8%8E-POST%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="nav-number">11.5.1.</span> <span class="nav-text">GET 与 POST的区别？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-HTTPS%E5%92%8CHTTP%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">11.6.</span> <span class="nav-text">6. HTTPS和HTTP的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-%E4%BB%80%E4%B9%88%E6%98%AFRARP%EF%BC%9F%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="nav-number">11.7.</span> <span class="nav-text">7. 什么是RARP？工作原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-%E4%BB%80%E4%B9%88%E6%98%AFARP%EF%BC%9F"><span class="nav-number">11.8.</span> <span class="nav-text">8. 什么是ARP？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-TCP-%E5%A4%B4%E9%83%A8%E4%BF%A1%E6%81%AF"><span class="nav-number">11.9.</span> <span class="nav-text">9. TCP 头部信息</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-TCP-%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%F0%9F%A4%9D%E4%B8%8E%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%F0%9F%91%8B%F0%9F%8F%BB"><span class="nav-number">11.10.</span> <span class="nav-text">10. TCP 三次握手🤝与四次挥手👋🏻</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%BE%E7%A8%8B%E6%B2%89%E6%B7%80"><span class="nav-number">11.11.</span> <span class="nav-text">课程沉淀</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%89%A9%E7%90%86%E5%B1%82"><span class="nav-number">11.11.1.</span> <span class="nav-text">1. 物理层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82"><span class="nav-number">11.11.2.</span> <span class="nav-text">2. 数据链路层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E7%BD%91%E7%BB%9C%E5%B1%82"><span class="nav-number">11.11.3.</span> <span class="nav-text">3. 网络层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#IP%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3"><span class="nav-number">11.11.3.1.</span> <span class="nav-text">IP协议详解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#IP%E5%8D%8F%E8%AE%AE%E7%9A%84%E8%BD%AC%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="nav-number">11.11.3.2.</span> <span class="nav-text">IP协议的转发流程</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E8%B7%AF%E7%94%B1%E8%A1%A8%E7%AE%80%E4%BB%8B"><span class="nav-number">11.11.3.2.1.</span> <span class="nav-text">1. 路由表简介</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-IP%E5%8D%8F%E8%AE%AE%E7%9A%84%E8%BD%AC%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="nav-number">11.11.3.2.2.</span> <span class="nav-text">2.IP协议的转发流程</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ARP%E5%8D%8F%E8%AE%AE%E5%92%8CRARP%E5%8D%8F%E8%AE%AE"><span class="nav-number">11.11.3.3.</span> <span class="nav-text">ARP协议和RARP协议</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ARP%E5%9C%B0%E5%9D%80%E8%A7%A3%E6%9E%90%E5%8D%8F%E8%AE%AE"><span class="nav-number">11.11.3.3.1.</span> <span class="nav-text">ARP地址解析协议</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RARP%E9%80%86%E5%9C%B0%E5%9D%80%E8%A7%A3%E6%9E%90%E5%8D%8F%E8%AE%AE"><span class="nav-number">11.11.3.3.2.</span> <span class="nav-text">RARP逆地址解析协议</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#IP%E5%9C%B0%E5%9D%80%E7%9A%84%E5%AD%90%E7%BD%91%E5%88%92%E5%88%86"><span class="nav-number">11.11.3.4.</span> <span class="nav-text">IP地址的子网划分</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E5%88%86%E7%B1%BB%E7%9A%84IP%E5%9C%B0%E5%9D%80"><span class="nav-number">11.11.3.4.1.</span> <span class="nav-text">1. 分类的IP地址</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%89%B9%E6%AE%8A%E7%9A%84%E4%B8%BB%E6%9C%BA%E5%8F%B7"><span class="nav-number">11.11.3.4.1.1.</span> <span class="nav-text">特殊的主机号</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E7%89%B9%E6%AE%8A%E7%9A%84%E7%BD%91%E7%BB%9C%E5%8F%B7"><span class="nav-number">11.11.3.4.1.2.</span> <span class="nav-text">特殊的网络号</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E5%88%92%E5%88%86%E5%AD%90%E7%BD%91"><span class="nav-number">11.11.3.4.2.</span> <span class="nav-text">2. 划分子网</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-%E6%97%A0%E5%88%86%E7%B1%BB%E7%BC%96%E5%9D%80CIDR"><span class="nav-number">11.11.3.4.3.</span> <span class="nav-text">3. 无分类编址CIDR</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%A4%8D%E4%B9%A0"><span class="nav-number">12.</span> <span class="nav-text">操作系统复习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Redis%E5%A4%8D%E4%B9%A0"><span class="nav-number">13.</span> <span class="nav-text">Redis复习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="nav-number">13.1.</span> <span class="nav-text">1.  常用数据类型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#String"><span class="nav-number">13.1.1.</span> <span class="nav-text">String</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#List"><span class="nav-number">13.1.2.</span> <span class="nav-text">List</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#set"><span class="nav-number">13.1.3.</span> <span class="nav-text">set</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hash"><span class="nav-number">13.1.4.</span> <span class="nav-text">Hash</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Zset"><span class="nav-number">13.1.5.</span> <span class="nav-text">Zset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BitMap%E4%BD%8D%E5%9B%BE"><span class="nav-number">13.1.6.</span> <span class="nav-text">BitMap位图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Redis%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">13.2.</span> <span class="nav-text">2. Redis持久化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RDB%EF%BC%88Redis-DataBase%EF%BC%89"><span class="nav-number">13.2.1.</span> <span class="nav-text">RDB（Redis DataBase）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AOF%EF%BC%88Append-Only-File%EF%BC%89"><span class="nav-number">13.2.2.</span> <span class="nav-text">AOF（Append Only File）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E5%93%A8%E5%85%B5Sentine%E6%A8%A1%E5%BC%8F%EF%BC%88%E5%8F%8D%E5%AE%A2%E4%B8%BA%E4%B8%BB%E7%9A%84%E8%87%AA%E5%8A%A8%E7%89%88%EF%BC%89"><span class="nav-number">13.3.</span> <span class="nav-text">3. 哨兵Sentine模式（反客为主的自动版）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Redis-%E4%B8%AD-Key%E7%9A%84%E8%BF%87%E6%9C%9F%E6%B7%98%E6%B1%B0%E6%9C%BA%E5%88%B6"><span class="nav-number">13.4.</span> <span class="nav-text">4. Redis 中 Key的过期淘汰机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E6%9C%9F%E5%88%A0%E9%99%A4"><span class="nav-number">13.4.1.</span> <span class="nav-text">定期删除</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%83%B0%E6%80%A7%E5%88%A0%E9%99%A4"><span class="nav-number">13.4.2.</span> <span class="nav-text">惰性删除</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0%E6%9C%BA%E5%88%B6"><span class="nav-number">13.5.</span> <span class="nav-text">5. 内存淘汰机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8%E7%A7%8D%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0%E6%9C%BA%E5%88%B6"><span class="nav-number">13.5.1.</span> <span class="nav-text">8种内存淘汰机制</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Git%E5%A4%8D%E4%B9%A0"><span class="nav-number">14.</span> <span class="nav-text">Git复习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#-1"><span class="nav-number">14.0.1.</span> <span class="nav-text"></span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Docker%E5%A4%8D%E4%B9%A0"><span class="nav-number">15.</span> <span class="nav-text">Docker复习</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0"><span class="nav-number">16.</span> <span class="nav-text">经典算法复习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F%E5%8F%8A%E9%83%A8%E5%88%86%E4%BC%98%E5%8C%96"><span class="nav-number">16.1.</span> <span class="nav-text">1. 经典排序及部分优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Bubble-Sort"><span class="nav-number">16.1.1.</span> <span class="nav-text">1. Bubble Sort</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Selection-Sort"><span class="nav-number">16.1.2.</span> <span class="nav-text">2. Selection Sort</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Insertion-Sort"><span class="nav-number">16.1.3.</span> <span class="nav-text">3. Insertion Sort</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Quick-Sort"><span class="nav-number">16.1.4.</span> <span class="nav-text">4. Quick Sort</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-HeapSort"><span class="nav-number">16.1.5.</span> <span class="nav-text">5. HeapSort</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F"><span class="nav-number">16.1.6.</span> <span class="nav-text">6. 归并排序</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-ShellSort"><span class="nav-number">16.1.7.</span> <span class="nav-text">7. ShellSort</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95"><span class="nav-number">16.2.</span> <span class="nav-text">2. 分布式算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%88%86%E5%B8%83%E5%BC%8Fid%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95%EF%BC%88%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95%EF%BC%89"><span class="nav-number">16.2.1.</span> <span class="nav-text">1. 分布式id生成算法（雪花算法）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SnowFlake"><span class="nav-number">16.2.1.1.</span> <span class="nav-text">SnowFlake</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Flink%E4%B8%AD%E7%9A%84%E6%A3%80%E6%9F%A5%E7%82%B9%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95"><span class="nav-number">16.2.2.</span> <span class="nav-text">2. Flink中的检查点一致性算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95"><span class="nav-number">16.2.3.</span> <span class="nav-text">3. 分布式一致性算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CAP%E7%90%86%E8%AE%BA%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-number">16.2.3.1.</span> <span class="nav-text">CAP理论的定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A8%A1%E5%9E%8B"><span class="nav-number">16.2.3.2.</span> <span class="nav-text">决策模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Paxos"><span class="nav-number">16.2.3.3.</span> <span class="nav-text">Paxos</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Basic-Paxos%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B"><span class="nav-number">16.2.3.3.1.</span> <span class="nav-text">Basic Paxos基本流程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Multi-Paxos"><span class="nav-number">16.2.3.3.2.</span> <span class="nav-text">Multi Paxos</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BC%BA%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%E2%80%93Raft"><span class="nav-number">16.2.3.3.3.</span> <span class="nav-text">强一致性算法–Raft</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BC%BA%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95-ZAB"><span class="nav-number">16.2.3.3.4.</span> <span class="nav-text">强一致性算法-ZAB</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9D%A2%E8%AF%95%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">17.</span> <span class="nav-text">面试中遇到的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E8%87%AA%E5%B7%B1%E7%9A%84%E4%BA%AE%E7%82%B9"><span class="nav-number">17.1.</span> <span class="nav-text">1. 自己的亮点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E9%9D%A2%E8%AF%95%E8%B0%88%E8%96%AA%E8%B5%84%E6%8A%80%E5%B7%A7"><span class="nav-number">17.2.</span> <span class="nav-text">2. 面试谈薪资技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E9%9D%A2%E8%AF%95%E5%AE%98%E9%97%AE%E4%BD%A0%E7%9A%84%E6%9C%9F%E6%9C%9B%E8%96%AA%E8%B5%84%E8%A6%81%E6%B1%82%EF%BC%88%E6%9C%9F%E6%9C%9B%E8%96%AA%E8%B5%84%E6%98%AF%E5%A4%9A%E5%B0%91%EF%BC%9F%EF%BC%89"><span class="nav-number">17.2.1.</span> <span class="nav-text">1. 面试官问你的期望薪资要求（期望薪资是多少？）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%88%91%E8%A7%89%E5%BE%97%E4%BD%A0%E8%A6%81%E7%9A%84%E8%96%AA%E9%85%AC%E6%9C%89%E7%82%B9%E9%AB%98%EF%BC%9F%EF%BC%9F%EF%BC%9F"><span class="nav-number">17.2.2.</span> <span class="nav-text">2. 我觉得你要的薪酬有点高？？？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%A6%82%E6%9E%9CHR%E6%B2%A1%E6%9C%89%E5%92%8C%E4%BD%A0%E8%B0%88%E8%96%AA%E8%B5%84"><span class="nav-number">17.2.3.</span> <span class="nav-text">3. 如果HR没有和你谈薪资</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E9%92%88%E5%AF%B9%E4%BA%8E%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE%EF%BC%8C%E4%B8%9A%E5%8A%A1%E4%B8%8A%E7%BC%BA%E4%B9%8F%E8%87%AA%E5%B7%B1%E7%8B%AC%E7%AB%8B%E7%9A%84%E6%80%9D%E8%80%83"><span class="nav-number">17.3.</span> <span class="nav-text">2. 针对于实习项目，业务上缺乏自己独立的思考</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%85%A8%E5%B9%B3%E5%8F%B0%E4%BA%A7%E5%93%81%E8%BF%90%E8%90%A5%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="nav-number">17.3.1.</span> <span class="nav-text">2.1 可视化全平台产品运营数据分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E6%9C%9F%E9%9C%80%E6%B1%82"><span class="nav-number">17.3.1.1.</span> <span class="nav-text">一期需求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8C%E6%9C%9F%E9%9C%80%E6%B1%82"><span class="nav-number">17.3.1.2.</span> <span class="nav-text">二期需求</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E5%85%A8%E7%94%9F%E6%80%81%E6%95%B0%E6%8D%AE%E5%A2%9E%E9%95%BF%E5%BB%BA%E8%AE%BE"><span class="nav-number">17.3.2.</span> <span class="nav-text">2.2 全生态数据增长建设</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E6%9C%9F%E9%9C%80%E6%B1%82-1"><span class="nav-number">17.3.2.1.</span> <span class="nav-text">一期需求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8C%E6%9C%9F%E9%9C%80%E6%B1%82-1"><span class="nav-number">17.3.2.2.</span> <span class="nav-text">二期需求</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E5%9B%9E%E7%AD%94%E9%97%AE%E9%A2%98%E9%80%BB%E8%BE%91%E7%BB%95%EF%BC%8C%E5%95%B0%E5%97%A6"><span class="nav-number">17.4.</span> <span class="nav-text">3. 回答问题逻辑绕，啰嗦</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%84%E4%BB%B6%E4%BA%86%E8%A7%A3%E5%B0%91"><span class="nav-number">17.5.</span> <span class="nav-text">4. 大数据组件了解少</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%88%86%E5%B1%82"><span class="nav-number">17.6.</span> <span class="nav-text">5. 为什么要分层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E8%A6%81%E5%B1%95%E7%8E%B0%E8%87%AA%E5%B7%B1%E7%9A%84%E4%BA%AE%E7%82%B9%EF%BC%8C%E4%BF%9D%E6%8C%81%E4%B8%BB%E5%8A%A8%E6%80%A7"><span class="nav-number">17.7.</span> <span class="nav-text">6. 要展现自己的亮点，保持主动性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-%E8%87%AA%E8%BA%AB%E5%AD%98%E5%9C%A8%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="nav-number">17.8.</span> <span class="nav-text">7. 自身存在的缺点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-%E8%87%AA%E8%BA%AB%E5%AD%98%E5%9C%A8%E7%9A%84%E4%BC%98%E7%82%B9"><span class="nav-number">17.9.</span> <span class="nav-text">8. 自身存在的优点</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BD%92%E5%9B%A0%E5%88%86%E6%9E%90%EF%BC%88%E5%A4%9A%E6%B8%A0%E9%81%93%E5%BD%92%E5%9B%A0%EF%BC%89"><span class="nav-number">18.</span> <span class="nav-text">归因分析（多渠道归因）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E6%9C%AB%E6%AC%A1%E5%BD%92%E5%9B%A0"><span class="nav-number">18.1.</span> <span class="nav-text">1. 末次归因</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E9%A6%96%E6%AC%A1%E5%BD%92%E5%9B%A0"><span class="nav-number">18.2.</span> <span class="nav-text">2. 首次归因</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E4%BD%8D%E7%BD%AE%E5%BD%92%E5%9B%A0"><span class="nav-number">18.3.</span> <span class="nav-text">3.位置归因</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E6%97%B6%E9%97%B4%E8%A1%B0%E5%87%8F"><span class="nav-number">18.4.</span> <span class="nav-text">4.时间衰减</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%AF%94%E9%87%8D"><span class="nav-number">18.5.</span> <span class="nav-text">5.自定义比重</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#-2"><span class="nav-number">19.</span> <span class="nav-text"></span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
